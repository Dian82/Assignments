{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5. Training Issues\n",
    "Through the previous learning, you may have already known how to train a model from scratch. In this part, we will firstly review the whole training process by setting up a classification network on MNIST dataset. Then we will highlight some useful tricks to improve the model performance.\n",
    "\n",
    "If you have any questions or suggestions about this part, please feel free to contact the teaching assistants Wanying Tao and Jianfei Xing on WeChat.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:42.762786Z",
     "start_time": "2019-03-21T14:21:42.753422Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:43.871351Z",
     "start_time": "2019-03-21T14:21:43.472080Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:43.880674Z",
     "start_time": "2019-03-21T14:21:43.872738Z"
    }
   },
   "outputs": [],
   "source": [
    "# cuda = torch.cuda.is_available() \n",
    "# torch.cuda.set_device(device) \n",
    "\n",
    "# Limited by GPU resources, we recommend computing on CPU\n",
    "cuda = torch.device('cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classfication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:46.327672Z",
     "start_time": "2019-03-21T14:21:46.308085Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs                Linear/Function        Output\n",
    "    [128, 1, 28, 28]   -> Linear(28*28, 100) -> [128, 100]  # the first hidden layer\n",
    "                       -> ReLU               -> [128, 100]  # ReLU activation function, may Sigmoid\n",
    "                       -> Linear(100, 100)   -> [128, 100]  # the second hidden layer\n",
    "                       -> ReLU               -> [128, 100]  # ReLU activation function, may Sigmoid\n",
    "                       -> Linear(100, 100)   -> [128, 100]  # the third hidden layer\n",
    "                       -> ReLU               -> [128, 100]  # ReLU activation function, may Sigmoid\n",
    "                       -> Linear(100, 10)    -> [128, 10]   # classification layer                                                          \n",
    "   \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation_function='RELU'):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.use_dropout = False\n",
    "        self.use_bn = False\n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size)  # Linear function 1: 784 --> 100 \n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size) # Linear function 2: 100 --> 100\n",
    "        self.hidden3 = nn.Linear(hidden_size, hidden_size) # Linear function 3: 100 --> 100\n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.classification_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=0.5) # Drop out with prob = 0.5\n",
    "        self.hidden1_bn = nn.BatchNorm1d(hidden_size) # Batch Normalization \n",
    "        self.hidden2_bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.hidden3_bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Non-linearity\n",
    "        if activation_function == 'SIGMOID':\n",
    "            self.activation_function1 = nn.Sigmoid()\n",
    "            self.activation_function2 = nn.Sigmoid()\n",
    "            self.activation_function3 = nn.Sigmoid()\n",
    "        elif activation_function == 'RELU':\n",
    "            self.activation_function1 = nn.ReLU()\n",
    "            self.activation_function2 = nn.ReLU()\n",
    "            self.activation_function3 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, channel, height, width], network input\n",
    "        Returns:\n",
    "            out: [batch_size, n_classes], network output\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flatten x in [128, 784]\n",
    "        out = self.hidden1(x)\n",
    "        out = self.activation_function1(out) # Non-linearity 1\n",
    "        if self.use_bn == True:\n",
    "            out = self.hidden1_bn(out)\n",
    "        out = self.hidden2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        if self.use_bn == True:\n",
    "            out = self.hidden2_bn(out)\n",
    "        out = self.hidden3(out)\n",
    "        if self.use_bn == True:\n",
    "            out = self.hidden3_bn(out)\n",
    "        out = self.activation_function3(out)\n",
    "        if self.use_dropout == True:\n",
    "            out = self.dropout(out)\n",
    "        out = self.classification_layer(out)\n",
    "        return out\n",
    "    \n",
    "    def set_use_dropout(self, use_dropout):\n",
    "        \"\"\"Whether to use dropout. Auxiliary function for our exp, not necessary.\n",
    "        Args:\n",
    "            use_dropout: True, False\n",
    "        \"\"\"\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def set_use_bn(self, use_bn):\n",
    "        \"\"\"Whether to use batch normalization. Auxiliary function for our exp, not necessary.\n",
    "        Args:\n",
    "            use_bn: True, False\n",
    "        \"\"\"\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "    def get_grad(self):\n",
    "        \"\"\"Return average grad for hidden2, hidden3. Auxiliary function for our exp, not necessary.\n",
    "        \"\"\"\n",
    "        hidden2_average_grad = np.mean(np.sqrt(np.square(self.hidden2.weight.grad.detach().numpy())))\n",
    "        hidden3_average_grad = np.mean(np.sqrt(np.square(self.hidden3.weight.grad.detach().numpy())))\n",
    "        return hidden2_average_grad, hidden3_average_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T07:40:23.774920Z",
     "start_time": "2019-03-17T07:40:23.769858Z"
    }
   },
   "source": [
    "### 3.1 Pre-set hyper-parameters\n",
    "* learning rate: usually set the learning rate to 1e-1, 1e-2 or 1e-3, and gradually decrease its value during iteration.\n",
    "* n_epochs: training epoch must be set large enough to ensure that the model can converge. \n",
    "* batch_size: bigger batch size means better usage of GPU and less time for model to converge, generally use the exponent power of 2, e.g., 2, 4, 8, 16, 32, 64, 128, 256.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:47.253279Z",
     "start_time": "2019-03-21T14:21:47.243176Z"
    }
   },
   "outputs": [],
   "source": [
    "### Hyper parameters\n",
    "\n",
    "batch_size = 128 # batch size is 128\n",
    "n_epochs = 5 # train 5 epochs\n",
    "learning_rate = 0.01 # learning rate is 0.01\n",
    "input_size = 28*28 # tthe size of image is 28x28\n",
    "hidden_size = 100 # 100 hidden neurons in each layer\n",
    "output_size = 10 # classes of prediction\n",
    "l2_norm = 0 # not to use l2 penalty\n",
    "dropout = False # not to use dropout\n",
    "get_grad = False # not to obtain grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:47.842655Z",
     "start_time": "2019-03-21T14:21:47.827237Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a model object\n",
    "model = FeedForwardNeuralNetwork(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "# loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# l2_norm can be done in SGD\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize model parameters\n",
    "PyTorch provides default initialization (**uniform intialization**) for linear layer, and there are also some other useful initialization methods you may use in the homework.\n",
    "\n",
    "Click on this [link](https://pytorch.org/docs/stable/_modules/torch/nn/init.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:48.832921Z",
     "start_time": "2019-03-21T14:21:48.814998Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_weight_bias(model):\n",
    "    \"\"\"Show weights and bias distribution. \n",
    "    \"\"\"\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, axs = plt.subplots(2,3, sharey=False, tight_layout=True)\n",
    "    \n",
    "    # weight and bias for every hidden layer\n",
    "    h1_w = model.hidden1.weight.detach().numpy().flatten()\n",
    "    h1_b = model.hidden1.bias.detach().numpy().flatten()\n",
    "    h2_w = model.hidden2.weight.detach().numpy().flatten()\n",
    "    h2_b = model.hidden2.bias.detach().numpy().flatten()\n",
    "    h3_w = model.hidden3.weight.detach().numpy().flatten()\n",
    "    h3_b = model.hidden3.bias.detach().numpy().flatten()\n",
    "    \n",
    "    axs[0,0].hist(h1_w)\n",
    "    axs[0,1].hist(h2_w)\n",
    "    axs[0,2].hist(h3_w)\n",
    "    axs[1,0].hist(h1_b)\n",
    "    axs[1,1].hist(h2_b)\n",
    "    axs[1,2].hist(h3_b)\n",
    "    \n",
    "    # set title for every sub plots\n",
    "    axs[0,0].set_title('hidden1_weight')\n",
    "    axs[0,1].set_title('hidden2_weight')\n",
    "    axs[0,2].set_title('hidden3_weight')\n",
    "    axs[1,0].set_title('hidden1_bias')\n",
    "    axs[1,1].set_title('hidden2_bias')\n",
    "    axs[1,2].set_title('hidden3_bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:49.873556Z",
     "start_time": "2019-03-21T14:21:49.311764Z"
    }
   },
   "outputs": [],
   "source": [
    "show_weight_bias(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:49.883619Z",
     "start_time": "2019-03-21T14:21:49.874868Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_bias_reset(model):\n",
    "    \"\"\"Custom initialization, you can imitate the code writing to use other initialization methods in homework.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            mean, std = 0, 0.1 \n",
    "            \n",
    "            torch.nn.init.normal_(m.weight, mean, std)\n",
    "            torch.nn.init.normal_(m.bias, mean, std)\n",
    "            \n",
    "#             m.weight.data.normal_(mean, std)\n",
    "#             m.bias.data.normal_(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业1\n",
    "使用constant, xavier, kaiming三种方式初始化参数，并显示模型隐藏层的参数分布，不必初始化bias。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:07:12.114421Z",
     "start_time": "2019-03-19T15:07:12.106167Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def weight_bias_reset_constant(model):\n",
    "    \"\"\"Constant initalization\n",
    "    \"\"\"\n",
    "    # remove pass and code here\n",
    "    pass\n",
    "        \n",
    "# Reset parameters and show the distribution\n",
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:07:12.132231Z",
     "start_time": "2019-03-19T15:07:12.124218Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def weight_bias_reset_xavier_uniform(model):\n",
    "    \"\"\"xaveir_uniform, gain=1\n",
    "    \"\"\"\n",
    "    # remove pass and code here\n",
    "    pass\n",
    "        \n",
    "# Reset parameters and show the distribution\n",
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:07:12.149622Z",
     "start_time": "2019-03-19T15:07:12.141658Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def weight_bias_reset_kaiming_uniform(model):\n",
    "    \"\"\"kaiming_uniform, a=0, mode='fan_in', nonlinearity='relu'\n",
    "    \"\"\"\n",
    "    # remove pass and code here\n",
    "    pass\n",
    "        \n",
    "# Reset parameters and show the distribution\n",
    "# code here        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:45:10.788689Z",
     "start_time": "2019-03-17T08:45:10.786778Z"
    }
   },
   "source": [
    "### 3.3 Repeat over certain numbers of epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 data loading \n",
    "Please pay attention to data augmentation. \n",
    "\n",
    "Click on this [link](https://pytorch.org/docs/stable/torchvision/transforms.html). for more details.\n",
    "\n",
    "```\n",
    "torchvision.transforms.RandomVerticalFlip\n",
    "torchvision.transforms.RandomHorizontalFlip\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:55.228109Z",
     "start_time": "2019-03-21T14:21:55.215113Z"
    }
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:55.730947Z",
     "start_time": "2019-03-21T14:21:55.695503Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=train_transform,\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=test_transform,\n",
    "                           download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:56.062415Z",
     "start_time": "2019-03-21T14:21:56.044599Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_dataset doesn't load any data, it just defines some method and stores some message to preprocess data\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:21:56.599805Z",
     "start_time": "2019-03-21T14:21:56.591383Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:01.173826Z",
     "start_time": "2019-03-21T14:22:01.163671Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_fn, optimizer, get_grad=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_loader: training data\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to calculate the distance between target and outputs\n",
    "        optimizer: optimize the loss function\n",
    "        get_grad: True, False\n",
    "    Returns:\n",
    "        total_loss: loss\n",
    "        average_grad2: average grad for hidden 2 in this epoch\n",
    "        average_grad3: average grad for hidden 3 in this epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    grad_2 = 0.0 # store sum(grad) for hidden 2 layer\n",
    "    grad_3 = 0.0 # store sum(grad) for hidden 3 layer\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, target)  \n",
    "        total_loss += loss.item() \n",
    "        loss.backward() \n",
    "        \n",
    "        if get_grad == True:\n",
    "            g2, g3 = model.get_grad() # get gradients of hiddern 2 and 3 layer in this batch\n",
    "            grad_2 += g2\n",
    "            grad_3 += g3 \n",
    "            \n",
    "        optimizer.step() \n",
    "            \n",
    "    average_loss = total_loss / batch_idx \n",
    "    average_grad2 = grad_2 / batch_idx \n",
    "    average_grad3 = grad_3 / batch_idx \n",
    "    \n",
    "    return average_loss, average_grad2, average_grad3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:01.396180Z",
     "start_time": "2019-03-21T14:22:01.381555Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(val_loader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        val_loader: data for evaluation\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to calculate the distance between target and outputs\n",
    "    Returns:\n",
    "        total_loss:loss\n",
    "        accuracy: model prediction accuracy\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()       \n",
    "        correct = 0.0\n",
    "        total_loss = 0  \n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            outputs = model(data) \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == target).sum().detach().numpy()\n",
    "            loss = loss_fn(outputs, target)  \n",
    "            total_loss += loss.item() \n",
    "            \n",
    "        accuracy = correct*100.0 / len(val_loader.dataset) \n",
    "        \n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:02.178518Z",
     "start_time": "2019-03-21T14:22:02.167029Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, n_epochs, get_grad=False):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        train_loader: training data\n",
    "        val_loader: validation data\n",
    "        model: prediction model\n",
    "        loss_fn: loss function to calculate the distance between target and outputs\n",
    "        optimizer: optimize the loss function\n",
    "        n_epochs: training epochs\n",
    "        get_grad: whether to get gradients of hidden2 layer and hidden3 layer or not\n",
    "    Returns:\n",
    "        train_accs: accuracy of training n_epochs, a list\n",
    "        train_losses: loss of n_epochs, a list\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_2 = [] \n",
    "    grad_3 = []\n",
    "    \n",
    "    train_accs = [] \n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        \n",
    "        train_loss, average_grad2, average_grad3 = train(train_loader, model, loss_fn, optimizer, get_grad)\n",
    "        \n",
    "        _, train_accuracy = evaluate(train_loader, model, loss_fn)\n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(epoch+1, \\\n",
    "                                                                n_epochs, train_loss, train_accuracy)\n",
    "        print(message)\n",
    "    \n",
    "        # save loss, accuracy, grad\n",
    "        train_accs.append(train_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        grad_2.append(average_grad2)\n",
    "        grad_3.append(average_grad3)\n",
    "    \n",
    "        # evaluate model performance on val dataset\n",
    "        val_loss, val_accuracy = evaluate(val_loader, model, loss_fn)\n",
    "        message = 'Epoch: {}/{}. Validation set: Average loss: {:.4f}, Accuracy: {:.4f}'.format(epoch+1, \\\n",
    "                                                                n_epochs, val_loss, val_accuracy)\n",
    "        print(message)\n",
    "        \n",
    "    if get_grad == True:\n",
    "        fig, ax = plt.subplots() \n",
    "        ax.plot(grad_2, label='Gradients of Hidden 2 Layer') \n",
    "        ax.plot(grad_3, label='Gradients of Hidden 3 Layer') \n",
    "        plt.ylim(top=0.004)\n",
    "        # place a legend on axes\n",
    "        legend = ax.legend(loc='best', shadow=True, fontsize='x-large')\n",
    "    \n",
    "    return train_accs, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:03.197224Z",
     "start_time": "2019-03-21T14:22:03.186196Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_curve(ys, title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ys: loss or acc list\n",
    "        title: Loss or Accuracy\n",
    "    \"\"\"\n",
    "    x = np.array(range(len(ys)))\n",
    "    y = np.array(ys)\n",
    "    plt.plot(x, y, c='b')\n",
    "    plt.axis()\n",
    "    plt.title('{} Curve:'.format(title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('{} Value'.format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T05:37:32.874960Z",
     "start_time": "2019-03-18T05:37:32.871828Z"
    }
   },
   "source": [
    "## 作业 2\n",
    "将n_epochs依次设为5和10，画出训练过程中loss和accuracy的变化曲线，分别观察模型在训练集上的拟合情况。\n",
    "\n",
    "Hints: 因为jupyter对变量有上下文关系，模型和优化器需要重新声明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:04.832344Z",
     "start_time": "2019-03-21T14:22:04.819950Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# set hyper parameters and declare model, optimizer\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.564733Z",
     "start_time": "2019-03-19T15:08:11.555571Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# train and evaluate model\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# show curve\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业 3\n",
    "适当调整其他参数，使模型能在5个epoch内在训练集上达到过拟合，画出训练过程中loss和accuracy的变化曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.595671Z",
     "start_time": "2019-03-19T15:08:11.585338Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# set hyper parameters\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.606812Z",
     "start_time": "2019-03-19T15:08:11.597484Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# train and evaluate model\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.617329Z",
     "start_time": "2019-03-19T15:08:11.607942Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# show curve\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 save model \n",
    "PyTorch provides two methods to save the model. And we recommend the one that only saves parameters, because it's more flexible.\n",
    "\n",
    "A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "\n",
    "Click on this [link](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.641312Z",
     "start_time": "2019-03-19T15:08:11.628417Z"
    }
   },
   "outputs": [],
   "source": [
    "# show parameters in model\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"\\nOptimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.654672Z",
     "start_time": "2019-03-19T15:08:11.643226Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "save_path = './model.pt'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.680365Z",
     "start_time": "2019-03-19T15:08:11.655663Z"
    }
   },
   "outputs": [],
   "source": [
    "# load parameters from files\n",
    "saved_parameters = torch.load(save_path)\n",
    "print(saved_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.692042Z",
     "start_time": "2019-03-19T15:08:11.681431Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model with saved parameters\n",
    "new_model = FeedForwardNeuralNetwork(input_size, hidden_size, output_size)\n",
    "new_model.load_state_dict(saved_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T09:20:50.581486Z",
     "start_time": "2019-03-17T09:20:50.577687Z"
    }
   },
   "source": [
    "### 4.1 l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T06:01:00.306242Z",
     "start_time": "2019-03-18T06:01:00.303820Z"
    }
   },
   "source": [
    "# 作业 4 \n",
    "思考正则项的作用，将l2_norm的值分别设置为0.01和1, 训练模型，观察和不使用l2_norm的结果之间的差异。\n",
    "\n",
    "Hint：we could minimize the regularization term by using $weight\\_decay$ in **SGD optimizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.595671Z",
     "start_time": "2019-03-19T15:08:11.585338Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# set hyper parameters and declare model, optimizer\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.606812Z",
     "start_time": "2019-03-19T15:08:11.597484Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# train and evaluate model\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.595671Z",
     "start_time": "2019-03-19T15:08:11.585338Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# set hyper parameters\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.606812Z",
     "start_time": "2019-03-19T15:08:11.597484Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# train and evaluate model\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 dropout\n",
    "Overfitting is a serious problem in large networks. And large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training.\n",
    "\n",
    "Click on this [link](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T06:01:00.306242Z",
     "start_time": "2019-03-18T06:01:00.303820Z"
    }
   },
   "source": [
    "# 作业 5 \n",
    "思考dropout的作用，使用dropout训练模型，观察和不使用dropout的结果之间的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.595671Z",
     "start_time": "2019-03-19T15:08:11.585338Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# set hyper parameters and declare model, optimizer\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Set dropout to True and probability = 0.5\n",
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.606812Z",
     "start_time": "2019-03-19T15:08:11.597484Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# train and evaluate model\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T11:22:10.949023Z",
     "start_time": "2019-03-17T11:22:10.947165Z"
    }
   },
   "source": [
    "### 4.3 batch normalization\n",
    "Batch normalization is a technique for improving the performance and stability of artificial neural networks\n",
    "\n",
    "\\begin{equation}\n",
    "    y=\\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}} * \\gamma + \\beta, \n",
    "\\end{equation}\n",
    "\n",
    "$\\gamma$ and $\\beta$ are learnable parameters\n",
    "\n",
    "Click on this [link](https://arxiv.org/abs/1502.03167) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T06:01:00.306242Z",
     "start_time": "2019-03-18T06:01:00.303820Z"
    }
   },
   "source": [
    "# 作业 6 \n",
    "思考batch normalization的作用，使用batch normalization训练模型，观察和不使用batch normalization的结果之间的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.595671Z",
     "start_time": "2019-03-19T15:08:11.585338Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# set hyper parameters and declare model, optimizer\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Set batch normalization to True \n",
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.606812Z",
     "start_time": "2019-03-19T15:08:11.597484Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# train and evaluate model\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 data augmentation\n",
    "PyTorch provides many transformation methods for data augmentation.\n",
    "\n",
    "Click on this [link](https://pytorch.org/docs/stable/torchvision/transforms.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业 7\n",
    "思考data augmentation的作用，使用data augmentation训练模型（具体transform方式不限），观察和不使用data augmentation的结果之间的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:12:16.365656Z",
     "start_time": "2019-03-19T15:12:16.356538Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# use data augmentation\n",
    "# reload train_loader with transform\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.595671Z",
     "start_time": "2019-03-19T15:08:11.585338Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# set hyper parameters and declare model, optimizer\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T15:08:11.606812Z",
     "start_time": "2019-03-19T15:08:11.597484Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# train and evaluate model\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient explosion and vanishing\n",
    "\n",
    "For plotting how gradients change, you need to set **get_grad=True** in **fit function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业 8\n",
    "调整超参数或改变初始化方式，分别产生梯度爆炸和梯度消失两种情况，观察它们与正常情况下梯度的变化曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:27.375778Z",
     "start_time": "2019-03-21T14:22:27.360954Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# normal gradient\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:27.375778Z",
     "start_time": "2019-03-21T14:22:27.360954Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# gradient explosion\n",
    "# code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:22:27.375778Z",
     "start_time": "2019-03-21T14:22:27.360954Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# gradient vanishing\n",
    "# code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
