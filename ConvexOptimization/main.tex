\documentclass[logo,reportComp]{thesis}
\usepackage[cpp,pseudo,linenum,optidef]{mypackage}

\title{最优化理论期末报告}
\subtitle{}
\school{数据科学与计算机学院}
\author{陈鸿峥}
\classname{17大数据与人工智能}
\stunum{17341015}
\headercontext{最优化理论期末报告}
\lstset{language=python}

\begin{document}

\maketitle

\section{问题一}
\subsection{问题描述}
考虑线性测量$\vb=A\vx+\ve$，其中$\vb$为$50$维的测量值，$A$为$50\times 100$维的测量矩阵，$\vx$为$100$维的未知稀疏向量且稀疏度为$5$，$\ve$为$50$维的测量噪声。
从$\vb$与$A$中恢复$\vx$的一范数规范化，最小二乘模型如下：
\[\min\lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vx}_1}\]
其中$p$为非负的正则化参数。

请设计下述算法求解该问题：
\begin{enumerate}
    \item 邻近点梯度下降法
    \item 交替方向乘子法
    \item 次梯度法
\end{enumerate}

在实验中，设$\vx$的真值中的非零元素服从均值为$0$方差为$1$的高斯分布，$A$中的元素服从均值为$0$方差为$1$的高斯分布，$\ve$中的元素服从均值为$0$方差为$0.1$的高斯分布。
对于每种算法，请给出每步计算结果与真值的距离以及每步计算结果与最优解的距离。
此外，请讨论正则化参数$p$对计算结果的影响。

\subsection{算法设计}
\subsubsection{邻近点梯度下降法}
设
\[\begin{cases}
s(\vx):=\frac{1}{2}\norm{A\vx-\vb}_2^2\\
r(\vx):=p\norm{\vx}_1
\end{cases}\]
其中，$A\in\rr^{m\times n}$，$\vx\in\rr^n$，$\vb,\ve\in\rr^m$（在本题中$m=50,n=100$），则原式
\[f(\vx):=\lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vx}_1}=s(\vx)+r(\vx)\]
先求$r(\vx)$的邻近点投影
\begin{equation}
\label{equ:prox}
\opprox \hat{\vx}=\argmin_\vx\lrp{p\norm{\vx}_1+\frac{1}{2\alpha}\norm{\vx-\hat{\vx}}^2}
\end{equation}
对\ref{equ:prox}式右侧展开有
\begin{equation}
\label{equ:prox2}
\argmin_\vx\lrp{p\sum_{i=1}^n|x_i|+\frac{1}{2\alpha}\sum_{i=1}^n(x_i-\hat{x}_i)^2}
\end{equation}
注意到(\ref{equ:prox2})式对于下标$i$相互独立，故要求\ref{equ:prox2}的最小值，等价于对每一个下标$i$求最小值后求和，即
\begin{equation}
\label{equ:prox3}
\argmin_{x_i}\lrp{p|x_i|+\frac{1}{2\alpha}(x_i-\hat{x}_i)^2},\;\forall i
\end{equation}
由不可微函数的极值判断条件有
\begin{equation}
0\in\partial_{x_i}p|x_i|+\frac{1}{\alpha}(x_i-\hat{x}_i)
\end{equation}
对$x_i$进行分类讨论
\begin{itemize}
	\item 若$x_i>0$，则$|x_i|$对于$|x_i|$可微，即$\partial_{x_i}|x_i|=1$，有
	\[p+\frac{1}{\alpha}(x_i-\hat{x}_i)=0\]
	整理得
	\[x_i=\hat{x}_i-\alpha p\]
	由于$x_i>0$，故$\hat{x}_i-\alpha p>0$，即$\hat{x}_i>\alpha p$
	\item 若$x_i<0$，则$|x_i|$对于$|x_i|$可微，即$\partial_{x_i}|x_i|=-1$，有
	\[-p+\frac{1}{\alpha}(x_i-\hat{x}_i)=0\]
	整理得
	\[x_i=\hat{x}_i+\alpha p\]
	由于$x_i<0$，故$\hat{x}_i+\alpha p<0$，即$\hat{x}_i<-\alpha p$
	\item 若$x_i=0$，则$|x_i|$对于$|x_i|$不可微，需要求次梯度，$\partial_{x_i}|x_i|=[-1,1]$，即
	\[0\in\left[-p-\frac{\hat{x}_i}{\alpha},p-\frac{\hat{x}_i}{\alpha}\right]\]
	那么，需要满足
	\[\begin{cases}
	p-\dfrac{\hat{x}_i}{\alpha}\geq 0\\
	-p-\dfrac{\hat{x}_i}{\alpha}\leq 0
	\end{cases}\]
	推得
	\[\hat{x}_i\in[-\alpha p,\alpha p]\]
\end{itemize}
综上，有
\begin{equation}
\label{equ:soft-thresholding}
x_i=\begin{cases}
\hat{x}_i+\alpha p & \hat{x}_i<-\alpha p\\
0 & \hat{x}_i\in[-\alpha p,\alpha p]\\
\hat{x}_i-\alpha p & \hat{x}_i>\alpha p
\end{cases}
\end{equation}
可以得到图\ref{fig:soft-thresholding}的软门限图
\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{fig/soft-thresholding.pdf}
\caption{关于$x_i$与$\hat{x}_i$的软门限图}
\label{fig:soft-thresholding}
\end{figure}

进一步，得到邻近点梯度下降法的迭代式如下
\begin{equation}
\begin{cases}
\iter{\vx}{k+\frac{1}{2}}=\iter{\vx}{k}-\alpha\nabla s(\iter{\vx}{k})=\iter{\vx}{k}-\alpha A^T(A\iter{\vx}{k}-\vb)\\
\iter{\vx}{k+1}=\opprox \iter{\vx}{k+\frac{1}{2}}=\argmin_\vx\lrp{p\norm{\vx}_1+\dfrac{1}{2\alpha}\norm{\vx-\iter{\vx}{k+\frac{1}{2}}}_2^2}
\end{cases}
\end{equation}
其中，$\iter{x}{k+\frac{1}{2}}$可直接计算，$\iter{x}{k+1}$可由(\ref{equ:soft-thresholding})式求得。

\subsubsection{交替方向乘子法}
对原问题进行变形，等价于下述约束问题
\begin{mini*}
{}{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vy}_1}{}{}
\addConstraint{\vx-\vy}{=0}
\end{mini*}

构造增广拉格朗日函数
\begin{equation}
\label{equ:clagrange}
L_c(\vx,\vy,\vv)=\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vy}_1+\vv^\T(\vx-\vy)+\frac{c}{2}\norm{\vx-\vy}_2^2
\end{equation}

可以得到交替方向乘子法的迭代格式
\begin{equation}
\label{equ:admm1}
\begin{cases}
\iter{\vx}{k+1}=\argmin_\vx L_c(\vx,\iter{\vy}{k},\iter{\vv}{k})\\
\iter{\vy}{k+1}=\argmin_\vy L_c(\iter{\vx}{k+1},\vy,\iter{\vv}{k})\\
\iter{\vv}{k+1}=\iter{\vv}{k}+c(\iter{\vx}{k+1}-\iter{\vy}{k+1})
\end{cases}
\end{equation}

对$\iter{\vx}{k+1}$展开并配方，并将非主元项忽略，可求得(\ref{equ:admm1})与下面的式子等价
\begin{subequations}
\begin{numcases}{}
\iter{\vx}{k+1}=\argmin_\vx \lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+\frac{c}{2}\norm{\vx-\iter{\vy}{k}+\frac{\iter{\vv}{k}}{c}}_2^2}\label{equ:admm21}\\
\iter{\vy}{k+1}=\argmin_\vy \lrp{p\norm{\vy}_1+\frac{c}{2}\norm{\iter{\vx}{k+1}-\vy+\frac{\iter{\vv}{k}}{c}}_2^2}\label{equ:admm22}\\
\iter{\vv}{k+1}=\iter{\vv}{k}+c(\iter{\vx}{k+1}-\iter{\vy}{k+1})\label{equ:admm23}
\end{numcases}
\end{subequations}

对于(\ref{equ:admm21})式，可直接通过求梯度的方法得到显式解，即
\[\iter{\vx}{k+1}=(A^\T A+cI)^{-1}(A^\T\vb+c\iter{\vy}{k}-\iter{\vv}{k})\]
对于(\ref{equ:admm22})式，由于涉及一范数，故需要求次微分，类似(\ref{equ:soft-thresholding})式的方法，设$z_i=\iter{x}{k+1}_i+\dfrac{\iter{v}{k}_i}{c}$，可得到
\[y_i=\begin{cases}
z_i-\dfrac{p}{c} & z_i>\dfrac{p}{c}\\
0 & z_i\in\lrs{-\dfrac{p}{c},\dfrac{p}{c}}\\
z_i+\dfrac{p}{c} & z_i<-\dfrac{p}{c}
\end{cases}\]

进而可以通过(\ref{equ:admm21})(\ref{equ:admm22})(\ref{equ:admm23})式迭代求解。

\subsubsection{次梯度法}
对原式直接求次梯度有
\begin{equation}
\partial f(\vx)=A^\T(A\vx-\vb)+p\partial\norm{\vx}_1
\end{equation}
其中
\[(\partial\norm{\vx}_1)_i=\partial|x_i|=
\begin{cases}
1 & x_i>0\\
[-1,1] & x_i=0\\
-1 & x_i<0
\end{cases}\]

进而可以直接得到次梯度法的迭代格式
\begin{equation}
\iter{\vx}{k+1}=\iter{\vx}{k}-\alpha\partial f(\iter{\vx}{k})
\end{equation}

\subsection{数值实验}
采用Python进行编程\footnote{使用numpy进行数值计算，matplotlib进行画图}，对上述三种方法进行迭代计算，直到精度达到$10^{-8}$。

完整代码请见\verb'p1.py'文件，这里只截取核心代码部分。

邻近点梯度法如下，设超参数$\alpha=10^{-3}$。
\begin{lstlisting}
def soft_thresholding(x,offset):
	if x < (-1)*offset:
		return x + offset
	elif x > offset:
		return x - offset
	else:
		return 0

def prox(xk_old,offset):
	xk_new = np.zeros(xk_old.size)
	for i in range(xk_old.size):
		xk_new[i] = soft_thresholding(xk_old[i],offset)
	return xk_new

def proxgrad(res):
	t = 0
	xk = np.zeros(n)
	while True:
		xhat = xk - alpha * np.dot(A.T, np.dot(A, xk) - b)
		xk_new = prox(xhat, alpha * p)
		if np.linalg.norm(xk_new - xk, ord=2) < acc:
			break
		res.append(xk_new)
		xk = xk_new.copy()
		t += 1
	print(t)
	return xk
\end{lstlisting}

交替方向乘子法如下，设超参数$c=10^{-3}$。
\begin{lstlisting}
def admm(res):
	t = 0
	xk = np.zeros(n)
	yk = np.zeros(n)
	vk = np.zeros(n)
	while True:
		xk_new = np.dot(
			np.linalg.inv(np.dot(A.T, A) + c * np.eye(n,n)),
			np.dot(A.T, b) + c * yk - vk)
		yk_new = prox(xk_new + vk / c, p / c)
		vk_new = vk + c * (xk_new - yk_new)
		if np.linalg.norm(xk_new - xk, ord=2) < acc:
			break
		res.append(xk_new)
		xk = xk_new.copy()
		yk = yk_new.copy()
		vk = vk_new.copy()
		t += 1
	print(t)
	return xk
\end{lstlisting}

次梯度法如下，采用递减步长$\iter{\alpha}{k+1}=\dfrac{\iter{\alpha}{k}}{k+1}$。
\begin{lstlisting}
def subgrad(res):
	xk = np.zeros(n)
	t = 0
	while True:
		pdx = np.zeros(xk.size)
		alphak = alpha / (t + 1) # remember to decay the step
		for i in range(xk.size):
			if xk[i] != 0:
				pdx[i] = 1 if xk[i] > 0 else -1
			else: # pick a random float from [-1,1]
				pdx[i] = 2 * np.random.random() - 1
		pdf = np.dot(A.T, np.dot(A,xk) - b) + pdx
		xk_new = xk - alphak * pdf
		if np.linalg.norm(xk_new - xk, ord=2) < acc:
			break
		res.append(xk_new)
		xk = xk_new.copy()
		t += 1
	print(t)
	return xk
\end{lstlisting}

\subsection{结果分析}
% 横轴迭代次数$k$，纵轴对数坐标，\norm{\iter{x}{k}-\iter{x}{t}}, \norm{\iter{x}{k}-x^\star}
记最优解$\vx^\star$为最后一次迭代获得的$\vx$值，真值为$\vx_{\text{true}}$。

运行结果如图\ref{fig:proxgrad}、图\ref{fig:admm}和图\ref{fig:subgrad}所示。

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/proxgrad.eps}
\caption{邻近点梯度下降法}
\label{fig:proxgrad}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{fig/admm.eps}
\caption{交替方向乘子法}
\label{fig:admm}
\end{figure}
\end{minipage}
\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{fig/subgrad.eps}
\caption{次梯度法}
\label{fig:subgrad}
\end{figure}

由实际的运行时间可以得出，交替方向乘子法达到目标精度需要的时间最短，邻近点梯度下降法次之，次梯度法所需的时间最长。
从迭代的次数也可以看出，邻近点梯度下降法需要$226545$轮迭代，交替方向乘子法需要$16249$轮迭代，次梯度法需要$3047223$轮迭代。
（此实验仅仅展示了一种情况，实际上迭代次数与超参数的选择有关，见下面的实验。）

不仅如此，从上述三幅图中还可以看出，交替方向乘子法和邻近点梯度下降法都能达到很高的精度（即与真值相差不多），但次梯度法迭代次数多且精度非常低，是一种非常糟糕的算法。

关于正则化参数$p$对最终结果的影响可见图\ref{fig:pprox}、图\ref{fig:padmm}和图\ref{fig:psubgrad}。
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/pprox.png}
\caption{邻近点梯度下降法}
\label{fig:pprox}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/padmm.png}
\caption{交替方向乘子法}
\label{fig:padmm}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/psubgrad.png}
\caption{次梯度法}
\label{fig:psubgrad}
\end{figure}

从图中可以看出，当正则化参数$p$对迭代次数和收敛精度都有一定的影响。
对于邻近点梯度法，$p$取较大的值时，收敛速度比较快，且精度也较高。
对于交替方向乘子法，$p$取较小的值时，收敛速度较快，但精度差异没有邻近点梯度法那么明显。
而对于次梯度法，$p$对收敛速度和收敛精度几乎没有影响。

\section{问题二}
\subsection{问题描述}
请设计下述算法，求解MNIST数据集上的Logistic Regression问题：
\begin{enumerate}
    \item 梯度下降法
    \item 随机梯度法
\end{enumerate}

对于每种算法，请给出每步计算结果与最优解的距离以及每步计算结果在测试集上所对应的分类精度。
此外，请讨论随机梯度法中Mini Batch大小对计算结果的影响。

可参考：\url{http://deeplearning.net/tutorial/logreg.html}


\subsection{算法设计}
由于MNIST手写数据集为多分类问题，需要将输入的图片映射到对应的$10$个数字上，故采用多类别的logistic回归，即softmax回归进行求解。
% multiclass logistic regression. Other common names for it include softmax regression and multinomial regression

设训练集$\{(\vx_i,y_i)\}_{i=1}^M$，其中$M$为训练集数目，${|\mathcal{C}|}$为类别数目，$\vx_i\in\rr^{n+1}$为输入样本，$y_i\in\{0,1,\ldots,|\mathcal{C}|-1\}$为对应的标签。
在本问题中${|\mathcal{C}|}=10$，$M=60000$，$n=28\times 28=784$，这里已经将二维的图片数组展平为一维的向量。

输入$\vx_i$属于第$y_i$类的条件概率可用softmax函数进行计算
\begin{equation}
p(y_i\mid \vx_i;W)=\frac{\ee^{\vw_{y_i}^\T\vx_i}}{\sum_{c=0}^{|\mathcal{C}|-1}\ee^{\vw_c^\T\vx_i}}
\end{equation}
其中，
\[W=\bmat{\vw_0^\T\\\vw_1^\T\\\vdots\\\vw_{|\mathcal{C}|-1}^\T}\in\rr^{{|\mathcal{C}|}\times (n+1)}\]
为需要训练的参数，$\vw_i\in\rr^{n+1}$为每一个类别对应的权重向量\footnote{注意到$\vw^\T\vx+b=\bmat{\vw^\T & b}\bmat{\vx\\1}$，故将最后的偏置项归入$\vw$中，并在$\vx$中添加为$1$的一个维度，进而偏置量和权重可以一起运算，$\vw_c^\T$和$\vx_i$都有$n+1$个维度。}。

从而得到数据集的似然函数为
\[\mL(Y\mid X;W)=\prod_{i=1}^m p(y_i\mid\vx_i;W)\]
对应的对数似然函数为
\[\ln\mL(Y\mid X;W)=\sum_{i=1}^m\ln p(y_i\mid\vx_i;W)\]
极大化对数似然函数，相当于极小化负对数似然函数(negative log likelihood, NLL)，故得到最优化问题
\begin{equation}
\min_W \;\ell(W):=-\ln\mL(Y\mid X;W)
\end{equation}

对$\ln p(y_i\mid\vx_i;W)$求梯度，有
\[\pd{\ln p(y_i\mid\vx_i;W)}{\vw_j}=
\begin{cases}
\vx(1-p(y_i\mid\vx_i;W)) & y_i=j\\
\vx p(y_i\mid\vx_i;W) & y_i\ne j
\end{cases},\;j\in\{0,1,\ldots,|\mathcal{C}|-1\}\]
进而
\begin{equation}
\nabla_{\vw_j}\ell(W)=-\sum_{i=1}^m\lrs{\vx_i\lrp{\vone(y_i=j)-p(y_i\mid\vx_i;W)}}
\end{equation}
其中，$\vone(\cdot)$为示性函数，当输入为真时返回$1$，输入为假时返回$0$。

最终得到梯度下降的表达式
\begin{equation}
\label{equ:gd}
\iter{\vw}{k+1}_j=\iter{\vw}{k}_j-\frac{\iter{\alpha}{k}}{m}\nabla_{\vw_j}\ell(W)
\end{equation}
其中，$m$为小批量大小。
当$m$选取不同值时，(\ref{equ:gd})式为不同的优化方法：
\begin{itemize}
\item 当$m=M$时，梯度下降法，每次选取\textbf{所有样本}用于更新权重
\item 当$m=1$时，随机梯度下降法，每次选取\textbf{一个样本}用于更新权重
\item 当$m\in[2,M-1]$时，小批量梯度下降法，每次只选取\textbf{部分样本}用于更新权重
\end{itemize}

\subsection{数值实验}
MNIST输入数据以numpy压缩格式存储，从\url{https://s3.amazonaws.com/img-datasets/mnist.npz}获取。

核心代码如下，完整代码请见\verb'p2.py'。
\begin{lstlisting}
\end{lstlisting}

\subsection{结果分析}


% \section{参考资料}
% logistic回归，梯度下降法，牛顿法/IRLS算法 - RookieJ的文章 - 知乎 https://zhuanlan.zhihu.com/p/67842740
% http://yann.lecun.com/exdb/mnist/

\end{document}

% 实验要求：
% 1、撰写并提交完整的实验报告，包括问题描述、算法设计、数值实验、结果分析。
% 2、编写并提交完整的源代码，建议用 MATLAB 实现。
% 3、2019 年 6 月 14 日（星期五）前提交至optimization_2019@163.com，要回信！