\documentclass[logo,reportComp]{thesis}
\usepackage[cpp,pseudo,linenum,optidef]{mypackage}
\usepackage{cases,subeqnarray}

\title{最优化理论期末报告}
\subtitle{}
\school{数据科学与计算机学院}
\author{陈鸿峥}
\classname{17大数据与人工智能}
\stunum{17341015}
\headercontext{最优化理论期末报告}
\lstset{language=python}

\begin{document}

\maketitle

\section{问题一}
\subsection{问题描述}
考虑线性测量$\vb=A\vx+\ve$，其中$\vb$为$50$维的测量值，$A$为$50\times 100$维的测量矩阵，$\vx$为$100$维的未知稀疏向量且稀疏度为$5$，$\ve$为$50$维的测量噪声。
从$\vb$与$A$中恢复$\vx$的一范数规范化，最小二乘模型如下：
\[\min\lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vx}_1}\]
其中$p$为非负的正则化参数。

请设计下述算法求解该问题：
\begin{enumerate}
    \item 邻近点梯度下降法
    \item 交替方向乘子法
    \item 次梯度法
\end{enumerate}

在实验中，设$\vx$的真值中的非零元素服从均值为$0$方差为$1$的高斯分布，$A$中的元素服从均值为$0$方差为$1$的高斯分布，$\ve$中的元素服从均值为$0$方差为$0.1$的高斯分布。
对于每种算法，请给出每步计算结果与真值的距离以及每步计算结果与最优解的距离。
此外，请讨论正则化参数$p$对计算结果的影响。

\subsection{算法设计}
\subsubsection{邻近点梯度下降法}
设
\[\begin{cases}
s(\vx):=\frac{1}{2}\norm{A\vx-\vb}_2^2\\
r(\vx):=p\norm{\vx}_1
\end{cases}\]
其中，$A\in\rr^{m\times n}$，$\vx\in\rr^n$，$\vb,\ve\in\rr^m$（在本题中$m=50,n=100$），则原式
\[f(\vx):=\lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vx}_1}=s(\vx)+r(\vx)\]
先求$r(\vx)$的邻近点投影
\begin{equation}
\label{equ:prox}
\opprox \hat{\vx}=\argmin_\vx\lrp{p\norm{\vx}_1+\frac{1}{2\alpha}\norm{\vx-\hat{\vx}}^2}
\end{equation}
对\ref{equ:prox}式右侧展开有
\begin{equation}
\label{equ:prox2}
\argmin_\vx\lrp{p\sum_{i=1}^n|x_i|+\frac{1}{2\alpha}\sum_{i=1}^n(x_i-\hat{x}_i)^2}
\end{equation}
注意到(\ref{equ:prox2})式对于下标$i$相互独立，故要求\ref{equ:prox2}的最小值，等价于对每一个下标$i$求最小值后求和，即
\begin{equation}
\label{equ:prox3}
\argmin_{x_i}\lrp{p|x_i|+\frac{1}{2\alpha}(x_i-\hat{x}_i)^2},\;\forall i
\end{equation}
由不可微函数的极值判断条件有
\begin{equation}
0\in\partial_{x_i}p|x_i|+\frac{1}{\alpha}(x_i-\hat{x}_i)
\end{equation}
对$x_i$进行分类讨论
\begin{itemize}
	\item 若$x_i>0$，则$|x_i|$对于$|x_i|$可微，即$\partial_{x_i}|x_i|=1$，有
	\[p+\frac{1}{\alpha}(x_i-\hat{x}_i)=0\]
	整理得
	\[x_i=\hat{x}_i-\alpha p\]
	由于$x_i>0$，故$\hat{x}_i-\alpha p>0$，即$\hat{x}_i>\alpha p$
	\item 若$x_i<0$，则$|x_i|$对于$|x_i|$可微，即$\partial_{x_i}|x_i|=-1$，有
	\[-p+\frac{1}{\alpha}(x_i-\hat{x}_i)=0\]
	整理得
	\[x_i=\hat{x}_i+\alpha p\]
	由于$x_i<0$，故$\hat{x}_i+\alpha p<0$，即$\hat{x}_i<-\alpha p$
	\item 若$x_i=0$，则$|x_i|$对于$|x_i|$不可微，需要求次梯度，$\partial_{x_i}|x_i|=[-1,1]$，即
	\[0\in\left[-p-\frac{\hat{x}_i}{\alpha},p-\frac{\hat{x}_i}{\alpha}\right]\]
	那么，需要满足
	\[\begin{cases}
	p-\dfrac{\hat{x}_i}{\alpha}\geq 0\\
	-p-\dfrac{\hat{x}_i}{\alpha}\leq 0
	\end{cases}\]
	推得
	\[\hat{x}_i\in[-\alpha p,\alpha p]\]
\end{itemize}
综上，有
\begin{equation}
\label{equ:soft-thresholding}
x_i=\begin{cases}
\hat{x}_i+\alpha p & \hat{x}_i<-\alpha p\\
0 & \hat{x}_i\in[-\alpha p,\alpha p]\\
\hat{x}_i-\alpha p & \hat{x}_i>\alpha p
\end{cases}
\end{equation}
可以得到图\ref{fig:soft-thresholding}的软门限图
\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{fig/soft-thresholding.pdf}
\caption{关于$x_i$与$\hat{x}_i$的软门限图}
\label{fig:soft-thresholding}
\end{figure}

进一步，得到邻近点梯度下降法的迭代式如下
\begin{equation}
\begin{cases}
\iter{\vx}{k+\frac{1}{2}}=\iter{\vx}{k}-\alpha\nabla s(\iter{\vx}{k})=\iter{\vx}{k}-\alpha A^T(A\iter{\vx}{k}-\vb)\\
\iter{\vx}{k+1}=\opprox \iter{\vx}{k+\frac{1}{2}}=\argmin_\vx\lrp{p\norm{\vx}_1+\dfrac{1}{2\alpha}\norm{\vx-\iter{\vx}{k+\frac{1}{2}}}_2^2}
\end{cases}
\end{equation}
其中，$\iter{x}{k+\frac{1}{2}}$可直接计算，$\iter{x}{k+1}$可由(\ref{equ:soft-thresholding})式求得。

\subsubsection{交替方向乘子法}
对原问题进行变形，等价于下述约束问题
\begin{mini*}
{}{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vy}_1}{}{}
\addConstraint{\vx-\vy}{=0}
\end{mini*}

构造增广拉格朗日函数
\begin{equation}
\label{equ:clagrange}
L_c(\vx,\vy,\vv)=\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vy}_1+\vv^\T(\vx-\vy)+\frac{c}{2}\norm{\vx-\vy}_2^2
\end{equation}

可以得到交替方向乘子法的迭代格式
\begin{equation}
\label{equ:admm1}
\begin{cases}
\iter{\vx}{k+1}=\argmin_\vx L_c(\vx,\iter{\vy}{k},\iter{\vv}{k})\\
\iter{\vy}{k+1}=\argmin_\vy L_c(\iter{\vx}{k+1},\vy,\iter{\vv}{k})\\
\iter{\vv}{k+1}=\iter{\vv}{k}+c(\iter{\vx}{k+1}-\iter{\vy}{k+1})
\end{cases}
\end{equation}

对$\iter{\vx}{k+1}$展开并配方，并将非主元项忽略，可求得(\ref{equ:admm1})与下面的式子等价
\begin{subequations}
\begin{numcases}{}
\iter{\vx}{k+1}=\argmin_\vx \lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+\frac{c}{2}\norm{\vx-\iter{\vy}{k}+\frac{\iter{\vv}{k}}{c}}_2^2}\label{equ:admm21}\\
\iter{\vy}{k+1}=\argmin_\vy \lrp{p\norm{\vy}_1+\frac{c}{2}\norm{\iter{\vx}{k+1}-\vy+\frac{\iter{\vv}{k}}{c}}_2^2}\label{equ:admm22}\\
\iter{\vv}{k+1}=\iter{\vv}{k}+c(\iter{\vx}{k+1}-\iter{\vy}{k+1})\label{equ:admm23}
\end{numcases}
\end{subequations}

对于(\ref{equ:admm21})式，可直接通过求梯度的方法得到显式解，即
\[\iter{\vx}{k+1}=(A^\T A+cI)^{-1}(A^\T\vb+c\iter{\vy}{k}-\iter{\vv}{k})\]
对于(\ref{equ:admm22})式，由于涉及一范数，故需要求次微分，类似(\ref{equ:soft-thresholding})式的方法，设$z_i=\iter{x}{k+1}_i+\dfrac{\iter{v}{k}_i}{c}$，可得到
\[y_i=\begin{cases}
z_i-\dfrac{p}{c} & z_i>\dfrac{p}{c}\\
0 & z_i\in\lrs{-\dfrac{p}{c},\dfrac{p}{c}}\\
z_i+\dfrac{p}{c} & z_i<-\dfrac{p}{c}
\end{cases}\]

进而可以通过(\ref{equ:admm21})(\ref{equ:admm22})(\ref{equ:admm23})式迭代求解。

\subsubsection{次梯度法}
对原式直接求次梯度有
\begin{equation}
\partial f(\vx)=A^\T(A\vx-\vb)+p\partial\norm{\vx}_1
\end{equation}
其中
\[(\partial\norm{\vx}_1)_i=\partial|x_i|=
\begin{cases}
1 & x_i>0\\
[-1,1] & x_i=0\\
-1 & x_i<0
\end{cases}\]

进而可以直接得到次梯度法的迭代格式
\begin{equation}
\iter{\vx}{k+1}=\iter{\vx}{k}-\alpha\partial f(\iter{\vx}{k})
\end{equation}

\subsection{数值实验}
采用Python进行编程\footnote{使用numpy进行数值计算，matplotlib进行画图}，对上述三种方法进行迭代计算，直到精度达到$10^{-8}$。

完整代码请见\verb'p1.py'文件，这里只截取核心代码部分。

邻近点梯度法如下，设超参数$\alpha=10^{-3}$。
\begin{lstlisting}
def soft_thresholding(x,offset):
	if x < (-1)*offset:
		return x + offset
	elif x > offset:
		return x - offset
	else:
		return 0

def prox(xk_old,offset):
	xk_new = np.zeros(xk_old.size)
	for i in range(xk_old.size):
		xk_new[i] = soft_thresholding(xk_old[i],offset)
	return xk_new

def proxgrad(x0):
	t = 0
	xk = x0.copy()
	while True:
		xhat = xk - alpha * np.dot(A.T, np.dot(A, xk) - b)
		xk_new = prox(xhat, alpha * p)
		if np.linalg.norm(xk_new - xk, ord=2) < acc:
			break
		xk = xk_new.copy()
		t += 1
	return xk
\end{lstlisting}

交替方向乘子法如下，设超参数$c=10^{-3}$。
\begin{lstlisting}
def admm(x0,y0,v0):
	t = 0
	xk = x0.copy()
	yk = y0.copy()
	vk = v0.copy()
	while True:
		xk_new = np.dot(
			np.linalg.inv(np.dot(A.T, A) + c * np.eye(n,n)),
			np.dot(A.T, b) + c * yk - vk)
		yk_new = prox(xk_new + vk / c, p / c)
		vk_new = vk + c * (xk_new - yk_new)
		if np.linalg.norm(xk_new - xk, ord=2) < acc:
			break
		xk = xk_new.copy()
		yk = yk_new.copy()
		vk = vk_new.copy()
		t += 1
	return xk
\end{lstlisting}

次梯度法如下。
\begin{lstlisting}
def subgrad(x0):
	xk = x0.copy()
	t = 0
	while True:
		pdx = np.zeros(xk.size)
		alphak = alpha / (t + 1) # remember to decay the step
		for i in range(xk.size):
			if xk[i] != 0:
				pdx[i] = 1 if xk[i] > 0 else -1
			else:
				pdx[i] = 2 * np.random.random() - 1 # pick a random float from [-1,1]
		pdf = np.dot(A.T, np.dot(A,xk) - b) + pdx
		xk_new = xk - alphak * pdf
		if np.linalg.norm(xk_new - xk, ord=2) < acc:
			break
		xk = xk_new.copy()
		t += 1
	return xk
\end{lstlisting}

\subsection{结果分析}
% 横轴迭代次数$k$，纵轴对数坐标，\norm{\iter{x}{k}-\iter{x}{t}}, \norm{\iter{x}{k}-x^\star}
记最优解$\vx^\star$为最后一次迭代获得的$\vx$值，真值为$\vx_{\text{true}}$。
运行时间，交替方向乘子法最短，邻近点梯度下降法次之，次梯度法最长。
邻近点梯度下降法需要226545轮迭代，交替方向乘子法需要16249轮迭代，次梯度法需要3047223轮迭代。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{fig/proxgrad.eps}
\caption{邻近点梯度下降法}
\label{fig:proxgrad}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{fig/admm.eps}
\caption{交替方向乘子法}
\label{fig:admm}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{fig/subgrad.eps}
\caption{次梯度法}
\label{fig:subgrad}
\end{figure}

\section{问题二}
\subsection{问题描述}
请设计下述算法，求解MNIST数据集上的Logistic Regression问题：
\begin{enumerate}
    \item 梯度下降法
    \item 随机梯度法
\end{enumerate}

对于每种算法，请给出每步计算结果与最优解的距离以及每步计算结果在测试集上所对应的分类精度。
此外，请讨论随机梯度法中Mini Batch大小对计算结果的影响。

可参考：\url{http://deeplearning.net/tutorial/logreg.html}


\subsection{算法设计}
由于MNIST数据集为多分类问题，故采用泛化的Logistic回归，即softmax回归进行求解。

设训练集$\{(\vx_i,y_i)\}_{i=1}^M$，其中$M$为训练集数目，${|\mathcal{C}|}$为类别数目（在本例中${|\mathcal{C}|}=10$），$\vx_i\in\rr^n$为输入样本，$y_i\in\{1,2,\ldots,{|\mathcal{C}|}\}$为对应的标签\footnote{为方便起见，在手写体问题中，我们将数字$0$归到类别$10$中。}。

输入$\vx_i$属于第$y_i$类的条件概率可用softmax函数进行计算
\begin{equation}
p(y_i\mid \vx;W)=\frac{\ee^{\vw_{y_i}^\T\vx}}{\sum_{c=1}^{|\mathcal{C}|}\ee^{\vw_c^\T\vx}}
\end{equation}
其中，
\[W=\bmat{\vw_1^\T\\\vw_2^\T\\\vdots\\\vw_{|\mathcal{C}|}^\T}\in\rr^{{|\mathcal{C}|}\times n}\]
为需要训练的参数，$\vw_i\in\rr^{n \times 1}$为每一个类别对应的权重向量。

从而得到数据集的似然函数为
\[\mL(Y\mid X;W)=\prod_{i=1}^m p(y_i\mid\vx_i;W)\]
对应的对数似然函数为
\[\ln\mL(Y\mid X;W)=\sum_{i=1}^m\ln p(y_i\mid\vx_i;W)\]
极大化对数似然函数，相当于极小化负对数似然函数，故得到最优化问题
\begin{equation}
\min_W \;\ell(W):=-\ln\mL(Y\mid X;W)
\end{equation}

对$\ln p(y_i\mid\vx_i;W)$求梯度，有
\[\pd{\ln p(y_i\mid\vx_i;W)}{\vw_j}=
\begin{cases}
\vx(1-p(y_i\mid\vx_i;W)) & y_i=j\\
\vx p(y_i\mid\vx_i;W) & y_i\ne j
\end{cases},\;j\in\{1,\ldots,|\mathcal{C}|\}\]
进而
\begin{equation}
\nabla_{\vw_j}\ell(W)=-\sum_{i=1}^m\lrs{\vx_i\lrp{\vone(y_i=j)-p(y_i=j\mid\vx_i;W)}}
\end{equation}

最终得到梯度下降的表达式
\begin{equation}
\label{equ:gd}
\iter{\vw}{k+1}_j=\iter{\vw}{k}_j-\frac{\iter{\alpha}{k}}{m}\nabla_{\vw_j}\ell(W)
\end{equation}
其中，$m$为小批量大小。
当$m$选取不同值时，(\ref{equ:gd})式为不同的优化方法：
\begin{itemize}
\item 当$m=M$时，梯度下降法，每次选取\textbf{所有样本}用于更新权重
\item 当$m=1$时，随机梯度下降法，每次选取\textbf{一个样本}用于更新权重
\item 当$m\in[2,M-1]$时，小批量梯度下降法，每次只选取\textbf{部分样本}用于更新权重
\end{itemize}

\subsection{数值实验}
代码如下，也已附在附件中。


\subsection{结果分析}


% \section{参考资料}
% logistic回归，梯度下降法，牛顿法/IRLS算法 - RookieJ的文章 - 知乎 https://zhuanlan.zhihu.com/p/67842740

\end{document}

% 实验要求：
% 1、撰写并提交完整的实验报告，包括问题描述、算法设计、数值实验、结果分析。
% 2、编写并提交完整的源代码，建议用 MATLAB 实现。
% 3、2019 年 6 月 14 日（星期五）前提交至optimization_2019@163.com，要回信！