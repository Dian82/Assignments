\documentclass[logo,reportComp]{thesis}
\usepackage[cpp,pseudo,optidef]{mypackage}
\usepackage{cases,subeqnarray}

\title{最优化理论期末报告}
\subtitle{}
\school{数据科学与计算机学院}
\author{陈鸿峥}
\classname{17大数据与人工智能}
\stunum{17341015}
\headercontext{最优化理论期末报告}
\lstset{language=python}

\begin{document}

\maketitle

\section{问题一}
\subsection{问题描述}
考虑线性测量$\vb=A\vx+\ve$，其中$\vb$为$50$维的测量值，$A$为$50\times 100$维的测量矩阵，$\vx$为$100$维的未知稀疏向量且稀疏度为$5$，$\ve$为$50$维的测量噪声。
从$\vb$与$A$中恢复$\vx$的一范数规范化，最小二乘模型如下：
\[\min\lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vx}_1}\]
其中$p$为非负的正则化参数。

请设计下述算法求解该问题：
\begin{enumerate}
    \item 邻近点梯度下降法
    \item 交替方向乘子法
    \item 次梯度法
\end{enumerate}

在实验中，设$\vx$的真值中的非零元素服从均值为$0$方差为$1$的高斯分布，$A$中的元素服从均值为$0$方差为$1$的高斯分布，$\ve$中的元素服从均值为$0$方差为$0.1$的高斯分布。
对于每种算法，请给出每步计算结果与真值的距离以及每步计算结果与最优解的距离。
此外，请讨论正则化参数$p$对计算结果的影响。

\subsection{算法设计}
\subsubsection{邻近点梯度下降法}
设
\[\begin{cases}
s(\vx):=\frac{1}{2}\norm{A\vx-\vb}_2^2\\
r(\vx):=p\norm{\vx}_1
\end{cases}\]
其中，$A\in\rr^m\times\rn$，$\vx,\vb,\ve\in\rn$，则原式
\[f(\vx):=\lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vx}_1}=s(\vx)+r(\vx)\]
先求$r(\vx)$的邻近点投影
\begin{equation}
\label{equ:prox}
\opprox \hat{\vx}=\argmin_\vx\lrp{p\norm{\vx}_1+\frac{1}{2\alpha}\norm{\vx-\hat{\vx}}^2}
\end{equation}
对\ref{equ:prox}式右侧展开有
\begin{equation}
\label{equ:prox2}
\argmin_\vx\lrp{p\sum_{i=1}^n|x_i|+\frac{1}{2\alpha}\sum_{i=1}^n(x_i-\hat{x}_i)^2}
\end{equation}
注意到(\ref{equ:prox2})式对于下标$i$相互独立，故要求\ref{equ:prox2}的最小值，等价于对每一个下标$i$求最小值后求和，即
\begin{equation}
\label{equ:prox3}
\argmin_{x_i}\lrp{p|x_i|+\frac{1}{2\alpha}(x_i-\hat{x}_i)^2},\;\forall i
\end{equation}
由不可微函数的极值判断条件有
\begin{equation}
0\in\partial_{x_i}p|x_i|+\frac{1}{\alpha}(x_i-\hat{x}_i)
\end{equation}
对$x_i$进行分类讨论
\begin{itemize}
	\item 若$x_i>0$，则$|x_i|$对于$|x_i|$可微，即$\partial_{x_i}|x_i|=1$，有
	\[p+\frac{1}{\alpha}(x_i-\hat{x}_i)=0\]
	整理得
	\[x_i=\hat{x}_i-\alpha p\]
	由于$x_i>0$，故$\hat{x}_i-\alpha p>0$，即$\hat{x}_i>\alpha p$
	\item 若$x_i<0$，则$|x_i|$对于$|x_i|$可微，即$\partial_{x_i}|x_i|=-1$，有
	\[-p+\frac{1}{\alpha}(x_i-\hat{x}_i)=0\]
	整理得
	\[x_i=\hat{x}_i+\alpha p\]
	由于$x_i<0$，故$\hat{x}_i+\alpha p<0$，即$\hat{x}_i<-\alpha p$
	\item 若$x_i=0$，则$|x_i|$对于$|x_i|$不可微，需要求次梯度，$\partial_{x_i}|x_i|=[-1,1]$，即
	\[0\in\left[-p-\frac{\hat{x}_i}{\alpha},p-\frac{\hat{x}_i}{\alpha}\right]\]
	那么，需要满足
	\[\begin{cases}
	p-\dfrac{\hat{x}_i}{\alpha}\geq 0\\
	-p-\dfrac{\hat{x}_i}{\alpha}\leq 0
	\end{cases}\]
	推得
	\[\hat{x}_i\in[-\alpha p,\alpha p]\]
\end{itemize}
综上，有
\begin{equation}
\label{equ:soft-thresholding}
x_i=\begin{cases}
\hat{x}_i+\alpha p & \hat{x}_i<-\alpha p\\
0 & \hat{x}_i\in[-\alpha p,\alpha p]\\
\hat{x}_i-\alpha p & \hat{x}_i>\alpha p
\end{cases}
\end{equation}
可以得到图\ref{fig:soft-thresholding}的软门限图
\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{fig/soft-thresholding.pdf}
\caption{关于$x_i$与$\hat{x}_i$的软门限图}
\label{fig:soft-thresholding}
\end{figure}

进一步，得到邻近点梯度下降法的迭代式如下
\begin{equation}
\begin{cases}
\iter{\vx}{k+\frac{1}{2}}=\iter{\vx}{k}-\alpha\nabla s(\iter{\vx}{k})=\iter{\vx}{k}-\alpha A^T(A\iter{\vx}{k}-\vb)\\
\iter{\vx}{k+1}=\opprox \iter{\vx}{k+\frac{1}{2}}=\argmin_\vx\lrp{p\norm{\vx}_1+\dfrac{1}{2\alpha}\norm{\vx-\iter{\vx}{k+\frac{1}{2}}}_2^2}
\end{cases}
\end{equation}
其中，$\iter{x}{k+\frac{1}{2}}$可直接计算，$\iter{x}{k+1}$可由(\ref{equ:soft-thresholding})式求得。

\subsubsection{交替方向乘子法}
对原问题进行变形，等价于下述约束问题
\begin{mini*}
{}{\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vy}_1}{}{}
\addConstraint{\vx-\vy}{=0}
\end{mini*}

构造增广拉格朗日函数
\begin{equation}
\label{equ:clagrange}
L_c(\vx,\vy,\vv)=\frac{1}{2}\norm{A\vx-\vb}_2^2+p\norm{\vy}_1+\vv^\T(\vx-\vy)+\frac{c}{2}\norm{\vx-\vy}_2^2
\end{equation}

可以得到交替方向乘子法的迭代格式
\begin{equation}
\label{equ:admm1}
\begin{cases}
\iter{\vx}{k+1}=\argmin_\vx L_c(\vx,\iter{\vy}{k},\iter{\vv}{k})\\
\iter{\vy}{k+1}=\argmin_\vy L_c(\iter{\vx}{k+1},\vy,\iter{\vv}{k})\\
\iter{\vv}{k+1}=\iter{\vv}{k}+c(\iter{\vx}{k+1}-\iter{\vy}{k+1})
\end{cases}
\end{equation}

对$\iter{\vx}{k+1}$展开并配方，并将非主元项忽略，可求得(\ref{equ:admm1})与下面的式子等价
\begin{subequations}
\begin{numcases}{}
\iter{\vx}{k+1}=\argmin_\vx \lrp{\frac{1}{2}\norm{A\vx-\vb}_2^2+\frac{c}{2}\norm{\vx-\iter{\vy}{k}+\frac{\iter{\vv}{k}}{c}}_2^2}\label{equ:admm21}\\
\iter{\vy}{k+1}=\argmin_\vy \lrp{p\norm{\vy}_1+\frac{c}{2}\norm{\iter{\vx}{k+1}-\vy+\frac{\iter{\vv}{k}}{c}}_2^2}\label{equ:admm22}\\
\iter{\vv}{k+1}=\iter{\vv}{k}+c(\iter{\vx}{k+1}-\iter{\vy}{k+1})\label{equ:admm23}
\end{numcases}
\end{subequations}

对于(\ref{equ:admm21})式，可直接通过求梯度的方法得到显式解，即
\[\iter{\vx}{k+1}=(A^\T A+cI)^{-1}(A^\T\vb+c\iter{\vy}{k}-\iter{\vv}{k})\]
对于(\ref{equ:admm22})式，由于涉及一范数，故需要求次微分，类似(\ref{equ:soft-thresholding})式的方法，设$z_i=\iter{x}{k+1}_i+\dfrac{\iter{v}{k}_i}{c}$，可得到
\[y_i=\begin{cases}
z_i-\dfrac{p}{c} & z_i>\dfrac{p}{c}\\
0 & z_i\in\lrs{-\dfrac{p}{c},\dfrac{p}{c}}\\
z_i+\dfrac{p}{c} & z_i<-\dfrac{p}{c}
\end{cases}\]

进而可以通过(\ref{equ:admm21})(\ref{equ:admm22})(\ref{equ:admm23})式迭代求解。

\subsubsection{次梯度法}
对原式直接求次梯度有
\begin{equation}
\partial f(\vx)=A^\T(A\vx-\vb)+p\partial\norm{\vx}_1
\end{equation}
其中
\[(\partial\norm{\vx}_1)_i=\partial|x_i|=
\begin{cases}
1 & x_i>0\\
[-1,1] & x_i=0\\
-1 & x_i<0
\end{cases}\]

进而可以直接得到次梯度法的迭代格式
\begin{equation}
\iter{\vx}{k+1}=\iter{\vx}{k}-\alpha\partial f(\iter{\vx}{k})
\end{equation}

\subsection{数值实验}
% 精度10^{-8}
代码如下，也已附在附件中。

\subsection{结果分析}
% 横轴迭代次数$k$，纵轴对数坐标，\norm{\iter{x}{k}-\iter{x}{t}}, \norm{\iter{x}{k}-x^\star}

\section{问题二}
\subsection{问题描述}
请设计下述算法，求解MNIST数据集上的Logistic Regression问题：
\begin{enumerate}
    \item 梯度下降法
    \item 随机梯度法
\end{enumerate}

对于每种算法，请给出每步计算结果与最优解的距离以及每步计算结果在测试集上所对应的分类精度。
此外，请讨论随机梯度法中Mini Batch大小对计算结果的影响。

可参考：\url{http://deeplearning.net/tutorial/logreg.html}


\subsection{算法设计}
由于MNIST数据集为多分类问题，故采用泛化的Logistic回归，即softmax回归进行求解。

设训练集$\{(\vx_i,y_i)\}_{i=1}^M$，其中$M$为训练集数目，${|\mathcal{C}|}$为类别数目（在本例中${|\mathcal{C}|}=10$），$\vx_i\in\rr^n$为输入样本，$y_i\in\{1,2,\ldots,{|\mathcal{C}|}\}$为对应的标签\footnote{为方便起见，在手写体问题中，我们将数字$0$归到类别$10$中。}。

输入$\vx$属于第$j$类的条件概率可用softmax函数进行计算
\[p(y=j\mid \vx;W)=\frac{\ee^{\vw_j^\T\vx}}{\sum_{c=1}^{|\mathcal{C}|}\ee^{\vw_j^\T\vx}}\]
其中，
\[W=\bmat{\vw_1^\T\\\vw_2^\T\\\vdots\\\vw_{|\mathcal{C}|}^\T}\in\rr^{{|\mathcal{C}|}\times n}\]
为需要训练的参数，$\vw_i\in\rr^{n \times 1}$为每一个类别对应的权重向量。

可以得到对应的假设函数为
\[h_W(\vx)=\bmat{p(y=1\mid\vx;W)\\p(y=2\mid\vx;W)\\\vdots\\p(y={|\mathcal{C}|}\mid\vx;W)}=\frac{1}{\sum_{c=1}^{|\mathcal{C}|}\ee^{\vw_j^\T\vx}}\bmat{\ee^{\vw_1^\T\vx}\\\ee^{\vw_2^\T\vx}\\\vdots\\\ee^{\vw_{|\mathcal{C}|}^\T\vx}}\]

进而可以得到单一样本的似然函数为
\[\mL(y_i\mid\vx_i;W)=\prod_{c=1}^{|\mathcal{C}|}\vone(y_i=c)p(y_i=j\mid\vx_i;W)\]
其中，$\vone(\cdot)$为示性函数，当输入为真时返回$1$，否则返回$0$。

从而得到数据集的似然函数为
\[\mL(Y\mid X;W)=\prod_{i=1}^m\mL(y_i\mid\vx_i;W)\]
对应的对数似然函数为
\[\ln\mL(Y\mid X;W)=\sum_{i=1}^m\sum_{c=1}^{|\mathcal{C}|}\vone(y_i=j)\ln p(y_i=j\mid\vx;W)\]
极大化对数似然函数，相当于极小化负对数似然函数，故得到最优化问题
\begin{equation}
\min_W \;\ell(W):=-\ln\mL(Y\mid X;W)
\end{equation}

对$\ln p(y=j\mid\vx;W)$求梯度，有
\[\pd{\ln p(y=j\mid\vx;W)}{\vw_i}=
\begin{cases}
\vx(1-p(y=j\mid\vx;W)) & i=j\\
\vx p(y=j\mid\vx;W) & i\ne j
\end{cases}\]
进而
\begin{equation}
\nabla_{\vw_i}\ell(W)=-\sum_{i=1}^m\sum_{j=0}^{|\mathcal{C}|}\lrs{\vx_i\lrp{\vone(y_i=j)-p(y_i=j\mid\vx_i;W)}}
\end{equation}

从而可以得到梯度下降的表达式
\begin{equation}
\label{equ:gd}
\iter{\vw}{k+1}_i=\iter{\vw}{k}_i-\frac{\iter{\alpha}{k}}{m}\nabla_{\vw_i}\ell(W)
\end{equation}
其中，$m$为小批量大小。
当$m$选取不同值时，(\ref{equ:gd})式为不同的优化方法：
\begin{itemize}
\item 当$m=M$时，梯度下降法，每次选取\textbf{所有样本}用于更新权重
\item 当$m=1$时，随机梯度下降法，每次选取\textbf{一个样本}用于更新权重
\item 当$m\in[2,M-1]$时，小批量梯度下降法，每次只选取\textbf{部分样本}用于更新权重
\end{itemize}

\subsection{数值实验}
代码如下，也已附在附件中。


\subsection{结果分析}


% \section{参考资料}
% logistic回归，梯度下降法，牛顿法/IRLS算法 - RookieJ的文章 - 知乎 https://zhuanlan.zhihu.com/p/67842740

\end{document}

% 实验要求：
% 1、撰写并提交完整的实验报告，包括问题描述、算法设计、数值实验、结果分析。
% 2、编写并提交完整的源代码，建议用 MATLAB 实现。
% 3、2019 年 6 月 14 日（星期五）前提交至optimization_2019@163.com，要回信！