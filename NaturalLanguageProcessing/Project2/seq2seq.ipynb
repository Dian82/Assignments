{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq Model with Attention for Chinese-English Machine Translation\n",
    "\n",
    "Some references on seq2seq:\n",
    "* Pytorch, <https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>\n",
    "\n",
    "![Seq2seq Model](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Some tricky things:\n",
    "* Three types of dashes in English:\n",
    "    * The Hypen (-)\n",
    "    * The En-dash (–)\n",
    "    * The Em-dash (—)\n",
    "    * Please refer to [Wikipedia]() or [English Language Help Desk](http://site.uit.no/english/punctuation/hyphen/) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['1  这意味着就业岗位创造不振. 在此次和前两次复苏中 就业增长的反弹都比gdp弱 而且滞后于gdp.', 'in both this recovery and the previous two the rebound in employment growth has been weaker and later than the rebound in gdp growth .']\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing\n",
    "\"\"\"\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.tmp_word_lst = []\n",
    "        self.n_sentences = 0\n",
    "        self.n_words = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def normalizeString(s,lang):\n",
    "        if lang == \"zh\":\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s) # so dirty!\n",
    "            s = re.sub(r\"�\",r\"\",s)\n",
    "            # Test if is Chinese\n",
    "            # https://cloud.tencent.com/developer/article/1499958\n",
    "            punc_pair = [(\"。\",\".\"),(\"！\",\"!\"),(\"？\",\"?\")]\n",
    "            for zh_punc,en_punc in punc_pair:\n",
    "                s = s.replace(zh_punc,en_punc)\n",
    "            s = re.sub(u\"[^a-zA-Z0-9\\u4e00-\\u9fa5.!?]\",u\" \",s)\n",
    "            s = s.lower().strip()\n",
    "        else: # lang == \"en\"\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s)\n",
    "            s = re.sub(r\"([.!?])\",r\" \\1\",s) # add a space between these punctuations\n",
    "            s = re.sub(r\"[^a-zA-Z0-9.!?]+\",r\" \",s) # remove most of the punctuations\n",
    "            s = s.lower().strip()\n",
    "        return s\n",
    "\n",
    "    def addSentence(self,sentence):\n",
    "        self.n_sentences += 1\n",
    "        if self.name == \"zh\": # need to use tools to split words\n",
    "            cut_lst = jieba.lcut(sentence,cut_all=False) # precisely cut\n",
    "            self.tmp_word_lst += filter(\" \".__ne__,cut_lst) # remove all the white spaces\n",
    "        else: # self.name == \"en\"\n",
    "            self.tmp_word_lst += sentence.split()\n",
    "\n",
    "    def processIndex(self):\n",
    "        self.word2count = Counter(self.tmp_word_lst) # {word: count}\n",
    "        self.word2count[\"SOS\"] = self.n_sentences # add begin of sentence (BOS) mark\n",
    "        self.word2count[\"EOS\"] = self.n_sentences # add end of sentence (EOS) mark\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(self.word2count, key=self.word2count.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        self.index2word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        self.word2index = {w: k for k, w in self.index2word.items()}\n",
    "        self.n_word = len(self.index2word)\n",
    "        print('Vocabulary size', self.n_word)\n",
    "        print(list(self.index2word.items())[:10])\n",
    "\n",
    "def preprocess(mode=\"train\",size=10000):\n",
    "    \"\"\"\n",
    "    Source file in Chinese, target file in English\n",
    "\n",
    "    Eg:\n",
    "    巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
    "    PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n",
    "    \"\"\"\n",
    "    data_path = \"data\"\n",
    "    zh_lang_file = \"{}/zh-lang-{}.pkl\".format(data_path,size)\n",
    "    en_lang_file = \"{}/en-lang-{}.pkl\".format(data_path,size)\n",
    "    pairs_file = \"{}/pairs-{}.pkl\".format(data_path,size)\n",
    "    if os.path.isfile(zh_lang_file) and os.path.isfile(en_lang_file) and os.path.isfile(pairs_file):\n",
    "        src_lang = pickle.load(open(zh_lang_file,\"rb\"))\n",
    "        dst_lang = pickle.load(open(en_lang_file,\"rb\"))\n",
    "        pairs = pickle.load(open(pairs_file,\"rb\"))\n",
    "        return src_lang, dst_lang, pairs\n",
    "    else:\n",
    "        src_lang = Lang(\"zh\")\n",
    "        dst_lang = Lang(\"en\")\n",
    "        pairs = []\n",
    "    path = \"dataset_{}\".format(size)\n",
    "    set_size = 8000 if mode == \"train\" else 1000\n",
    "    set_size = set_size * 10 if size == 100000 else set_size\n",
    "    src_file = open(\"{}/{}_source_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "    dst_file = open(\"{}/{}_target_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Reading data...\")\n",
    "    for i,(src_line,dst_line) in enumerate(zip(src_file,dst_file),1):\n",
    "        src = src_line.splitlines()[0]\n",
    "        dst = dst_line.splitlines()[0]\n",
    "        norm_src = Lang.normalizeString(src,\"zh\")\n",
    "        norm_dst = Lang.normalizeString(dst,\"en\")\n",
    "        src_lang.addSentence(norm_src)\n",
    "        dst_lang.addSentence(norm_dst)\n",
    "        # print(i,norm_src,norm_dst,end=\"\\t\")\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Done {}/8000\".format(i))\n",
    "        pairs.append([norm_src,norm_dst])\n",
    "        # sys.exit()\n",
    "\n",
    "    src_lang.processIndex()\n",
    "    dst_lang.processIndex()\n",
    "\n",
    "    print(\"Dumped to file!\")\n",
    "    pickle.dump(src_lang,open(zh_lang_file,\"wb\"))\n",
    "    pickle.dump(dst_lang,open(en_lang_file,\"wb\"))\n",
    "    pickle.dump(pairs,open(pairs_file,\"wb\"))\n",
    "    return src_lang, dst_lang, pairs\n",
    "\n",
    "src_lang, dst_lang, pairs = preprocess(\"train\",10000)\n",
    "print(random.choice(pairs))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}