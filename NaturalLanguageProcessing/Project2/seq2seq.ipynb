{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq Model with Attention for Chinese-English Machine Translation\n",
    "\n",
    "Some references on seq2seq:\n",
    "* Pytorch, *seq2seq translation tutorial*, <https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>\n",
    "* Practical Pytorch, *Batched seq2seq*, <https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb>\n",
    "\n",
    "![Seq2seq Model](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tricky things:\n",
    "* Three types of dashes in English:\n",
    "    * The Hypen (-)\n",
    "    * The En-dash (–)\n",
    "    * The Em-dash (—)\n",
    "    * Please refer to [Wikipedia]() or [English Language Help Desk](http://site.uit.no/english/punctuation/hyphen/) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import logging, pseudologger\n",
    "import pickle\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "\n",
    "flags = Namespace(\n",
    "    checkpoint_path='checkpoint',\n",
    "    log_flag=True,\n",
    "    log_path=\"log\",\n",
    "    data_path=\"data\",\n",
    "    seq_size=32,\n",
    "    batch_size=32,\n",
    "    embedding_size=512, # embedding dimension\n",
    "    lstm_size=512, # hidden dimension\n",
    "    gradients_norm=5, # gradient clipping\n",
    "    top_k=5,\n",
    "    num_epochs=40,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "for path in [flags.checkpoint_path,flags.log_path,flags.data_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "if flags.log_flag:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(level = logging.INFO)\n",
    "    handler = logging.FileHandler(\"{}/seq2seq-01.log\".format(flags.log_path))\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "else:\n",
    "    logger = pseudologger.PseudoLogger()\n",
    "\n",
    "logger.info(str(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.tmp_word_lst = []\n",
    "        self.n_sentences = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def normalizeString(s,lang):\n",
    "        if lang == \"zh\":\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s) # so dirty!\n",
    "            s = re.sub(r\"�\",r\"\",s)\n",
    "            # Test if is Chinese\n",
    "            # https://cloud.tencent.com/developer/article/1499958\n",
    "            punc_pair = [(\"。\",\".\"),(\"！\",\"!\"),(\"？\",\"?\"),(\"，\",\",\")]\n",
    "            for zh_punc,en_punc in punc_pair:\n",
    "                s = s.replace(zh_punc,en_punc)\n",
    "            s = re.sub(u\"[^a-zA-Z0-9\\u4e00-\\u9fa5,.!?]\",u\" \",s)\n",
    "            s = re.sub(r\"\\s+\", r\" \", s)\n",
    "            s = s.lower().strip()\n",
    "        else: # lang == \"en\"\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s)\n",
    "            s = re.sub(r\"([,.!?])\",r\" \\1\",s) # add a space between these punctuations\n",
    "            s = re.sub(r\"[^a-zA-Z0-9,.!?]+\",r\" \",s) # remove most of the punctuations\n",
    "            s = re.sub(r\"\\s+\", r\" \", s)\n",
    "            s = s.lower().strip()\n",
    "        return s\n",
    "\n",
    "    def addSentence(self,sentence):\n",
    "        self.n_sentences += 1\n",
    "        if self.name == \"zh\": # need to use tools to split words\n",
    "            cut_lst = jieba.lcut(sentence,cut_all=False) # precisely cut\n",
    "            self.tmp_word_lst += filter(\" \".__ne__,cut_lst) # remove all the white spaces\n",
    "        else: # self.name == \"en\"\n",
    "            self.tmp_word_lst += sentence.split()\n",
    "\n",
    "    def getSentenceIndex(self,sentence,max_len,pad=True):\n",
    "        \"\"\"\n",
    "        Do after processIndex\n",
    "        \"\"\"\n",
    "        if self.name == \"zh\":\n",
    "            cut_lst = jieba.lcut(sentence,cut_all=False)\n",
    "            filter_lst = filter(\" \".__ne__,cut_lst)\n",
    "            res_lst = [self.word2index.get(word,self.word2index[\"<PAD>\"]) for word in filter_lst] + [self.word2index[\"<EOS>\"]]\n",
    "            return self.padIndex(res_lst,max_len) if pad else res_lst\n",
    "        else: # self.name == \"en\"\n",
    "            res_lst = [self.word2index.get(word,self.word2index[\"<PAD>\"]) for word in sentence.split()] + [self.word2index[\"<EOS>\"]]\n",
    "            return self.padIndex(res_lst,max_len) if pad else res_lst\n",
    "\n",
    "    def padIndex(self,lst,max_len):\n",
    "        \"\"\"\n",
    "        Do after processIndex\n",
    "        \"\"\"\n",
    "        if len(lst) > max_len:\n",
    "            return []\n",
    "        lst += [self.word2index[\"<PAD>\"] for i in range(max_len - len(lst))]\n",
    "        return lst\n",
    "\n",
    "    def getSentenceFromIndex(self,index_lst):\n",
    "        \"\"\"\n",
    "        Call after processIndex\n",
    "        \"\"\"\n",
    "        if self.name == \"zh\":\n",
    "            return \"\".join([self.index2word[index] for index in index_lst])\n",
    "        else:\n",
    "            return \" \".join([self.index2word[index] for index in index_lst])\n",
    "\n",
    "    def processIndex(self):\n",
    "        \"\"\"\n",
    "        Do after all the addSentence\n",
    "        \"\"\"\n",
    "        self.word2count = Counter(self.tmp_word_lst) # {word: count}\n",
    "        del self.tmp_word_lst # delete temporary word list\n",
    "        self.word2count[\"<PAD>\"] = self.n_sentences * 50 # add padding mark, label as 0\n",
    "        self.word2count[\"<BOS>\"] = self.n_sentences # add begin of sentence (BOS) mark\n",
    "        self.word2count[\"<EOS>\"] = self.n_sentences # add end of sentence (EOS) mark\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(self.word2count, key=self.word2count.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        self.index2word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        self.word2index = {w: k for k, w in self.index2word.items()}\n",
    "        self.n_words = len(self.index2word)\n",
    "        print('Vocabulary size of {}'.format(self.name), self.n_words)\n",
    "        print(list(self.index2word.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(mode=\"train\",size=10000):\n",
    "    \"\"\"\n",
    "    Source file in Chinese, target file in English\n",
    "\n",
    "    Eg:\n",
    "    巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
    "    PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n",
    "    \"\"\"\n",
    "    data_path = flags.data_path\n",
    "    zh_lang_file = \"{}/zh-lang-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    en_lang_file = \"{}/en-lang-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    pairs_file = \"{}/pairs-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    if mode == \"train\" and os.path.isfile(zh_lang_file) and os.path.isfile(en_lang_file) and os.path.isfile(pairs_file):\n",
    "        src_lang = pickle.load(open(zh_lang_file,\"rb\"))\n",
    "        dst_lang = pickle.load(open(en_lang_file,\"rb\"))\n",
    "        pairs = pickle.load(open(pairs_file,\"rb\"))\n",
    "        print('Vocabulary size of {}'.format(src_lang.name), src_lang.n_words)\n",
    "        print('Vocabulary size of {}'.format(dst_lang.name), dst_lang.n_words)\n",
    "        return src_lang, dst_lang, pairs\n",
    "    else:\n",
    "        src_lang = Lang(\"zh\")\n",
    "        dst_lang = Lang(\"en\")\n",
    "        pairs = []\n",
    "    path = \"dataset_{}\".format(size)\n",
    "    set_size = 8000 if mode == \"train\" else 1000\n",
    "    set_size = set_size * 10 if size == 100000 else set_size\n",
    "    src_file = open(\"{}/{}_source_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "    dst_file = open(\"{}/{}_target_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Reading data...\")\n",
    "    for i,(src_line,dst_line) in enumerate(zip(src_file,dst_file),1):\n",
    "        src = src_line.splitlines()[0]\n",
    "        dst = dst_line.splitlines()[0]\n",
    "        norm_src = Lang.normalizeString(src,\"zh\")\n",
    "        norm_dst = Lang.normalizeString(dst,\"en\")\n",
    "        if mode == \"train\":\n",
    "            src_lang.addSentence(norm_src)\n",
    "            dst_lang.addSentence(norm_dst)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Done {}/{}\".format(i,set_size))\n",
    "        pairs.append([norm_src,norm_dst])\n",
    "\n",
    "    if mode != \"train\":\n",
    "        return src_lang, dst_lang, pairs\n",
    "\n",
    "    src_lang.processIndex()\n",
    "    dst_lang.processIndex()\n",
    "\n",
    "    pickle.dump(src_lang,open(zh_lang_file,\"wb\"))\n",
    "    pickle.dump(dst_lang,open(en_lang_file,\"wb\"))\n",
    "    pickle.dump(pairs,open(pairs_file,\"wb\"))\n",
    "    print(\"Dumped to file!\")\n",
    "    return src_lang, dst_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    My own text dataset\n",
    "    ref: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "    \"\"\"\n",
    "    def __init__(self,mode=\"train\",dataset_size=10000,max_seq_len=32,batch_size=32):\n",
    "        self.src_lang, self.dst_lang, self.pairs = preprocess(mode,dataset_size)\n",
    "        print(\"Read {} sentence pairs\".format(len(self.pairs)))\n",
    "        # filter long sentence pairs\n",
    "        # self.pairs = self.filter_pairs()\n",
    "        # print(\"Filtered to {} pairs\".format(len(self.pairs)))\n",
    "        # need to pad the sentences for easy generating Dataloader\n",
    "        index_pairs_file_name = \"{}/index_pairs-{}-{}.pkl\".format(flags.data_path,mode,dataset_size)\n",
    "        if False:\n",
    "        # if os.path.isfile(index_pairs_file_name):\n",
    "            self.index_pairs = pickle.load(open(index_pairs_file_name,\"rb\"))\n",
    "        else:\n",
    "            self.index_pairs = []\n",
    "            for src, dst in self.pairs:\n",
    "                src_index = self.src_lang.getSentenceIndex(src,max_seq_len)\n",
    "                dst_index = self.dst_lang.getSentenceIndex(dst,max_seq_len)\n",
    "                if len(src_index) == 0 or len(dst_index) == 0:\n",
    "                    continue\n",
    "                self.index_pairs.append([src_index,dst_index])\n",
    "            print(\"Further trimmed to {} pairs\".format(len(self.index_pairs)))\n",
    "            pickle.dump(self.index_pairs,open(index_pairs_file_name,\"wb\"))\n",
    "            print(\"Dumped index pairs!\")\n",
    "        self.in_text = np.array(self.index_pairs)[:,0].reshape(-1,max_seq_len)\n",
    "        self.out_text = np.array(self.index_pairs)[:,1].reshape(-1,max_seq_len)\n",
    "        num_pairs = len(self.in_text) // batch_size * batch_size\n",
    "        self.in_text = self.in_text[:num_pairs]\n",
    "        self.out_text = self.out_text[:num_pairs]\n",
    "        print(\"Use {} pairs to {}\".format(num_pairs,mode))\n",
    "        print(\"In_text shape: {}\\t Out_text shape: {}\".format(self.in_text.shape,self.out_text.shape))\n",
    "        print(\"Done generating {}_{} dataset!\".format(mode,dataset_size))\n",
    "\n",
    "    def filter_pairs(self):\n",
    "        self.MIN_LENGTH = {\"zh\":1,\"en\":3}\n",
    "        self.MAX_LENGTH = {\"zh\":60,\"en\":150}\n",
    "        filter_pairs = []\n",
    "        for pair in self.pairs:\n",
    "            if self.MIN_LENGTH[\"zh\"] <= len(pair[0]) <= self.MAX_LENGTH[\"zh\"] \\\n",
    "                and self.MIN_LENGTH[\"en\"] <= len(pair[1]) <= self.MAX_LENGTH[\"en\"]:\n",
    "                    filter_pairs.append(pair)\n",
    "        return filter_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.in_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one sample of the data\n",
    "        \"\"\"\n",
    "        x = self.in_text[idx]\n",
    "        y = self.out_text[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (LSTM / GRU)\n",
    "* Reference\n",
    "    * Animated RNN (LSTM & GRU), <https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45>\n",
    "    * Pytorch LSTM, <https://pytorch.org/docs/stable/nn.html#lstm>\n",
    "\n",
    "![RNN](https://miro.medium.com/max/1516/1*yBXV9o5q7L_CvY7quJt3WQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "![Encoder network](https://pytorch.org/tutorials/_images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # embed = nn.Embedding(vocab_size, vector_size)\n",
    "        # \"vocab_size\" is the number of words in your train, val and test set\n",
    "        # \"vector_size\" is the dimension of the word vectors you are using\n",
    "        # you can view it as a linear transformation\n",
    "        # the tensor is initialized randomly\n",
    "        # Input: (*), LongTensor of arbitrary shape containing the indices to extract (i.e. batch size)\n",
    "        # Output: (*, H), where * is the input shape and H = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # make the embedding size equal to the hidden dimension (lstm size)\n",
    "        # batch_first makes it to (batch_size, seq_len, features)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "            seq_len can be viewed as the time step (many small chunks)\n",
    "        embedding: (batch_size, seq_len, embedding_size)\n",
    "            since batch_first flag is set to True, the first dimension is batch_size\n",
    "        output: (batch_size, seq_len, embedding_size)\n",
    "        h_t: (1, batch_size, hidden_size) # Actually, 1 = num_layers*num_directions\n",
    "        c_t: (1, batch_size, hidden_size)\n",
    "\n",
    "        Pytorch's pack_padded_sequence can be used to\n",
    "        tackle the problem of variable length sequences\n",
    "        Reference:\n",
    "        * https://discuss.pytorch.org/t/understanding-lstm-input/31110/3\n",
    "        * https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence\n",
    "        * https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "        * https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism\n",
    "        * https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x)\n",
    "        output, state = self.lstm(embedding, prev_state)\n",
    "        return output, state\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "![Decoder network](https://pytorch.org/tutorials/_images/decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        embedding: (batch_size, seq_len, embedding_size) # embedding_size = hidden_size\n",
    "        output: (batch_size, seq_len, embedding_size)\n",
    "        h_t: (1, batch_size, hidden_size)\n",
    "        c_t: (1, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x)\n",
    "        embedding = F.relu(embedding)\n",
    "        output, state = self.lstm(embedding, prev_state)\n",
    "        output = self.linear(output)\n",
    "#         output = self.softmax(output)\n",
    "        return output, state\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with Attention\n",
    "\n",
    "![Decoder with Attention](https://i.imgur.com/1152PYf.png)\n",
    "![pytorch decoder with attention](https://pytorch.org/tutorials/_images/attention-decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=flags.seq_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,train_loader):\n",
    "    \"\"\"\n",
    "    Core training function\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0) # ignore padding\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=flags.learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "    iteration = 0\n",
    "    losses = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for e in range(flags.num_epochs):\n",
    "        encoder_ht, encoder_ct = encoder.initHidden(flags.batch_size)\n",
    "        decoder_ht, decoder_ct = decoder.initHidden(flags.batch_size)\n",
    "\n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            iteration += 1\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "#             encoder_ht, encoder_ct = encoder.initHidden(flags.batch_size)\n",
    "#             decoder_ht, decoder_ct = decoder.initHidden(flags.batch_size)\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            x = torch.tensor(x).to(torch.int64).to(device) # (batch_size, seq_size)\n",
    "            y = torch.tensor(y).to(torch.int64).to(device) # (batch_size, seq_size)\n",
    "\n",
    "            _, (encoder_ht, encoder_ct) = encoder(x, (encoder_ht, encoder_ct))\n",
    "\n",
    "            decoder_input = torch.tensor([0] * flags.batch_size).reshape(flags.batch_size,1).to(device) # <BOS> token\n",
    "            decoder_ht, decoder_ct = encoder_ht, encoder_ct # use last hidden state from encoder\n",
    "            # print(decoder_input.shape,decoder_ht.shape,decoder_ct.shape)\n",
    "\n",
    "            # run through decoder one time step at a time\n",
    "            max_dst_len = y.shape[1]\n",
    "            all_decoder_outputs = torch.zeros((max_dst_len,flags.batch_size,decoder.output_size))\n",
    "            for t in range(max_dst_len):\n",
    "                decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "                all_decoder_outputs[t] = decoder_output.transpose(1,0)\n",
    "                # teaching forcing: next input is the current target\n",
    "                decoder_input = y[:,t].reshape(flags.batch_size,1) # remember to reshape\n",
    "\n",
    "            # loss calculation\n",
    "            loss = criterion(all_decoder_outputs.transpose(1,0).transpose(1,2).to(device), y)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # avoid delivering loss from h_t and c_t\n",
    "            # thus need to remove them from the computation graph\n",
    "            encoder_ht, encoder_ct = encoder_ht.detach(), encoder_ct.detach()\n",
    "            decoder_ht, decoder_ct = decoder_ht.detach(), decoder_ct.detach()\n",
    "\n",
    "            # avoid gradient explosion\n",
    "            _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), flags.gradients_norm)\n",
    "            _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # update parameters with optimizers\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            losses.append(loss_value)\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                percent = iteration / (flags.num_epochs * len(train_loader))\n",
    "                time_since = time.time() - start_time\n",
    "                time_remaining = time_since / percent - time_since\n",
    "                print('Epoch: {}/{}'.format(e+1, flags.num_epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Time: {:.2f}m (- {:.2f}m)'.format(time_since/60, time_remaining/60),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "                logger.info('Epoch: {}/{} Iteration: {} Loss: {}'.format(e+1, flags.num_epochs, iteration, loss_value))\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                torch.save(encoder,\n",
    "                           '{}/encoder-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "                torch.save(decoder,\n",
    "                           '{}/decoder-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "\n",
    "    print(\"Time:{}s\".format(time.time()-start_time))\n",
    "    torch.save(encoder,'{}/encoder-final.pth'.format(flags.checkpoint_path))\n",
    "    torch.save(decoder,'{}/decoder-final.pth'.format(flags.checkpoint_path))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TextDataset(\"train\",10000,max_seq_len=flags.seq_size,batch_size=flags.batch_size)\n",
    "train_loader = data.DataLoader(dataset=train_set,batch_size=flags.batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(train_set.src_lang.n_words, flags.lstm_size).to(device)\n",
    "decoder = DecoderRNN(flags.lstm_size,train_set.dst_lang.n_words).to(device)\n",
    "losses = train(encoder,decoder,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(in_text, out_text, src_lang, dst_lang, encoder, decoder):\n",
    "    encoder.eval() # set in evaluation mode\n",
    "    decoder.eval()\n",
    "\n",
    "    x = torch.tensor(in_text).to(device).reshape(1,-1)\n",
    "    y = torch.tensor(in_text).to(device)\n",
    "    # encoder\n",
    "    encoder_ht, encoder_ct = encoder.initHidden(1)\n",
    "    _, (encoder_ht, encoder_ct) = encoder(x, (encoder_ht, encoder_ct))\n",
    "\n",
    "    decoder_input = torch.tensor([0] * 1).reshape(1,1).to(device) # <BOS> token\n",
    "    decoder_ht, decoder_ct = encoder_ht, encoder_ct # use last hidden state from encoder\n",
    "\n",
    "    # decoder\n",
    "    # run through decoder one time step at a time\n",
    "    decoded_words = []\n",
    "    for t in range(flags.seq_size):\n",
    "        decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "        # choose top word from output\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "        print(top_value,top_index)\n",
    "        ni = top_index[0][0].item()\n",
    "        word = dst_lang.index2word[ni]\n",
    "        decoded_words.append(word)\n",
    "        if word == \"<EOS>\":\n",
    "            break\n",
    "        decoder_input = torch.LongTensor([ni]).reshape(1,1).to(device)\n",
    "\n",
    "    res_words = \" \".join(decoded_words)\n",
    "    print(\"< {}\".format(src_lang.getSentenceFromIndex(in_text)))\n",
    "    print(\"= {}\".format(dst_lang.getSentenceFromIndex(out_text)))\n",
    "    print(\"> {}\".format(res_words))\n",
    "    return res_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = TextDataset(\"test\",10000,max_seq_len=flags.seq_size,batch_size=flags.batch_size)\n",
    "# test_loader = data.DataLoader(dataset=test_set,batch_size=1,shuffle=False)\n",
    "src_lang = train_set.src_lang\n",
    "dst_lang = train_set.dst_lang\n",
    "_, _, test_set = preprocess(\"test\",10000)\n",
    "for i,(in_text, out_text) in enumerate(test_set):\n",
    "    if i == 5:\n",
    "        break\n",
    "    in_text = src_lang.getSentenceIndex(in_text,0,False)\n",
    "    out_text = dst_lang.getSentenceIndex(out_text,0,False)\n",
    "    print(in_text,out_text)\n",
    "    evaluation(in_text,out_text,src_lang,dst_lang,encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
