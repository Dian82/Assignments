{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq Model with Attention for Chinese-English Machine Translation\n",
    "\n",
    "Some references on seq2seq:\n",
    "* Pytorch, *seq2seq translation tutorial*, <https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>\n",
    "* Practical Pytorch, *Batched seq2seq*, <https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb>\n",
    "\n",
    "![Seq2seq Model](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tricky things:\n",
    "* Three types of dashes in English:\n",
    "    * The Hypen (-)\n",
    "    * The En-dash (–)\n",
    "    * The Em-dash (—)\n",
    "    * Please refer to [Wikipedia]() or [English Language Help Desk](http://site.uit.no/english/punctuation/hyphen/) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import logging, pseudologger\n",
    "import pickle\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "\n",
    "flags = Namespace(\n",
    "    checkpoint_path='checkpoint',\n",
    "    log_flag=True,\n",
    "    log_path=\"log\",\n",
    "    data_path=\"data\",\n",
    "    seq_size=32,\n",
    "    batch_size=32,\n",
    "    embedding_size=256, # embedding dimension\n",
    "    lstm_size=256, # hidden dimension\n",
    "    gradients_norm=5, # gradient clipping\n",
    "    top_k=5,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "for path in [flags.checkpoint_path,flags.log_path,flags.data_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "if flags.log_flag:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(level = logging.INFO)\n",
    "    handler = logging.FileHandler(\"{}/seq2seq-05.log\".format(flags.log_path))\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "else:\n",
    "    logger = pseudologger.PseudoLogger()\n",
    "\n",
    "logger.info(str(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "BOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.tmp_word_lst = []\n",
    "        self.n_sentences = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def normalizeString(s,lang):\n",
    "        if lang == \"zh\":\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s) # so dirty!\n",
    "            s = re.sub(r\"�\",r\"\",s)\n",
    "            # Test if is Chinese\n",
    "            # https://cloud.tencent.com/developer/article/1499958\n",
    "            punc_pair = [(\"。\",\".\"),(\"！\",\"!\"),(\"？\",\"?\"),(\"，\",\",\")]\n",
    "            for zh_punc,en_punc in punc_pair:\n",
    "                s = s.replace(zh_punc,en_punc)\n",
    "            s = re.sub(u\"[^a-zA-Z0-9\\u4e00-\\u9fa5,.!?]\",u\" \",s)\n",
    "            s = re.sub(r\"\\s+\", r\" \", s)\n",
    "            s = s.lower().strip()\n",
    "        else: # lang == \"en\"\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s)\n",
    "            s = re.sub(r\"([,.!?])\",r\" \\1\",s) # add a space between these punctuations\n",
    "            s = re.sub(r\"[^a-zA-Z0-9,.!?]+\",r\" \",s) # remove most of the punctuations\n",
    "            s = re.sub(r\"\\s+\", r\" \", s)\n",
    "            s = s.lower().strip()\n",
    "        return s\n",
    "\n",
    "    def addSentence(self,sentence):\n",
    "        self.n_sentences += 1\n",
    "        if self.name == \"zh\": # need to use tools to split words\n",
    "            cut_lst = jieba.lcut(sentence,cut_all=False) # precisely cut\n",
    "            self.tmp_word_lst += filter(\" \".__ne__,cut_lst) # remove all the white spaces\n",
    "        else: # self.name == \"en\"\n",
    "            self.tmp_word_lst += sentence.split()\n",
    "\n",
    "    def getSentenceIndex(self,sentence,max_len,pad=True):\n",
    "        \"\"\"\n",
    "        Do after processIndex\n",
    "        \"\"\"\n",
    "        if self.name == \"zh\":\n",
    "            cut_lst = jieba.lcut(sentence,cut_all=False)\n",
    "            filter_lst = filter(\" \".__ne__,cut_lst)\n",
    "            res_lst = [self.word2index.get(word,self.word2index[\"<PAD>\"]) for word in filter_lst] + [self.word2index[\"<EOS>\"]]\n",
    "            return self.padIndex(res_lst,max_len) if pad else res_lst\n",
    "        else: # self.name == \"en\"\n",
    "            res_lst = [self.word2index.get(word,self.word2index[\"<PAD>\"]) for word in sentence.split()] + [self.word2index[\"<EOS>\"]]\n",
    "            return self.padIndex(res_lst,max_len) if pad else res_lst\n",
    "\n",
    "    def padIndex(self,lst,max_len):\n",
    "        \"\"\"\n",
    "        Do after processIndex\n",
    "        \"\"\"\n",
    "        if len(lst) > max_len:\n",
    "            return []\n",
    "        lst += [self.word2index[\"<PAD>\"] for i in range(max_len - len(lst))]\n",
    "        return lst\n",
    "\n",
    "    def getSentenceFromIndex(self,index_lst):\n",
    "        \"\"\"\n",
    "        Call after processIndex\n",
    "        \"\"\"\n",
    "        if self.name == \"zh\":\n",
    "            return \"\".join([self.index2word[index] for index in index_lst])\n",
    "        else:\n",
    "            return \" \".join([self.index2word[index] for index in index_lst])\n",
    "\n",
    "    def processIndex(self):\n",
    "        \"\"\"\n",
    "        Do after all the addSentence\n",
    "        \"\"\"\n",
    "        self.word2count = Counter(self.tmp_word_lst) # {word: count}\n",
    "        del self.tmp_word_lst # delete temporary word list\n",
    "        max_count = max(self.word2count.values())\n",
    "        self.word2count[\"<PAD>\"] = max_count + 3 # add padding mark, label as 0\n",
    "        self.word2count[\"<BOS>\"] = max_count + 2 # add begin of sentence (BOS) mark\n",
    "        self.word2count[\"<EOS>\"] = max_count + 1 # add end of sentence (EOS) mark\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(self.word2count, key=self.word2count.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        self.index2word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        self.word2index = {w: k for k, w in self.index2word.items()}\n",
    "        self.n_words = len(self.index2word)\n",
    "        print('Vocabulary size of {}'.format(self.name), self.n_words)\n",
    "        print(list(self.index2word.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(mode=\"train\",size=10000):\n",
    "    \"\"\"\n",
    "    Source file in Chinese, target file in English\n",
    "\n",
    "    Eg:\n",
    "    巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
    "    PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n",
    "    \"\"\"\n",
    "    data_path = flags.data_path\n",
    "    zh_lang_file = \"{}/zh-lang-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    en_lang_file = \"{}/en-lang-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    pairs_file = \"{}/pairs-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    if mode == \"train\" and os.path.isfile(zh_lang_file) and os.path.isfile(en_lang_file) and os.path.isfile(pairs_file):\n",
    "        src_lang = pickle.load(open(zh_lang_file,\"rb\"))\n",
    "        dst_lang = pickle.load(open(en_lang_file,\"rb\"))\n",
    "        pairs = pickle.load(open(pairs_file,\"rb\"))\n",
    "        print('Vocabulary size of {}'.format(src_lang.name), src_lang.n_words)\n",
    "        print(list(src_lang.index2word.items())[:10])\n",
    "        print('Vocabulary size of {}'.format(dst_lang.name), dst_lang.n_words)\n",
    "        print(list(dst_lang.index2word.items())[:10])\n",
    "        return src_lang, dst_lang, pairs\n",
    "    else:\n",
    "        src_lang = Lang(\"zh\")\n",
    "        dst_lang = Lang(\"en\")\n",
    "        pairs = []\n",
    "    path = \"dataset_{}\".format(size)\n",
    "    set_size = 8000 if mode == \"train\" else 1000\n",
    "    set_size = set_size * 10 if size == 100000 else set_size\n",
    "    src_file = open(\"{}/{}_source_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "    dst_file = open(\"{}/{}_target_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Reading data...\")\n",
    "    for i,(src_line,dst_line) in enumerate(zip(src_file,dst_file),1):\n",
    "        src = src_line.splitlines()[0]\n",
    "        dst = dst_line.splitlines()[0]\n",
    "        norm_src = Lang.normalizeString(src,\"zh\")\n",
    "        norm_dst = Lang.normalizeString(dst,\"en\")\n",
    "        if mode == \"train\":\n",
    "            src_lang.addSentence(norm_src)\n",
    "            dst_lang.addSentence(norm_dst)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Done {}/{}\".format(i,set_size))\n",
    "        pairs.append([norm_src,norm_dst])\n",
    "\n",
    "    if mode != \"train\":\n",
    "        return src_lang, dst_lang, pairs\n",
    "\n",
    "    src_lang.processIndex()\n",
    "    dst_lang.processIndex()\n",
    "\n",
    "    pickle.dump(src_lang,open(zh_lang_file,\"wb\"))\n",
    "    pickle.dump(dst_lang,open(en_lang_file,\"wb\"))\n",
    "    pickle.dump(pairs,open(pairs_file,\"wb\"))\n",
    "    print(\"Dumped to file!\")\n",
    "    return src_lang, dst_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    My own text dataset\n",
    "    ref: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "    \"\"\"\n",
    "    def __init__(self,mode=\"train\",dataset_size=10000,max_seq_len=32,batch_size=32,trunc=-1):\n",
    "        self.src_lang, self.dst_lang, self.pairs = preprocess(mode,dataset_size)\n",
    "        print(\"Read {} sentence pairs\".format(len(self.pairs)))\n",
    "        # filter long sentence pairs\n",
    "        # self.pairs = self.filter_pairs()\n",
    "        # print(\"Filtered to {} pairs\".format(len(self.pairs)))\n",
    "        # need to pad the sentences for easy generating Dataloader\n",
    "        index_pairs_file_name = \"{}/index_pairs-{}-{}.pkl\".format(flags.data_path,mode,dataset_size)\n",
    "        if False:\n",
    "        # if os.path.isfile(index_pairs_file_name):\n",
    "            self.index_pairs = pickle.load(open(index_pairs_file_name,\"rb\"))\n",
    "        else:\n",
    "            self.index_pairs = []\n",
    "            all_pairs = []\n",
    "            src_len = []\n",
    "            dst_len = []\n",
    "            for src, dst in self.pairs:\n",
    "                src_index = self.src_lang.getSentenceIndex(src,max_seq_len,False)\n",
    "                dst_index = self.dst_lang.getSentenceIndex(dst,max_seq_len,False)\n",
    "                all_pairs.append([src_index,dst_index])\n",
    "                src_len.append(len(src_index))\n",
    "                dst_len.append(len(dst_index))\n",
    "            src_len = np.array(src_len)\n",
    "            dst_len = np.array(dst_len)\n",
    "            print(\"Src avg sentence len: {}, max: {}, min: {}\".format(src_len.mean(),src_len.max(),src_len.min()))\n",
    "            print(\"Dst avg sentence len: {}, max: {}, min: {}\".format(dst_len.mean(),dst_len.max(),dst_len.min()))\n",
    "            for src_index, dst_index in all_pairs:\n",
    "                src_index = self.src_lang.padIndex(src_index,max_seq_len)\n",
    "                dst_index = self.dst_lang.padIndex(dst_index,max_seq_len)\n",
    "                if len(src_index) == 0 or len(dst_index) == 0:\n",
    "                    continue\n",
    "                self.index_pairs.append([src_index,dst_index])\n",
    "            print(\"Further trimmed to {} pairs\".format(len(self.index_pairs)))\n",
    "            pickle.dump(self.index_pairs,open(index_pairs_file_name,\"wb\"))\n",
    "            print(\"Dumped index pairs!\")\n",
    "        self.in_text = np.array(self.index_pairs)[:,0].reshape(-1,max_seq_len)\n",
    "        self.out_text = np.array(self.index_pairs)[:,1].reshape(-1,max_seq_len)\n",
    "        if trunc == -1:\n",
    "            num_pairs = len(self.in_text) // batch_size * batch_size\n",
    "        else:\n",
    "            num_pairs = trunc\n",
    "        self.in_text = self.in_text[:num_pairs]\n",
    "        self.out_text = self.out_text[:num_pairs]\n",
    "        print(\"Use {} pairs to {}\".format(num_pairs,mode))\n",
    "        print(\"In_text shape: {}\\t Out_text shape: {}\".format(self.in_text.shape,self.out_text.shape))\n",
    "        print(\"Done generating {}_{} dataset!\".format(mode,dataset_size))\n",
    "\n",
    "    def filter_pairs(self):\n",
    "        self.MIN_LENGTH = {\"zh\":1,\"en\":3}\n",
    "        self.MAX_LENGTH = {\"zh\":60,\"en\":150}\n",
    "        filter_pairs = []\n",
    "        for pair in self.pairs:\n",
    "            if self.MIN_LENGTH[\"zh\"] <= len(pair[0]) <= self.MAX_LENGTH[\"zh\"] \\\n",
    "                and self.MIN_LENGTH[\"en\"] <= len(pair[1]) <= self.MAX_LENGTH[\"en\"]:\n",
    "                    filter_pairs.append(pair)\n",
    "        return filter_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.in_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one sample of the data\n",
    "        \"\"\"\n",
    "        x = self.in_text[idx]\n",
    "        y = self.out_text[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (LSTM / GRU)\n",
    "* Reference\n",
    "    * Animated RNN (LSTM & GRU), <https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45>\n",
    "    * Pytorch LSTM, <https://pytorch.org/docs/stable/nn.html#lstm>\n",
    "\n",
    "![RNN](https://miro.medium.com/max/1516/1*yBXV9o5q7L_CvY7quJt3WQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "![Encoder network](https://pytorch.org/tutorials/_images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # embed = nn.Embedding(vocab_size, vector_size)\n",
    "        # \"vocab_size\" is the number of words in your train, val and test set\n",
    "        # \"vector_size\" is the dimension of the word vectors you are using\n",
    "        # you can view it as a linear transformation\n",
    "        # the tensor is initialized randomly\n",
    "        # Input: (*), LongTensor of arbitrary shape containing the indices to extract (i.e. batch size)\n",
    "        # Output: (*, H), where * is the input shape and H = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # make the embedding size equal to the hidden dimension (lstm size)\n",
    "        # batch_first makes it to (batch_size, seq_len, features)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "            seq_len can be viewed as the time step (many small chunks)\n",
    "        embedding: (batch_size, seq_len, embedding_size)\n",
    "            since batch_first flag is set to True, the first dimension is batch_size\n",
    "        output: (batch_size, seq_len, embedding_size)\n",
    "        h_t: (1, batch_size, hidden_size) # Actually, 1 = num_layers*num_directions\n",
    "        c_t: (1, batch_size, hidden_size)\n",
    "\n",
    "        Pytorch's pack_padded_sequence can be used to\n",
    "        tackle the problem of variable length sequences\n",
    "        Reference:\n",
    "        * https://discuss.pytorch.org/t/understanding-lstm-input/31110/3\n",
    "        * https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence\n",
    "        * https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "        * https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism\n",
    "        * https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x)\n",
    "        output, state = self.lstm(embedding, prev_state)\n",
    "        return output, state\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "![Decoder network](https://pytorch.org/tutorials/_images/decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.name = \"Toy\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1) # since CrossEntropyLoss is used\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        embedding: (batch_size, seq_len, embedding_size) # embedding_size = hidden_size\n",
    "            operate the words in embedding space\n",
    "        output: (batch_size, seq_len, embedding_size)\n",
    "        output: (batch_size, seq_len, output_size)\n",
    "            from embedding space to index space\n",
    "        h_t: (1, batch_size, hidden_size)\n",
    "        c_t: (1, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # outputs of the encoder are passed from hidden_state\n",
    "        embedding = self.embedding(x)\n",
    "#         embedding = F.relu(embedding)\n",
    "        output, state = self.lstm(embedding, prev_state)\n",
    "        output = self.linear(output)\n",
    "#         output = self.softmax(output)\n",
    "        return output, state\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with Attention\n",
    "\n",
    "![Decoder with Attention](https://i.imgur.com/1152PYf.png)\n",
    "![pytorch decoder with attention](https://pytorch.org/tutorials/_images/attention-decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=32):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        # run this function one step at a time\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden_in, encoder_outputs_in):\n",
    "        \"\"\"\n",
    "        hidden: (batch_size, 1, hidden_size)\n",
    "        encoder_outputs: (batch_size, seq_len, embedding_size)\n",
    "        \"\"\"\n",
    "        hidden = hidden_in.permute(1,0,2)\n",
    "        encoder_outputs = encoder_outputs_in.permute(1,0,2)\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = torch.zeros(this_batch_size, max_len).to(device) # B x S\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b].reshape(-1), encoder_outputs[i, b].reshape(-1))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.dot(energy)\n",
    "            return energy\n",
    "\n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.name = \"Luong\"\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "#         embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "#         print(embedded,last_hidden)\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, (hidden, cell_state) = self.lstm(embedded, last_hidden)\n",
    "#         rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # batch matrix-matrix product\n",
    "        context = attn_weights.bmm(encoder_outputs) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(1) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, (hidden, cell_state), attn_weights\n",
    "    \n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,train_loader,batch_size=32,num_epochs=100,print_every=50,save_every=100):\n",
    "    \"\"\"\n",
    "    Core training function\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token) # ignore padding\n",
    "    # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=flags.learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "    iteration = 0\n",
    "    losses = []\n",
    "\n",
    "    print(\"Begin training ...\")\n",
    "    start_time = time.time()\n",
    "    for e in range(num_epochs):\n",
    "        encoder_ht, encoder_ct = encoder.initHidden(batch_size)\n",
    "        decoder_ht, decoder_ct = decoder.initHidden(batch_size)\n",
    "\n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            iteration += 1\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            x = torch.tensor(x).to(torch.int64).to(device) # (batch_size, seq_size)\n",
    "            y = torch.tensor(y).to(torch.int64).to(device) # (batch_size, seq_size)\n",
    "\n",
    "            encoder_outputs, (encoder_ht, encoder_ct) = encoder(x, (encoder_ht, encoder_ct))\n",
    "\n",
    "            decoder_input = torch.tensor([BOS_token] * batch_size).reshape(batch_size,1).to(device) # <BOS> token\n",
    "            decoder_ht, decoder_ct = encoder_ht, encoder_ct # use last hidden state from encoder\n",
    "            # print(decoder_input.shape,decoder_ht.shape,decoder_ct.shape)\n",
    "\n",
    "            # run through decoder one time step at a time\n",
    "            max_dst_len = y.shape[1]\n",
    "            all_decoder_outputs = torch.zeros((max_dst_len,flags.batch_size,decoder.output_size))\n",
    "            for t in range(max_dst_len): # for each time step\n",
    "                if decoder.name == \"Toy\":\n",
    "                    # decoder_output: (batch_size, seq_len, output_size)\n",
    "                    decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "                    all_decoder_outputs[t] = decoder_output.transpose(1,0)\n",
    "                else:\n",
    "                    decoder_output, (decoder_ht, decoder_ct), decoder_attn = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                    all_decoder_outputs[t] = decoder_output\n",
    "                # teaching forcing: next input is the current target\n",
    "                decoder_input = y[:,t].reshape(flags.batch_size,1) # remember to reshape\n",
    "\n",
    "            # loss calculation\n",
    "            # (max_dst_len, batch_size, output_size)\n",
    "            # (batch_size, max_dst_len, output_size)\n",
    "            # (batch_size, output_size, max_dst_len)\n",
    "            loss = criterion(all_decoder_outputs.permute(1,2,0).to(device).to(device), y) # transpose(1,0).transpose(1,2)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # avoid delivering loss from h_t and c_t\n",
    "            # thus need to remove them from the computation graph\n",
    "            encoder_ht, encoder_ct = encoder_ht.detach(), encoder_ct.detach()\n",
    "            decoder_ht, decoder_ct = decoder_ht.detach(), decoder_ct.detach()\n",
    "\n",
    "            # avoid gradient explosion\n",
    "            _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), flags.gradients_norm)\n",
    "            _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # update parameters with optimizers\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            losses.append(loss_value)\n",
    "\n",
    "            if iteration % print_every == 0:\n",
    "                percent = iteration / (flags.num_epochs * len(train_loader))\n",
    "                time_since = time.time() - start_time\n",
    "                time_remaining = time_since / percent - time_since\n",
    "                print('Epoch: {}/{}'.format(e+1, num_epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Time: {:.2f}m (- {:.2f}m)'.format(time_since/60, time_remaining/60),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "                logger.info('Epoch: {}/{} Iteration: {} Loss: {}'.format(e+1, num_epochs, iteration, loss_value))\n",
    "\n",
    "            if iteration % save_every == 0:\n",
    "                torch.save(encoder,\n",
    "                           '{}/encoder-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "                torch.save(decoder,\n",
    "                           '{}/decoder-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "\n",
    "    print(\"Time:{}s\".format(time.time()-start_time))\n",
    "    torch.save(encoder,'{}/encoder-final.pth'.format(flags.checkpoint_path))\n",
    "    torch.save(decoder,'{}/decoder-final.pth'.format(flags.checkpoint_path))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TextDataset(\"train\",10000,max_seq_len=flags.seq_size,batch_size=flags.batch_size)\n",
    "train_loader = data.DataLoader(dataset=train_set,batch_size=flags.batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoder = EncoderRNN(train_set.src_lang.n_words, flags.lstm_size).to(device)\n",
    "# decoder = LuongAttnDecoderRNN(\"dot\",flags.lstm_size,train_set.dst_lang.n_words).to(device)\n",
    "encoder = torch.load(\"checkpoint/encoder-final.pth\")\n",
    "decoder = torch.load(\"checkpoint/decoder-final.pth\")\n",
    "losses = train(encoder,decoder,train_loader,batch_size=flags.batch_size,num_epochs=flags.num_epochs,print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "# https://cloud.tencent.com/developer/article/1042161\n",
    "\n",
    "def evaluation(in_text, out_text, src_lang, dst_lang, encoder, decoder, print_flag=False):\n",
    "    encoder.eval() # set in evaluation mode\n",
    "    decoder.eval()\n",
    "\n",
    "    x = torch.tensor(in_text).to(device).reshape(1,-1)\n",
    "    y = torch.tensor(in_text).to(device)\n",
    "    # encoder\n",
    "    encoder_ht, encoder_ct = encoder.initHidden(1)\n",
    "    encoder_outputs, (encoder_ht, encoder_ct) = encoder(x, (encoder_ht, encoder_ct))\n",
    "\n",
    "    decoder_input = torch.tensor([BOS_token] * 1).reshape(1,1).to(device) # <BOS> token\n",
    "    decoder_ht, decoder_ct = encoder_ht, encoder_ct # use last hidden state from encoder\n",
    "\n",
    "    # decoder\n",
    "    # run through decoder one time step at a time\n",
    "    decoded_words = []\n",
    "    decoded_index = []\n",
    "    max_len = flags.seq_size*10\n",
    "    decoder_attentions = torch.zeros(max_len,max_len)\n",
    "    for t in range(max_len):\n",
    "        if decoder.name == \"Toy\":\n",
    "            decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "        else:\n",
    "            decoder_output, (decoder_ht, decoder_ct), decoder_attention = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "            decoder_attentions[t,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "        # choose top word from output\n",
    "#         print(decoder_output.data)\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "#         print(top_value,top_index)\n",
    "        ni = top_index[0][0].item()\n",
    "        decoded_index.append(ni)\n",
    "        word = dst_lang.index2word[ni]\n",
    "        decoded_words.append(word)\n",
    "        if word == \"<EOS>\":\n",
    "            break\n",
    "        decoder_input = torch.LongTensor([ni]).reshape(1,1).to(device)\n",
    "\n",
    "    pad_index = np.where(out_text == PAD_token)\n",
    "    if len(pad_index[0]) == 0:\n",
    "        pad_index = len(out_text)\n",
    "    else:\n",
    "        pad_index = pad_index[0][0]\n",
    "    sm = SmoothingFunction()\n",
    "    bleu = sentence_bleu([out_text[:pad_index]],decoded_index,smoothing_function=sm.method4)\n",
    "    if print_flag:\n",
    "        print(out_text[:pad_index])\n",
    "        print(decoded_index)\n",
    "        print(\"Bleu score: {}\".format(bleu))\n",
    "        res_words = \" \".join(decoded_words)\n",
    "        print(\"< {}\".format(src_lang.getSentenceFromIndex(in_text)))\n",
    "        print(\"= {}\".format(dst_lang.getSentenceFromIndex(out_text)))\n",
    "        print(\"> {}\".format(res_words))\n",
    "        print()\n",
    "    return decoded_words, decoder_attentions[:t+1, :flags.seq_size], bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "# https://cloud.tencent.com/developer/article/1042161\n",
    "\n",
    "def BeamSearch(in_text, out_text, src_lang, dst_lang, encoder, decoder, beam_width=2, print_flag=False):\n",
    "    encoder.eval() # set in evaluation mode\n",
    "    decoder.eval()\n",
    "\n",
    "    x = torch.tensor(in_text).to(device).reshape(1,-1)\n",
    "    y = torch.tensor(in_text).to(device)\n",
    "    # encoder\n",
    "    encoder_ht, encoder_ct = encoder.initHidden(1)\n",
    "    encoder_outputs, (encoder_ht, encoder_ct) = encoder(x, (encoder_ht, encoder_ct))\n",
    "\n",
    "    decoder_ht, decoder_ct = encoder_ht, encoder_ct # use last hidden state from encoder\n",
    "\n",
    "    # decoder\n",
    "    # run through decoder one time step at a time\n",
    "    path = [(BOS_token,0,[])] # input, value, words on the path\n",
    "    max_len = flags.seq_size*2\n",
    "#     decoder_attentions = torch.zeros(max_len,max_len)\n",
    "    for t in range(max_len):\n",
    "        new_path = []\n",
    "        flag_done = True\n",
    "        for decoder_input, value, indices in path:\n",
    "            if decoder_input == EOS_token:\n",
    "                new_path.append((decoder_input,value,indices))\n",
    "                continue\n",
    "            flag_done = False\n",
    "            decoder_input = torch.tensor([decoder_input]).reshape(1,1).to(device)\n",
    "            if decoder.name == \"Toy\":\n",
    "                decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "            else:\n",
    "                decoder_output, (decoder_ht, decoder_ct), decoder_attention = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "#                 decoder_attentions[t,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            # choose top word from output\n",
    "            softmax_output = F.softmax(decoder_output,dim=1)\n",
    "            top_value, top_index = softmax_output.data.topk(beam_width)\n",
    "            top_value = top_value.cpu().numpy()[0] + value\n",
    "            top_index = top_index.cpu().numpy()[0]\n",
    "            for i in range(beam_width):\n",
    "                ni = int(top_index[i])\n",
    "                new_path.append((ni,top_value[i],indices+[ni]))\n",
    "        if flag_done:\n",
    "            _, value, decoded_index = new_path[0]\n",
    "            break\n",
    "        else:\n",
    "            new_path.sort(key=lambda x:x[1] / len(x[2]),reverse=True) # normalization\n",
    "            path = new_path[:beam_width]\n",
    "\n",
    "    if not flag_done:\n",
    "        _, value, decoded_index = path[0]\n",
    "    decoded_words = []\n",
    "    for ni in decoded_index:\n",
    "        word = dst_lang.index2word[ni]\n",
    "        decoded_words.append(word)\n",
    "\n",
    "    pad_index = np.where(out_text == PAD_token)\n",
    "    if len(pad_index[0]) == 0:\n",
    "        pad_index = len(out_text)\n",
    "    else:\n",
    "        pad_index = pad_index[0][0]\n",
    "    sm = SmoothingFunction()\n",
    "    bleu = sentence_bleu([out_text[:pad_index]],decoded_index,smoothing_function=sm.method4)\n",
    "    if print_flag:\n",
    "        print(out_text[:pad_index])\n",
    "        print(decoded_index)\n",
    "        print(\"Bleu score: {}\".format(bleu))\n",
    "        res_words = \" \".join(decoded_words)\n",
    "        print(\"< {}\".format(src_lang.getSentenceFromIndex(in_text)))\n",
    "        print(\"= {}\".format(dst_lang.getSentenceFromIndex(out_text)))\n",
    "        print(\"> {}\".format(res_words))\n",
    "        print()\n",
    "    return decoded_words, decoder_attentions[:t+1, :flags.seq_size], bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = TextDataset(\"test\",10000,max_seq_len=flags.seq_size,batch_size=flags.batch_size)\n",
    "# test_loader = data.DataLoader(dataset=test_set,batch_size=1,shuffle=False)\n",
    "src_lang = train_set.src_lang\n",
    "dst_lang = train_set.dst_lang\n",
    "bleus = []\n",
    "# for i in range(5,10):\n",
    "#     res_words, attention, bleu = evaluation(train_set.in_text[i],train_set.out_text[i],src_lang,dst_lang,encoder,decoder,True)\n",
    "#     res_words, attention, bleu = BeamSearch(train_set.in_text[i],train_set.out_text[i],src_lang,dst_lang,encoder,decoder,3,True)\n",
    "#     bleus.append(bleu)\n",
    "\n",
    "_, _, test_set = preprocess(\"test\",10000)\n",
    "for i,(in_text, out_text) in enumerate(test_set):\n",
    "#     if i == 5:\n",
    "#         break\n",
    "    in_text = src_lang.getSentenceIndex(in_text,0,False)\n",
    "    out_text = dst_lang.getSentenceIndex(out_text,0,False)\n",
    "    print_flag = True if i % 50 == 0 else False\n",
    "    res_words, attention, bleu = BeamSearch(in_text,out_text,src_lang,dst_lang,encoder,decoder,2,print_flag)\n",
    "    bleus.append(bleu)\n",
    "print(np.mean(bleus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_text = src_lang.getSentenceIndex(\"目前的趋势是\",0,False)\n",
    "# out_text = dst_lang.getSentenceIndex(\"the tendency is\",32,False)\n",
    "# evaluation(in_text,out_text,src_lang,dst_lang,encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the train loss curve\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.savefig(r\"fig/train_loss.pdf\",format=\"pdf\",dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_attention(\" \".join([str(i) for i in range(len(res_words))]),res_words,attention[:,:len(res_words)])\n",
    "# src_lang.getSentenceFromIndex(train_set.in_text[0][:len(res_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "src_lang = train_set.src_lang\n",
    "dst_lang = train_set.dst_lang\n",
    "for i in range(5):\n",
    "    res_words, attention = evaluation(train_set.in_text[i],train_set.out_text[i],src_lang,dst_lang,encoder,decoder)\n",
    "    print(res_words)\n",
    "#     sentence_bleu(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
