{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq Model with Attention for Chinese-English Machine Translation\n",
    "\n",
    "Some references on seq2seq:\n",
    "* Pytorch, *seq2seq translation tutorial*, <https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>\n",
    "* Practical Pytorch, *Batched seq2seq*, <https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb>\n",
    "\n",
    "![Seq2seq Model](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tricky things:\n",
    "* Three types of dashes in English:\n",
    "    * The Hypen (-)\n",
    "    * The En-dash (–)\n",
    "    * The Em-dash (—)\n",
    "    * Please refer to [Wikipedia]() or [English Language Help Desk](http://site.uit.no/english/punctuation/hyphen/) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import logging, pseudologger\n",
    "import pickle\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from argparse import Namespace\n",
    "\n",
    "flags = Namespace(\n",
    "    checkpoint_path='checkpoint',\n",
    "    log_flag=False,\n",
    "    log_path=\"log\",\n",
    "    data_path=\"data\",\n",
    "    seq_size=32,\n",
    "    batch_size=5,\n",
    "    embedding_size=256, # embedding dimension\n",
    "    lstm_size=256, # hidden dimension\n",
    "    gradients_norm=5, # gradient clipping\n",
    "    trunc=5,\n",
    "    top_k=5,\n",
    "    num_epochs=2000,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay_epochs=10,\n",
    "    learning_rate_decay_ratio=0.5,\n",
    "    teacher_forcing_ratio=1,\n",
    "    print_every=20,\n",
    "    save_every=1000\n",
    ")\n",
    "\n",
    "for path in [flags.checkpoint_path,flags.log_path,flags.data_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "if flags.log_flag:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(level = logging.INFO)\n",
    "    handler = logging.FileHandler(\"{}/seq2seq-12.log\".format(flags.log_path))\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "else:\n",
    "    logger = pseudologger.PseudoLogger()\n",
    "\n",
    "logger.info(str(flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "BOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.tmp_word_lst = []\n",
    "        self.n_sentences = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def normalizeString(s,lang):\n",
    "        if lang == \"zh\":\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s) # so dirty!\n",
    "            s = re.sub(r\"�\",r\"\",s)\n",
    "            # Test if is Chinese\n",
    "            # https://cloud.tencent.com/developer/article/1499958\n",
    "            punc_pair = [(\"。\",\".\"),(\"！\",\"!\"),(\"？\",\"?\"),(\"，\",\",\")]\n",
    "            for zh_punc,en_punc in punc_pair:\n",
    "                s = s.replace(zh_punc,en_punc)\n",
    "            s = re.sub(u\"[^a-zA-Z0-9\\u4e00-\\u9fa5,.!?]\",u\" \",s)\n",
    "            s = re.sub(r\"\\s+\", r\" \", s)\n",
    "            s = s.lower().strip()\n",
    "            s = re.sub(r\"[0-9]+\", r\" <NUM> \", s)\n",
    "        else: # lang == \"en\"\n",
    "            s = re.sub(r\"&#[0-9]+;\",r\"\",s)\n",
    "            s = re.sub(r\"([,.!?])\",r\" \\1\",s) # add a space between these punctuations\n",
    "            s = re.sub(r\"[^a-zA-Z0-9,.!?]+\",r\" \",s) # remove most of the punctuations\n",
    "            s = re.sub(r\"\\s+\", r\" \", s)\n",
    "            s = s.lower().strip()\n",
    "            s = re.sub(r\"[0-9]+\", r\" <NUM> \", s) # replace numbers\n",
    "        return s\n",
    "\n",
    "    def addSentence(self,sentence):\n",
    "        self.n_sentences += 1\n",
    "        if self.name == \"zh\": # need to use tools to split words\n",
    "            split_sentence = sentence.split(\" <NUM> \")\n",
    "            cut_lst = []\n",
    "            for i,item in enumerate(split_sentence):\n",
    "                if i != 0:\n",
    "                    cut_lst += [\"<NUM>\"]\n",
    "                cut_lst += jieba.lcut(item,cut_all=False) # precisely cut\n",
    "            try: # throw out error\n",
    "                if cut_lst.index(\"NUM\"):\n",
    "                    print(cut_lst)\n",
    "            except:\n",
    "                pass\n",
    "            self.tmp_word_lst += filter(\" \".__ne__,cut_lst) # remove all the white spaces\n",
    "        else: # self.name == \"en\"\n",
    "            split_lst = sentence.split()\n",
    "            self.tmp_word_lst += filter(\" \".__ne__,split_lst) # remove all the white spaces\n",
    "\n",
    "    def getSentenceIndex(self,sentence,max_len,pad=True):\n",
    "        \"\"\"\n",
    "        Do after processIndex\n",
    "        \"\"\"\n",
    "        if self.name == \"zh\":\n",
    "            split_sentence = sentence.split(\" <NUM> \")\n",
    "            cut_lst = []\n",
    "            for i,item in enumerate(split_sentence):\n",
    "                if i != 0:\n",
    "                    cut_lst += [\"<NUM>\"]\n",
    "                cut_lst += jieba.lcut(item,cut_all=False) # precisely cut\n",
    "            filter_lst = filter(\" \".__ne__,cut_lst)\n",
    "            res_lst = [self.word2index.get(word,self.word2index[\"<PAD>\"]) for word in filter_lst] + [self.word2index[\"<EOS>\"]]\n",
    "            return self.padIndex(res_lst,max_len) if pad else res_lst\n",
    "        else: # self.name == \"en\"\n",
    "            split_lst = sentence.split()\n",
    "            res_lst = [self.word2index.get(word,self.word2index[\"<PAD>\"]) for word in split_lst] + [self.word2index[\"<EOS>\"]]\n",
    "            return self.padIndex(res_lst,max_len) if pad else res_lst\n",
    "\n",
    "    def padIndex(self,lst,max_len):\n",
    "        \"\"\"\n",
    "        Do after processIndex\n",
    "        \"\"\"\n",
    "        if len(lst) > max_len:\n",
    "            return []\n",
    "        lst += [self.word2index[\"<PAD>\"] for i in range(max_len - len(lst))]\n",
    "        return lst\n",
    "\n",
    "    def getSentenceFromIndex(self,index_lst):\n",
    "        \"\"\"\n",
    "        Call after processIndex\n",
    "        \"\"\"\n",
    "        if self.name == \"zh\":\n",
    "            return \"\".join([self.index2word[index] for index in index_lst])\n",
    "        else:\n",
    "            return \" \".join([self.index2word[index] for index in index_lst])\n",
    "\n",
    "    def processIndex(self):\n",
    "        \"\"\"\n",
    "        Do after all the addSentence\n",
    "        \"\"\"\n",
    "        self.word2count = Counter(self.tmp_word_lst) # {word: count}\n",
    "        del self.tmp_word_lst # delete temporary word list\n",
    "        max_count = max(self.word2count.values())\n",
    "        self.word2count[\"<PAD>\"] = max_count + 3 # add padding mark, label as 0\n",
    "        self.word2count[\"<BOS>\"] = max_count + 2 # add begin of sentence (BOS) mark\n",
    "        self.word2count[\"<EOS>\"] = max_count + 1 # add end of sentence (EOS) mark\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(self.word2count, key=self.word2count.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        self.index2word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        self.word2index = {w: k for k, w in self.index2word.items()}\n",
    "        self.n_words = len(self.index2word)\n",
    "        print('Vocabulary size of {}'.format(self.name), self.n_words)\n",
    "        print(list(self.index2word.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(mode=\"train\",size=10000):\n",
    "    \"\"\"\n",
    "    Source file in Chinese, target file in English\n",
    "\n",
    "    Eg:\n",
    "    巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
    "    PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\n",
    "    \"\"\"\n",
    "    data_path = flags.data_path\n",
    "    zh_lang_file = \"{}/zh-lang-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    en_lang_file = \"{}/en-lang-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    pairs_file = \"{}/pairs-{}-{}.pkl\".format(data_path,mode,size)\n",
    "    if mode == \"train\" and os.path.isfile(zh_lang_file) and os.path.isfile(en_lang_file) and os.path.isfile(pairs_file):\n",
    "        src_lang = pickle.load(open(zh_lang_file,\"rb\"))\n",
    "        dst_lang = pickle.load(open(en_lang_file,\"rb\"))\n",
    "        pairs = pickle.load(open(pairs_file,\"rb\"))\n",
    "        print('Vocabulary size of {}'.format(src_lang.name), src_lang.n_words)\n",
    "        print(list(src_lang.index2word.items())[:10])\n",
    "        print('Vocabulary size of {}'.format(dst_lang.name), dst_lang.n_words)\n",
    "        print(list(dst_lang.index2word.items())[:10])\n",
    "        return src_lang, dst_lang, pairs\n",
    "    else:\n",
    "        src_lang = Lang(\"zh\")\n",
    "        dst_lang = Lang(\"en\")\n",
    "        pairs = []\n",
    "    path = \"dataset_{}\".format(size)\n",
    "    set_size = 8000 if mode == \"train\" else 1000\n",
    "    set_size = set_size * 10 if size == 100000 else set_size\n",
    "    src_file = open(\"{}/{}_source_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "    dst_file = open(\"{}/{}_target_{}.txt\".format(path,mode,set_size),\"r\",encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Reading data...\")\n",
    "    for i,(src_line,dst_line) in enumerate(zip(src_file,dst_file),1):\n",
    "        src = src_line.splitlines()[0]\n",
    "        dst = dst_line.splitlines()[0]\n",
    "        norm_src = Lang.normalizeString(src,\"zh\")\n",
    "        norm_dst = Lang.normalizeString(dst,\"en\")\n",
    "        if mode == \"train\":\n",
    "            src_lang.addSentence(norm_src)\n",
    "            dst_lang.addSentence(norm_dst)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Done {}/{}\".format(i,set_size))\n",
    "        pairs.append([norm_src,norm_dst])\n",
    "\n",
    "    if mode != \"train\":\n",
    "        return src_lang, dst_lang, pairs\n",
    "\n",
    "    src_lang.processIndex()\n",
    "    dst_lang.processIndex()\n",
    "\n",
    "    pickle.dump(src_lang,open(zh_lang_file,\"wb\"))\n",
    "    pickle.dump(dst_lang,open(en_lang_file,\"wb\"))\n",
    "    pickle.dump(pairs,open(pairs_file,\"wb\"))\n",
    "    print(\"Dumped to file!\")\n",
    "    return src_lang, dst_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class TextDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    My own text dataset\n",
    "    ref: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "    \"\"\"\n",
    "    def __init__(self,mode=\"train\",dataset_size=10000,max_seq_len=32,batch_size=32,trunc=-1):\n",
    "        self.src_lang, self.dst_lang, self.pairs = preprocess(mode,dataset_size)\n",
    "        print(\"Read {} sentence pairs\".format(len(self.pairs)))\n",
    "        # need to pad the sentences for easy generating Dataloader\n",
    "        self.index_pairs = []\n",
    "        all_pairs = []\n",
    "        src_len = []\n",
    "        dst_len = []\n",
    "        for src, dst in self.pairs:\n",
    "            src_index = self.src_lang.getSentenceIndex(src,max_seq_len,False)\n",
    "            dst_index = self.dst_lang.getSentenceIndex(dst,max_seq_len,False)\n",
    "            all_pairs.append([src_index,dst_index])\n",
    "            src_len.append(len(src_index))\n",
    "            dst_len.append(len(dst_index))\n",
    "        src_len = np.array(src_len)\n",
    "        dst_len = np.array(dst_len)\n",
    "        print(\"Src avg sentence len: {}, max: {}, min: {}\".format(src_len.mean(),src_len.max(),src_len.min()))\n",
    "        print(\"Dst avg sentence len: {}, max: {}, min: {}\".format(dst_len.mean(),dst_len.max(),dst_len.min()))\n",
    "        self.src_len = []\n",
    "        self.dst_len = []\n",
    "        for src_index, dst_index in all_pairs:\n",
    "            if len(src_index) <= max_seq_len and len(dst_index) <= max_seq_len:\n",
    "                self.src_len.append(len(src_index))\n",
    "                self.dst_len.append(len(dst_index))\n",
    "                src_index = self.src_lang.padIndex(src_index,max_seq_len)\n",
    "                dst_index = self.dst_lang.padIndex(dst_index,max_seq_len)\n",
    "                self.index_pairs.append([src_index,dst_index])\n",
    "        print(\"Further trimmed to {} pairs\".format(len(self.index_pairs)))\n",
    "        self.in_text = np.array(self.index_pairs)[:,0].reshape(-1,max_seq_len)\n",
    "        self.out_text = np.array(self.index_pairs)[:,1].reshape(-1,max_seq_len)\n",
    "        if trunc == -1:\n",
    "            num_pairs = len(self.in_text) // batch_size * batch_size\n",
    "        else:\n",
    "            num_pairs = trunc\n",
    "        self.in_text = self.in_text[:num_pairs]\n",
    "        self.out_text = self.out_text[:num_pairs]\n",
    "        self.pairs = self.pairs[:num_pairs]\n",
    "        print(\"Use {} pairs to {}\".format(num_pairs,mode))\n",
    "        print(\"In_text shape: {}\\t Out_text shape: {}\".format(self.in_text.shape,self.out_text.shape))\n",
    "        print(\"Done generating {}_{} dataset!\".format(mode,dataset_size))\n",
    "\n",
    "    def filter_pairs(self):\n",
    "        self.MIN_LENGTH = {\"zh\":1,\"en\":3}\n",
    "        self.MAX_LENGTH = {\"zh\":60,\"en\":150}\n",
    "        filter_pairs = []\n",
    "        for pair in self.pairs:\n",
    "            if self.MIN_LENGTH[\"zh\"] <= len(pair[0]) <= self.MAX_LENGTH[\"zh\"] \\\n",
    "                and self.MIN_LENGTH[\"en\"] <= len(pair[1]) <= self.MAX_LENGTH[\"en\"]:\n",
    "                    filter_pairs.append(pair)\n",
    "        return filter_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.in_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one sample of the data\n",
    "        \"\"\"\n",
    "        x = self.in_text[idx]\n",
    "        y = self.out_text[idx]\n",
    "        x_len = self.src_len[idx]\n",
    "        y_len = self.dst_len[idx]\n",
    "        return x, y, x_len, y_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (LSTM / GRU)\n",
    "* Reference\n",
    "    * Animated RNN (LSTM & GRU), <https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45>\n",
    "    * Pytorch LSTM, <https://pytorch.org/docs/stable/nn.html#lstm>\n",
    "\n",
    "![RNN](https://miro.medium.com/max/1516/1*yBXV9o5q7L_CvY7quJt3WQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "![Encoder network](https://pytorch.org/tutorials/_images/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # embed = nn.Embedding(vocab_size, vector_size)\n",
    "        # \"vocab_size\" is the number of words in your train, val and test set\n",
    "        # \"vector_size\" is the dimension of the word vectors you are using\n",
    "        # you can view it as a linear transformation\n",
    "        # the tensor is initialized randomly\n",
    "        # Input: (*), LongTensor of arbitrary shape containing the indices to extract (i.e. batch size)\n",
    "        # Output: (*, H), where * is the input shape and H = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # make the embedding size equal to the hidden dimension (lstm size)\n",
    "        # batch_first makes it to (batch_size, seq_len, features)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, dropout=self.dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x, prev_state, input_lengths):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "            seq_len can be viewed as the time step (many small chunks)\n",
    "        embedding: (batch_size, seq_len, embedding_size)\n",
    "            since batch_first flag is set to True, the first dimension is batch_size\n",
    "        output: (batch_size, seq_len, embedding_size)\n",
    "        h_t: (1, batch_size, hidden_size) # Actually, 1 = num_layers*num_directions\n",
    "        c_t: (1, batch_size, hidden_size)\n",
    "\n",
    "        Pytorch's pack_padded_sequence can be used to\n",
    "        tackle the problem of variable length sequences\n",
    "        Packs a Tensor containing padded sequences of variable length.\n",
    "        \n",
    "        torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)\n",
    "        input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]),\n",
    "        B is the batch size, and * is any number of dimensions (including 0).\n",
    "        If batch_first is True, B x T x * input is expected.\n",
    "        \n",
    "        Reference:\n",
    "        * https://discuss.pytorch.org/t/understanding-lstm-input/31110/3\n",
    "        * https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence\n",
    "        * https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "        * https://pytorch.org/docs/stable/notes/faq.html#pack-rnn-unpack-with-data-parallelism\n",
    "        * https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        \"\"\"\n",
    "        embedding = self.embedding(x)\n",
    "        total_length = x.size(1) # max sequence length\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedding, input_lengths, batch_first=True) # reduce computation\n",
    "        output, state = self.lstm(packed, prev_state)\n",
    "        output, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=total_length) # unpack (back to padded)\n",
    "        return output, state\n",
    "\n",
    "    def forward_without_padding(self, x, prev_state):\n",
    "        embedding = self.embedding(x)\n",
    "        output, state = self.lstm(embedding, prev_state)\n",
    "        return output, state\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "![Decoder network](https://pytorch.org/tutorials/_images/decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.name = \"Toy\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1) # since CrossEntropyLoss is used\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        embedding: (batch_size, seq_len, embedding_size) # embedding_size = hidden_size\n",
    "            operate the words in embedding space\n",
    "        output: (batch_size, seq_len, hidden_size)\n",
    "        output: (batch_size, seq_len, output_size)\n",
    "            from embedding space to index space\n",
    "        h_t: (1, batch_size, hidden_size)\n",
    "        c_t: (1, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # outputs of the encoder are passed from hidden_state\n",
    "        embedding = self.embedding(x)\n",
    "        embedding = F.relu(embedding)\n",
    "        output, state = self.lstm(embedding, prev_state)\n",
    "        output = self.linear(output)\n",
    "#         output = self.softmax(output)\n",
    "        return output, state\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with Attention\n",
    "\n",
    "Reference:\n",
    "* <https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05>\n",
    "\n",
    "![Decoder with Attention](https://i.imgur.com/1152PYf.png)\n",
    "![pytorch decoder with attention](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n",
    "![attention](https://miro.medium.com/max/1091/1*wxv56cPyJdrEFSkknrlP-A.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=32):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.name = \"Attn\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "\n",
    "    def forward(self, x, prev_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        x: (batch_size=32, seq_len=1, output_size)\n",
    "        prev_hidden: (1, batch_size=32, hidden_size)\n",
    "        encoder_outputs: (batch_size=32, seq_len=32, hidden_size) # encoder hidden states\n",
    "        \n",
    "        embedded: (batch_size=32, seq_len=1, hidden_size)\n",
    "        decoder_output: (batch_size, seq_len=1, hidden_size)\n",
    "        attn_score: dot(encoder_outputs,decoder_output)\n",
    "            (batch_size,seq_len,1)\n",
    "        attn_weights: softmax(attn_score)\n",
    "            (batch_size,seq_len,1) -> (batch_size,1,seq_len)\n",
    "        attn_output: dot(attn_weights,encoder_outputs)\n",
    "            (batch_size,1,hidden_size)\n",
    "        cat: cat(attn_output,decoder_output)\n",
    "            (batch_size,1,2*hidden_size)\n",
    "        output: (batch_size,1,output_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        decoder_output, hidden = self.lstm(embedded, prev_hidden)\n",
    "        attn_score = torch.bmm(encoder_outputs,decoder_output.transpose(1,2))\n",
    "        attn_weights = F.softmax(attn_score,dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.transpose(1,2),encoder_outputs)\n",
    "        cat = torch.cat((attn_output,decoder_output),dim=2)\n",
    "        output = self.linear(cat)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self,batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device), # h_t\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device)) # c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,dataset,losses,batch_size=32,num_epochs=100,print_every=50,save_every=100):\n",
    "    \"\"\"\n",
    "    Core training function\n",
    "    \"\"\"\n",
    "\n",
    "    train_set = dataset[\"train_set\"]\n",
    "    dev_set = dataset[\"dev_set\"]\n",
    "    train_loader = data.DataLoader(dataset=train_set,batch_size=flags.batch_size,shuffle=True)\n",
    "    \n",
    "    # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token) # ignore padding\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=flags.learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=flags.learning_rate)\n",
    "    encoder_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer,step_size=flags.learning_rate_decay_epochs,gamma=flags.learning_rate_decay_ratio)\n",
    "    decoder_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer,step_size=flags.learning_rate_decay_epochs,gamma=flags.learning_rate_decay_ratio)\n",
    "\n",
    "    iteration = 0\n",
    "    losses = []\n",
    "\n",
    "    print(\"Begin training ...\")\n",
    "    start_time = time.time()\n",
    "    for e in range(num_epochs):\n",
    "        encoder_ht, encoder_ct = encoder.initHidden(batch_size)\n",
    "        decoder_ht, decoder_ct = decoder.initHidden(batch_size)\n",
    "\n",
    "        for step, (x, y, x_len, y_len) in enumerate(train_loader):\n",
    "            iteration += 1\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            seq_lengths, perm_idx = torch.tensor(x_len).sort(0,descending=True)\n",
    "\n",
    "            x = torch.tensor(x).to(torch.int64).to(device) # (batch_size, seq_size)\n",
    "            y = torch.tensor(y).to(torch.int64).to(device) # (batch_size, seq_size)\n",
    "            x = x[perm_idx]\n",
    "            y = y[perm_idx]\n",
    "\n",
    "            encoder_outputs, (encoder_ht, encoder_ct) = encoder(x, (encoder_ht, encoder_ct), seq_lengths)\n",
    "\n",
    "            decoder_input = torch.tensor([BOS_token] * batch_size).reshape(batch_size,1).to(device) # <BOS> token\n",
    "            decoder_ht, decoder_ct = encoder_ht, encoder_ct # use last hidden state from encoder\n",
    "            # print(decoder_input.shape,decoder_ht.shape,decoder_ct.shape)\n",
    "\n",
    "            # run through decoder one time step at a time\n",
    "            max_dst_len = y.shape[1]\n",
    "            all_decoder_outputs = torch.zeros((max_dst_len,flags.batch_size,decoder.output_size))\n",
    "            if random.random() < flags.teacher_forcing_ratio:\n",
    "                for t in range(max_dst_len): # for each time step\n",
    "                    if decoder.name == \"Toy\":\n",
    "                        # decoder_output: (batch_size, seq_len, output_size)\n",
    "                        decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "                        all_decoder_outputs[t] = decoder_output.transpose(1,0)\n",
    "                    elif decoder.name == \"Attn\":\n",
    "                        decoder_output, (decoder_ht, decoder_ct), decoder_attn = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                        all_decoder_outputs[t] = decoder_output.transpose(1,0)\n",
    "                    else:\n",
    "                        decoder_output, (decoder_ht, decoder_ct), decoder_attn = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                        all_decoder_outputs[t] = decoder_output\n",
    "                    # teaching forcing: next input is the current target\n",
    "                    decoder_input = y[:,t].reshape(batch_size,1) # remember to reshape\n",
    "            else: # without teacher forcing\n",
    "                for t in range(max_dst_len): # for each time step\n",
    "                    if decoder.name == \"Toy\":\n",
    "                        # decoder_output: (batch_size, seq_len, output_size)\n",
    "                        decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "                        all_decoder_outputs[t] = decoder_output.transpose(1,0)\n",
    "                    elif decoder.name == \"Attn\":\n",
    "                        decoder_output, (decoder_ht, decoder_ct), decoder_attn = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                        all_decoder_outputs[t] = decoder_output.transpose(1,0)\n",
    "                    else:\n",
    "                        decoder_output, (decoder_ht, decoder_ct), decoder_attn = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                        all_decoder_outputs[t] = decoder_output\n",
    "                    # use the current output as the next input\n",
    "                    topv, topi = decoder_output.topk(1)\n",
    "                    decoder_input = topi.squeeze().detach().reshape(batch_size,1)\n",
    "#                     if decoder_input.item() == EOS_token: # cannot add for batch training!\n",
    "#                         break\n",
    "\n",
    "            # loss calculation\n",
    "            # (max_dst_len, batch_size, output_size)\n",
    "            # (batch_size, max_dst_len, output_size)\n",
    "            # (batch_size, output_size, max_dst_len)\n",
    "            loss = criterion(all_decoder_outputs.permute(1,2,0).to(device).to(device), y) # transpose(1,0).transpose(1,2)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # avoid delivering loss from h_t and c_t\n",
    "            # thus need to remove them from the computation graph\n",
    "            encoder_ht, encoder_ct = encoder_ht.detach(), encoder_ct.detach()\n",
    "            decoder_ht, decoder_ct = decoder_ht.detach(), decoder_ct.detach()\n",
    "\n",
    "            # avoid gradient explosion\n",
    "            _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), flags.gradients_norm)\n",
    "            _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # update parameters with optimizers\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            losses.append(loss_value)\n",
    "\n",
    "            if iteration % print_every == 0:\n",
    "                percent = iteration / (flags.num_epochs * len(train_loader))\n",
    "                time_since = time.time() - start_time\n",
    "                time_remaining = time_since / percent - time_since\n",
    "                print('Epoch: {}/{}'.format(e+1, num_epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Time: {:.2f}m (- {:.2f}m)'.format(time_since/60, time_remaining/60),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "                logger.info('Epoch: {}/{} Iteration: {} Loss: {}'.format(e+1, num_epochs, iteration, loss_value))\n",
    "\n",
    "            if iteration % save_every == 0:\n",
    "                try:\n",
    "                    bleu = evaluation(dev_set,train_set.src_lang,train_set.dst_lang,encoder,decoder)\n",
    "                except:\n",
    "                    bleu = None\n",
    "                logger.info(\"BLEU score: {}\".format(bleu))\n",
    "                torch.save(encoder,\n",
    "                           '{}/encoder-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "                torch.save(decoder,\n",
    "                           '{}/decoder-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "\n",
    "        # learning rate decay\n",
    "        encoder_scheduler.step()\n",
    "        decoder_scheduler.step()\n",
    "\n",
    "    print(\"Time:{}s\".format(time.time()-start_time))\n",
    "    torch.save(encoder,'{}/encoder-final.pth'.format(flags.checkpoint_path))\n",
    "    torch.save(decoder,'{}/decoder-final.pth'.format(flags.checkpoint_path))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "# https://cloud.tencent.com/developer/article/1042161\n",
    "\n",
    "def evalOne(in_text, out_text, src_lang, dst_lang, encoder, decoder, beam_search=False, beam_width=2, print_flag=False):\n",
    "    if len(in_text) > flags.seq_size or len(out_text) > flags.seq_size:\n",
    "        return None, None, -1\n",
    "    encoder.eval() # set in evaluation mode\n",
    "    decoder.eval()\n",
    "\n",
    "    in_text, in_text_len = in_text\n",
    "    x = torch.tensor(in_text).to(device).reshape(1,-1)\n",
    "    seq_len = torch.tensor([in_text_len]).to(torch.int64).to(device)\n",
    "    # encoder\n",
    "    encoder_ht, encoder_ct = encoder.initHidden(1)\n",
    "    encoder_outputs, (encoder_ht, encoder_ct) = encoder(x, (encoder_ht, encoder_ct), seq_len)\n",
    "\n",
    "    decoder_input = torch.tensor([BOS_token] * 1).reshape(1,1).to(device) # <BOS> token\n",
    "    decoder_ht, decoder_ct = encoder_ht, encoder_ct # use last hidden state from encoder\n",
    "\n",
    "    # decoder\n",
    "    # run through decoder one time step at a time\n",
    "    max_len = int(flags.seq_size*1.5)\n",
    "    decoder_attentions = torch.zeros(max_len,flags.seq_size)\n",
    "    if not beam_search:\n",
    "        decoded_words = []\n",
    "        decoded_index = []\n",
    "        for t in range(max_len):\n",
    "            if decoder.name == \"Toy\":\n",
    "                decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "            elif decoder.name == \"Attn\":\n",
    "                decoder_output, (decoder_ht, decoder_ct), decoder_attn = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                decoder_attentions[t] = decoder_attn.transpose(1,2).squeeze(0).squeeze(0).cpu().data\n",
    "#                 print(sum(decoder_attentions[t]))\n",
    "            else:\n",
    "                decoder_output, (decoder_ht, decoder_ct), decoder_attention = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                decoder_attentions[t,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "            # choose top word from output\n",
    "            top_value, top_index = decoder_output.data.topk(1)\n",
    "            ni = top_index[0][0].item()\n",
    "            decoded_index.append(ni)\n",
    "            word = dst_lang.index2word[ni]\n",
    "            decoded_words.append(word)\n",
    "            if word == \"<EOS>\":\n",
    "                break\n",
    "            decoder_input = torch.LongTensor([ni]).reshape(1,1).to(device)\n",
    "    else:\n",
    "        \"\"\"\n",
    "        Beam seach:\n",
    "        https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5\n",
    "        \"\"\"\n",
    "        path = [(BOS_token,0,[])] # input, value, words on the path\n",
    "        for t in range(max_len):\n",
    "            new_path = []\n",
    "            flag_done = True\n",
    "            for decoder_input, value, indices in path:\n",
    "                if decoder_input == EOS_token:\n",
    "                    new_path.append((decoder_input,value,indices))\n",
    "                    continue\n",
    "                elif len(path) != 1 and decoder_input in [BOS_token,PAD_token]:\n",
    "                    continue\n",
    "                flag_done = False\n",
    "                decoder_input = torch.tensor([decoder_input]).reshape(1,1).to(device)\n",
    "                if decoder.name == \"Toy\":\n",
    "                    decoder_output, (decoder_ht, decoder_ct) = decoder(decoder_input, (decoder_ht, decoder_ct))\n",
    "                elif decoder.name == \"Attn\":\n",
    "                    decoder_output, (decoder_ht, decoder_ct), decoder_attn = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "                    decoder_attentions[t] = decoder_attn.transpose(1,2).cpu().data\n",
    "                else:\n",
    "                    decoder_output, (decoder_ht, decoder_ct), decoder_attention = decoder(decoder_input, (decoder_ht, decoder_ct), encoder_outputs)\n",
    "    #                 decoder_attentions[t,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "                # choose top word from output\n",
    "                softmax_output = F.log_softmax(decoder_output,dim=2) # dim 2!\n",
    "                top_value, top_index = softmax_output.data.topk(beam_width)\n",
    "                top_value = top_value.cpu().squeeze().numpy() + value\n",
    "                top_index = top_index.cpu().squeeze().numpy()\n",
    "                for i in range(beam_width):\n",
    "                    ni = int(top_index[i])\n",
    "                    new_path.append((ni,top_value[i],indices+[ni]))\n",
    "            if flag_done:\n",
    "                _, value, decoded_index = new_path[0]\n",
    "                break\n",
    "            else:\n",
    "                new_path.sort(key=lambda x:x[1]/len(x[2])**0.7,reverse=True) # normalization\n",
    "                path = new_path[:beam_width]\n",
    "\n",
    "        if not flag_done:\n",
    "            _, value, decoded_index = path[0]\n",
    "        decoded_words = []\n",
    "        for ni in decoded_index:\n",
    "            word = dst_lang.index2word[ni]\n",
    "            decoded_words.append(word)\n",
    "\n",
    "    pad_index = np.where(out_text == PAD_token)\n",
    "    if len(pad_index[0]) == 0:\n",
    "        pad_index = len(out_text)\n",
    "    else:\n",
    "        pad_index = pad_index[0][0]\n",
    "    filter_outtext = list(filter(\"<PAD>\".__ne__,out_text[:pad_index]))\n",
    "    decoded_index = list(filter(\"<PAD>\".__ne__,decoded_index))\n",
    "    sm = SmoothingFunction()\n",
    "    bleu = sentence_bleu([filter_outtext],decoded_index,smoothing_function=sm.method4)\n",
    "    if print_flag:\n",
    "        print(out_text[:pad_index])\n",
    "        print(decoded_index)\n",
    "        print(\"Bleu score: {}\".format(bleu))\n",
    "        res_words = \" \".join(decoded_words)\n",
    "        print(\"< {}\".format(src_lang.getSentenceFromIndex(in_text)))\n",
    "        print(\"= {}\".format(dst_lang.getSentenceFromIndex(filter_outtext)))\n",
    "        print(\"> {}\".format(res_words))\n",
    "        print()\n",
    "    return decoded_words, decoder_attentions[:t+1, :flags.seq_size], bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataset, src_lang, dst_lang, encoder, decoder, beam_search=False, beam_width=2, all_print_flag=False):\n",
    "    start_time = time.time()\n",
    "    bleus = []\n",
    "    for i,(in_text, out_text) in enumerate(dataset):\n",
    "        in_text = src_lang.getSentenceIndex(in_text,0,False)\n",
    "        in_text_len = len(in_text)\n",
    "        in_text = src_lang.padIndex(in_text,flags.seq_size)\n",
    "        if len(in_text) == 0:\n",
    "            continue\n",
    "        out_text = dst_lang.getSentenceIndex(out_text,0,False)\n",
    "        if all_print_flag:\n",
    "            print_flag = True if i % 50 == 0 else False\n",
    "        res_words, attention, bleu = evalOne((in_text,in_text_len),out_text,src_lang,dst_lang,encoder,decoder,beam_search=beam_search,beam_width=beam_width,print_flag=all_print_flag)\n",
    "        if res_words != None:\n",
    "            bleus.append(bleu)\n",
    "    avg_bleu = np.mean(bleus)\n",
    "    print(\"Evaluation time: {:.2f}s BLEU Score: {}\".format(time.time()-start_time,avg_bleu))\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = TextDataset(\"train\",10000,max_seq_len=flags.seq_size,batch_size=flags.batch_size,trunc=flags.trunc)\n",
    "\n",
    "if flags.trunc == -1:\n",
    "    _, _, dev_set = preprocess(\"dev\",10000)\n",
    "    _, _, test_set = preprocess(\"test\",10000)\n",
    "    dataset = {\"train_set\":train_set,\n",
    "               \"dev_set\":dev_set,\n",
    "               \"test_set\":test_set}\n",
    "else: # trunc\n",
    "    dataset = {\"train_set\":train_set,\n",
    "               \"dev_set\":train_set.pairs,\n",
    "               \"test_set\":train_set.pairs}\n",
    "\n",
    "encoder = EncoderRNN(train_set.src_lang.n_words, flags.lstm_size).to(device)\n",
    "decoder = AttnDecoderRNN(flags.lstm_size,train_set.dst_lang.n_words).to(device)\n",
    "# encoder = torch.load(\"checkpoint/encoder-final.pth\")\n",
    "# decoder = torch.load(\"checkpoint/decoder-final.pth\")\n",
    "losses = []\n",
    "losses = train(encoder,decoder,dataset,losses,batch_size=flags.batch_size,num_epochs=flags.num_epochs,print_every=flags.print_every,save_every=flags.save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the train loss curve\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.savefig(r\"fig/train_loss.pdf\",format=\"pdf\",dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(dataset[\"test_set\"], dataset[\"train_set\"].src_lang, dataset[\"train_set\"].dst_lang, encoder, decoder, beam_search=False, beam_width=2, all_print_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = dataset[\"train_set\"].src_lang\n",
    "dst_lang = dataset[\"train_set\"].dst_lang\n",
    "for i,(in_text, out_text) in enumerate(dataset[\"test_set\"]):\n",
    "    if i == 5:\n",
    "        break\n",
    "    in_text = src_lang.getSentenceIndex(in_text,0,False)\n",
    "    in_text_len = len(in_text)\n",
    "    in_text = src_lang.padIndex(in_text,flags.seq_size)\n",
    "    if len(in_text) == 0:\n",
    "        continue\n",
    "    out_text = dst_lang.getSentenceIndex(out_text,0,False)\n",
    "    res_words, attention, bleu = evalOne((in_text,in_text_len),out_text,src_lang,dst_lang,encoder,decoder,beam_search=False,beam_width=2,print_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = dataset[\"train_set\"].src_lang\n",
    "dst_lang = dataset[\"train_set\"].dst_lang\n",
    "for i,(in_text, out_text) in enumerate(dataset[\"train_set\"].pairs):\n",
    "    if i == 5:\n",
    "        break\n",
    "    in_text = src_lang.getSentenceIndex(in_text,0,False)\n",
    "    in_text_len = len(in_text)\n",
    "    print(in_text,in_text_len)\n",
    "    in_text = src_lang.padIndex(in_text,flags.seq_size)\n",
    "    if len(in_text) == 0:\n",
    "        continue\n",
    "    out_text = dst_lang.getSentenceIndex(out_text,0,False)\n",
    "    res_words, attention, bleu = evalOne((in_text,in_text_len),out_text,src_lang,dst_lang,encoder,decoder,beam_search=False,beam_width=2,print_flag=True)\n",
    "    show_attention(\" \".join([str(i) for i in range(in_text_len)]),res_words,attention[:len(res_words),:in_text_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
