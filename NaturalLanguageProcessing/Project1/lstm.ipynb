{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level = logging.INFO)\n",
    "handler = logging.FileHandler(\"lstm-3.log\")\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "flags = Namespace(\n",
    "    train_file_path='dict_no_stop_jieba',\n",
    "    seq_size=32,\n",
    "    batch_size=64,\n",
    "    embedding_size=128,\n",
    "    lstm_size=128,\n",
    "    gradients_norm=5,\n",
    "    predict_top_k=5,\n",
    "    num_epochs=20,\n",
    "    checkpoint_path='checkpoint',\n",
    ")\n",
    "logger.info(str(flags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reference on LSTMs:\n",
    "* Colah, <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>\n",
    "* Pytorch tutorial, <https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html>\n",
    "* Text Generation With Pytorch, <https://machinetalk.org/2019/02/08/text-generation-with-pytorch/>\n",
    "* Language Modelling and Text Generation using LSTMs — Deep Learning for NLP, <https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file_path,batch_size,seq_size):\n",
    "    text = []\n",
    "    for i,file_name in enumerate(os.listdir(file_path),1):\n",
    "        with open(\"{}/{}\".format(file_path,file_name),\"r\",encoding=\"utf-8\") as infile:\n",
    "            for j,line in enumerate(infile):\n",
    "#                 if j == 0 or line[0] == \"（\":\n",
    "#                     continue\n",
    "                text += line.split()\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print(\"Int_to_vocab\",list(int_to_vocab.items())[:10])\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    print(in_text[:10, :10])\n",
    "    print(out_text[:10, :10])\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "        flags.train_file_path, flags.batch_size, flags.seq_size)\n",
    "\n",
    "    net = RNNModule(n_vocab, flags.seq_size,\n",
    "                    flags.embedding_size, flags.lstm_size)\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for e in range(flags.num_epochs):\n",
    "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "        state_h, state_c = net.zero_state(flags.batch_size)\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "            net.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            _ = torch.nn.utils.clip_grad_norm_(\n",
    "                net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(e+1, flags.num_epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "                logger.info('Epoch: {}/{} Iteration: {} Loss: {}'.format(e+1, flags.num_epochs, iteration, loss_value))\n",
    "\n",
    "            if iteration % 1000 == 0:\n",
    "                torch.save(net.state_dict(),\n",
    "                           '{}/model-{}.pth'.format(flags.checkpoint_path,iteration))\n",
    "    print(\"Time:{}s\".format(time.time()-start_time))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "stopwords = [word[:-1] for word in open(\"stopwords.txt\",\"r\",encoding=\"utf-8\")] # delete \\n\n",
    "\n",
    "def predict(device, net, question, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "    q_index = question_str.index(\"[MASK]\")\n",
    "    question_pre, question_post = question_str[:q_index], question_str[q_index+len(\"[MASK]\"):]\n",
    "    seg_pre, seg_post = jieba.lcut(question_pre,cut_all=False), jieba.lcut(question_post,cut_all=False)\n",
    "    seg_pre.insert(0,\"<BOS>\")\n",
    "    seg_post.insert(len(seg_post),\"<EOS>\")\n",
    "    seg_pre_lst, seg_post_lst = [], []\n",
    "    for word in seg_pre:\n",
    "        if word not in stopwords and word != \"\\n\":\n",
    "            seg_pre_lst.append(word)\n",
    "    for word in seg_post:\n",
    "        if word not in stopwords and word != \"\\n\":\n",
    "            seg_post_lst.append(word)\n",
    "    words = seg_pre_lst\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        index = vocab_to_int.get(w,vocab_to_int[\"<BOS>\"])\n",
    "        ix = torch.tensor([[index]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "#     return int_to_vocab[choice]\n",
    "    return [int_to_vocab[x] for x in choices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int_to_vocab [(0, '<BOS>'), (1, '<EOS>'), (2, '月'), (3, '公司'), (4, '年'), (5, '日'), (6, '中'), (7, '中国'), (8, '5G'), (9, '新')]\n",
      "Vocabulary size 48469\n",
      "[[    0     1     0    19     2     5    31   785   777    23]\n",
      " [   62   155    79  1300  1233 18149  4255 18150   923   110]\n",
      " [ 3381     1     0  2267 13263    64    39    59  4872  2303]\n",
      " [ 8972    77    73    34     6  6492  1662   366  1662  1056]\n",
      " [ 5838  6042     1     0   941  6042 15493  1646  3927     4]\n",
      " [    5  1191  1259   731   197     4     2  1191 15617 13578]\n",
      " [  132    29   638   250  1244   489   530   532   440     1]\n",
      " [    6   138 33373     1     0     1     0 15890   258  2215]\n",
      " [ 2658  9604     1     0     1     0    99  2054  5683  4500]\n",
      " [ 3081   137 24479 16072  1573   137 24480  2328 24481 10018]]\n",
      "[[    1     0    19     2     5    31   785   777    23    42]\n",
      " [  155    79  1300  1233 18149  4255 18150   923   110   148]\n",
      " [    1     0  2267 13263    64    39    59  4872  2303  2455]\n",
      " [   77    73    34     6  6492  1662   366  1662  1056   606]\n",
      " [ 6042     1     0   941  6042 15493  1646  3927     4     1]\n",
      " [ 1191  1259   731   197     4     2  1191 15617 13578 15618]\n",
      " [   29   638   250  1244   489   530   532   440     1     0]\n",
      " [  138 33373     1     0     1     0 15890   258  2215     1]\n",
      " [ 9604     1     0     1     0    99  2054  5683  4500  5291]\n",
      " [  137 24479 16072  1573   137 24480  2328 24481 10018     1]]\n",
      "Epoch: 1/20 Iteration: 100 Loss: 7.338778495788574\n",
      "Epoch: 1/20 Iteration: 200 Loss: 7.167088031768799\n",
      "Epoch: 1/20 Iteration: 300 Loss: 6.692594528198242\n",
      "Epoch: 1/20 Iteration: 400 Loss: 6.768966197967529\n",
      "Epoch: 2/20 Iteration: 500 Loss: 6.169685363769531\n",
      "Epoch: 2/20 Iteration: 600 Loss: 5.783901691436768\n",
      "Epoch: 2/20 Iteration: 700 Loss: 5.3414387702941895\n",
      "Epoch: 2/20 Iteration: 800 Loss: 5.001124858856201\n",
      "Epoch: 3/20 Iteration: 900 Loss: 4.96266508102417\n",
      "Epoch: 3/20 Iteration: 1000 Loss: 4.557365894317627\n",
      "Epoch: 3/20 Iteration: 1100 Loss: 4.651360034942627\n",
      "Epoch: 3/20 Iteration: 1200 Loss: 4.522105693817139\n",
      "Epoch: 3/20 Iteration: 1300 Loss: 4.159364223480225\n",
      "Epoch: 4/20 Iteration: 1400 Loss: 4.231049537658691\n",
      "Epoch: 4/20 Iteration: 1500 Loss: 4.305510520935059\n",
      "Epoch: 4/20 Iteration: 1600 Loss: 4.116302490234375\n",
      "Epoch: 4/20 Iteration: 1700 Loss: 3.8806188106536865\n",
      "Epoch: 5/20 Iteration: 1800 Loss: 4.019192218780518\n",
      "Epoch: 5/20 Iteration: 1900 Loss: 3.9433438777923584\n",
      "Epoch: 5/20 Iteration: 2000 Loss: 3.5301733016967773\n",
      "Epoch: 5/20 Iteration: 2100 Loss: 3.339850902557373\n",
      "Epoch: 6/20 Iteration: 2200 Loss: 3.263195514678955\n",
      "Epoch: 6/20 Iteration: 2300 Loss: 3.49224591255188\n",
      "Epoch: 6/20 Iteration: 2400 Loss: 3.3105552196502686\n",
      "Epoch: 6/20 Iteration: 2500 Loss: 3.5269527435302734\n",
      "Epoch: 6/20 Iteration: 2600 Loss: 3.201812505722046\n",
      "Epoch: 7/20 Iteration: 2700 Loss: 3.242086410522461\n",
      "Epoch: 7/20 Iteration: 2800 Loss: 3.1673355102539062\n",
      "Epoch: 7/20 Iteration: 2900 Loss: 3.122633457183838\n",
      "Epoch: 7/20 Iteration: 3000 Loss: 2.970158100128174\n",
      "Epoch: 8/20 Iteration: 3100 Loss: 3.2317404747009277\n",
      "Epoch: 8/20 Iteration: 3200 Loss: 3.269364833831787\n",
      "Epoch: 8/20 Iteration: 3300 Loss: 2.9055800437927246\n",
      "Epoch: 8/20 Iteration: 3400 Loss: 3.2079575061798096\n",
      "Epoch: 9/20 Iteration: 3500 Loss: 3.0836925506591797\n",
      "Epoch: 9/20 Iteration: 3600 Loss: 2.8714325428009033\n",
      "Epoch: 9/20 Iteration: 3700 Loss: 2.8443427085876465\n",
      "Epoch: 9/20 Iteration: 3800 Loss: 3.017503261566162\n",
      "Epoch: 9/20 Iteration: 3900 Loss: 2.6204774379730225\n",
      "Epoch: 10/20 Iteration: 4000 Loss: 2.7962679862976074\n",
      "Epoch: 10/20 Iteration: 4100 Loss: 2.9198434352874756\n",
      "Epoch: 10/20 Iteration: 4200 Loss: 2.87804913520813\n",
      "Epoch: 10/20 Iteration: 4300 Loss: 2.5235137939453125\n",
      "Epoch: 11/20 Iteration: 4400 Loss: 2.906075954437256\n",
      "Epoch: 11/20 Iteration: 4500 Loss: 3.011983871459961\n",
      "Epoch: 11/20 Iteration: 4600 Loss: 2.7686095237731934\n",
      "Epoch: 11/20 Iteration: 4700 Loss: 2.9228110313415527\n",
      "Epoch: 12/20 Iteration: 4800 Loss: 2.8477683067321777\n",
      "Epoch: 12/20 Iteration: 4900 Loss: 2.612544298171997\n",
      "Epoch: 12/20 Iteration: 5000 Loss: 2.5081565380096436\n",
      "Epoch: 12/20 Iteration: 5100 Loss: 2.5184385776519775\n",
      "Epoch: 12/20 Iteration: 5200 Loss: 2.5625240802764893\n",
      "Epoch: 13/20 Iteration: 5300 Loss: 2.731290102005005\n",
      "Epoch: 13/20 Iteration: 5400 Loss: 2.84778094291687\n",
      "Epoch: 13/20 Iteration: 5500 Loss: 2.5643632411956787\n",
      "Epoch: 13/20 Iteration: 5600 Loss: 2.7074968814849854\n",
      "Epoch: 14/20 Iteration: 5700 Loss: 2.4623801708221436\n",
      "Epoch: 14/20 Iteration: 5800 Loss: 2.870107412338257\n",
      "Epoch: 14/20 Iteration: 5900 Loss: 2.6419663429260254\n",
      "Epoch: 14/20 Iteration: 6000 Loss: 2.53572678565979\n",
      "Epoch: 15/20 Iteration: 6100 Loss: 2.7161405086517334\n",
      "Epoch: 15/20 Iteration: 6200 Loss: 2.462271213531494\n",
      "Epoch: 15/20 Iteration: 6300 Loss: 2.5822322368621826\n",
      "Epoch: 15/20 Iteration: 6400 Loss: 2.728172540664673\n",
      "Epoch: 15/20 Iteration: 6500 Loss: 2.3609883785247803\n",
      "Epoch: 16/20 Iteration: 6600 Loss: 2.558506965637207\n",
      "Epoch: 16/20 Iteration: 6700 Loss: 2.6919898986816406\n",
      "Epoch: 16/20 Iteration: 6800 Loss: 2.332758665084839\n",
      "Epoch: 16/20 Iteration: 6900 Loss: 2.4275386333465576\n",
      "Epoch: 17/20 Iteration: 7000 Loss: 2.7708888053894043\n",
      "Epoch: 17/20 Iteration: 7100 Loss: 2.519447088241577\n",
      "Epoch: 17/20 Iteration: 7200 Loss: 2.44439435005188\n",
      "Epoch: 17/20 Iteration: 7300 Loss: 2.6508121490478516\n",
      "Epoch: 18/20 Iteration: 7400 Loss: 2.708397150039673\n",
      "Epoch: 18/20 Iteration: 7500 Loss: 2.345707654953003\n",
      "Epoch: 18/20 Iteration: 7600 Loss: 2.378099203109741\n",
      "Epoch: 18/20 Iteration: 7700 Loss: 2.4663662910461426\n",
      "Epoch: 18/20 Iteration: 7800 Loss: 2.3426260948181152\n",
      "Epoch: 19/20 Iteration: 7900 Loss: 2.6268398761749268\n",
      "Epoch: 19/20 Iteration: 8000 Loss: 2.353820562362671\n",
      "Epoch: 19/20 Iteration: 8100 Loss: 2.221730947494507\n",
      "Epoch: 19/20 Iteration: 8200 Loss: 2.225994110107422\n",
      "Epoch: 20/20 Iteration: 8300 Loss: 2.4984164237976074\n",
      "Epoch: 20/20 Iteration: 8400 Loss: 2.4393703937530518\n",
      "Epoch: 20/20 Iteration: 8500 Loss: 2.408766269683838\n",
      "Epoch: 20/20 Iteration: 8600 Loss: 2.3704581260681152\n",
      "Epoch: 20/20 Iteration: 8700 Loss: 2.3364334106445312\n",
      "Time:549.0194776058197s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net = main()\n",
    "    torch.save(net.state_dict(),'{}/model-{}.pth'.format(flags.checkpoint_path,\"final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int_to_vocab [(0, '<BOS>'), (1, '<EOS>'), (2, '月'), (3, '公司'), (4, '年'), (5, '日'), (6, '中'), (7, '中国'), (8, '5G'), (9, '新')]\n",
      "Vocabulary size 48469\n",
      "[[    0     1     0    19     2     5    31   785   777    23]\n",
      " [   62   155    79  1300  1233 18149  4255 18150   923   110]\n",
      " [ 3381     1     0  2267 13263    64    39    59  4872  2303]\n",
      " [ 8972    77    73    34     6  6492  1662   366  1662  1056]\n",
      " [ 5838  6042     1     0   941  6042 15493  1646  3927     4]\n",
      " [    5  1191  1259   731   197     4     2  1191 15617 13578]\n",
      " [  132    29   638   250  1244   489   530   532   440     1]\n",
      " [    6   138 33373     1     0     1     0 15890   258  2215]\n",
      " [ 2658  9604     1     0     1     0    99  2054  5683  4500]\n",
      " [ 3081   137 24479 16072  1573   137 24480  2328 24481 10018]]\n",
      "[[    1     0    19     2     5    31   785   777    23    42]\n",
      " [  155    79  1300  1233 18149  4255 18150   923   110   148]\n",
      " [    1     0  2267 13263    64    39    59  4872  2303  2455]\n",
      " [   77    73    34     6  6492  1662   366  1662  1056   606]\n",
      " [ 6042     1     0   941  6042 15493  1646  3927     4     1]\n",
      " [ 1191  1259   731   197     4     2  1191 15617 13578 15618]\n",
      " [   29   638   250  1244   489   530   532   440     1     0]\n",
      " [  138 33373     1     0     1     0 15890   258  2215     1]\n",
      " [ 9604     1     0     1     0    99  2054  5683  4500  5291]\n",
      " [  137 24479 16072  1573   137 24480  2328 24481 10018     1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNModule(\n",
       "  (embedding): Embedding(48469, 128)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (dense): Linear(in_features=128, out_features=48469, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "    flags.train_file_path, flags.batch_size, flags.seq_size)\n",
    "net=RNNModule(n_vocab, flags.seq_size,flags.embedding_size, flags.lstm_size)\n",
    "net.load_state_dict(torch.load(\"checkpoint/model-final.pth\"))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use LSTM model to predict\n",
      "1 [MASK] = ['色情', '不想', '<EOS>', '发生', '很难'] - 座椅\n",
      "2 [MASK] = ['推测', '现状', '企业', '头部', '市'] - 汽车\n",
      "3 [MASK] = ['B', '采访', '量产', 'APP', '每经'] - 欧洲\n",
      "4 [MASK] = ['设计', '音效', '低价', '元素', '体系'] - 玻璃\n",
      "5 [MASK] = ['就活', '增加', '<EOS>', '支付', '显得'] - 利润\n",
      "6√ [MASK] = ['绑定', '故宫', '消费', '相关', '查看'] - 故宫\n",
      "7 [MASK] = ['新能源', '固定', '销售', '客户', '品牌'] - 颁发\n",
      "8 [MASK] = ['<EOS>', '新', '专注', '希望', '达成'] - 广阔\n",
      "9 [MASK] = ['<EOS>', '这是', '计划', '推出', '包括'] - 对话\n",
      "10 [MASK] = ['全球', '时间', '产品', '操作系统', '5G'] - 手机\n",
      "11 [MASK] = ['未来', '<EOS>', '中', '最终', '推进'] - 神经网络\n",
      "12 [MASK] = ['中', '技术', '公司', '实验室', '未成年人'] - 机器人\n",
      "13 [MASK] = ['预期', '影响力', '深远', '优势', '短期'] - 应用\n",
      "14√ [MASK] = ['增加', '增长', '提升', '提高', '经营'] - 增长\n",
      "15 [MASK] = ['操作', '兆瓦', '运行', '算法', '识别'] - 学习\n",
      "16 [MASK] = ['技术', '创新', 'AI', '支持', '带来'] - 门店\n",
      "17 [MASK] = ['<EOS>', '密密麻麻', '进一步', '提供', '更'] - 干燥\n",
      "18 [MASK] = ['棒棒糖', '身份验证', '电信公司', '情报机构', '努力'] - 政府\n",
      "19√ [MASK] = ['厂商', '业务', '行业', '产品', '工作'] - 厂商\n",
      "20 [MASK] = ['多达', '减少', '这项', '波音', '价值'] - 裁员\n",
      "21 [MASK] = ['优惠', '大礼包', '一半', '需', '速度'] - 需求\n",
      "22 [MASK] = ['将会弃', '保障', 'Go', '盗用', '视频'] - 他们\n",
      "23 [MASK] = ['材料', '覆盖', '销售', '领域', '房产'] - 鲜艳\n",
      "24 [MASK] = ['保护', '60', '<EOS>', '保守', '技术'] - 打击\n",
      "25 [MASK] = ['违反', '抄袭', '质量', '滥用', '多元化'] - 传销\n",
      "26 [MASK] = ['假', '数据', '前', '编辑', '广告'] - 招聘\n",
      "27 [MASK] = ['全球', '<EOS>', '数据', '发展', '中国'] - 中心\n",
      "28 [MASK] = ['公司', '<EOS>', '测试人员', '京东', '华为'] - 人类\n",
      "29 [MASK] = ['约', '轨道', '<EOS>', '100', 'SpaceX'] - 程序\n",
      "30 [MASK] = ['维修服务', '通话时间', '优先', '原先', '延后'] - 班主任\n",
      "31 [MASK] = ['<EOS>', '提升', '更', '做', '卖'] - 浪费\n",
      "32 [MASK] = ['推出', '明年', '2020', '斥资', '2021'] - 建设\n",
      "33 [MASK] = ['成熟', '高清', '完善', '建设', '承压'] - 发展\n",
      "34 [MASK] = ['影响', '更', '价值', '中来', '发展'] - 信息技术\n",
      "35√ [MASK] = ['最为', '阶段', '平台', '行业', '道'] - 阶段\n",
      "36 [MASK] = ['敏感', '超', '最快', '最多', '若干'] - 公安局\n",
      "37√ [MASK] = ['诽谤', '裁决', '公正', '操纵', '文字'] - 诽谤\n",
      "38 [MASK] = ['证券交易', '选择', '合作', '资本', '索赔'] - 商务部\n",
      "39 [MASK] = ['想', '<EOS>', '参与', '中', '希望'] - 关注\n",
      "40√ [MASK] = ['明天', '量', '服务', '产品', '共计'] - 服务\n",
      "41 [MASK] = ['两个', '平台', '行业', '微博', '估计'] - 成本\n",
      "42 [MASK] = ['郑小松', '覃艺', '娱乐', '核验', '后果'] - 泄露\n",
      "43 [MASK] = ['价比', '出太多', '监禁', '价格比', '泛化'] - 运营商\n",
      "44 [MASK] = ['组件', '情况', '功能', '称', '中'] - 拖地\n",
      "45 [MASK] = ['<EOS>', '员工', '疾病', '志愿者', '超越'] - 观众\n",
      "46 [MASK] = ['结果显示', 'Twitter', '扫', '最具', '邀请书'] - 基础\n",
      "47 [MASK] = ['提供', '<EOS>', '信息', '数据', '商家'] - 成员\n",
      "48 [MASK] = ['损害', '服务', '<EOS>', '迈进', '定向'] - 改善\n",
      "49 [MASK] = ['团队', '技术', '全球', '人工智能', '强大'] - 资本\n",
      "50√ [MASK] = ['白酒', '端口', '容量', '个人信用', '食品'] - 白酒\n",
      "51 [MASK] = ['交易', '恢复', '两家', '标准', '方案'] - 安全\n",
      "52 [MASK] = ['房源', '切入', '表现', '<EOS>', '关注'] - 零售业\n",
      "53 [MASK] = ['发布', '发表', '推出', '离职', '举办'] - 宣布\n",
      "54 [MASK] = ['<EOS>', '履行', '料', '公司', '有限'] - 价格\n",
      "55 [MASK] = ['黑暗', '非法', '时长', '环境', '信实'] - 用户\n",
      "56 [MASK] = ['中', '包括', '缺乏', '升级版', '攻击'] - 电池\n",
      "57 [MASK] = ['用户', '均', '手机', '共计', '30'] - 屏幕\n",
      "58√ [MASK] = ['汽车', '<EOS>', '车', '计划', '电池'] - 汽车\n",
      "59√ [MASK] = ['公司', '石家庄', '零售', '一对一', '企业'] - 企业\n",
      "60 [MASK] = ['降至', '近', '10%', '游客', '相比'] - 时间\n",
      "61 [MASK] = ['去年', '两年', '<EOS>', '仅仅', '包括'] - 价格\n",
      "62 [MASK] = ['链条', '渠道', '深刻', '找', '接待'] - 面积\n",
      "63 [MASK] = ['<EOS>', '前', '时间', '2019', '2025'] - 占比\n",
      "64 [MASK] = ['企业', '新', '<EOS>', '经济', '互联网'] - 政策\n",
      "65 [MASK] = ['技术', '<EOS>', '调整', '建议', '相关'] - 法律\n",
      "66 [MASK] = ['整车', '交付', '豪华', '戴姆勒', '净资产'] - 销量\n",
      "67 [MASK] = ['期限', '车主', 'Mission', '<EOS>', '利益'] - 法律\n",
      "68 [MASK] = ['操控', '探测', '六大', '协同', '一体化'] - 远\n",
      "69 [MASK] = ['增长', '<EOS>', '占', '数据', '快速'] - 原因\n",
      "70√ [MASK] = ['发布', '几个', '变化', '中国', '做出'] - 发布\n",
      "71 [MASK] = ['<EOS>', '安装', '中', '曝光', '15'] - 版本\n",
      "72 [MASK] = ['看作', '剧烈', '财务报告', '绝不能', '趋势'] - 因素\n",
      "73√ [MASK] = ['<EOS>', '影响', '意味着', '负责', '包括'] - 影响\n",
      "74√ [MASK] = ['澎湃', '记者', '媒体', '经济学', '专家'] - 媒体\n",
      "75 [MASK] = ['稳定', '强有力', '年化', '用户', '增长'] - 功能\n",
      "76 [MASK] = ['人士', '<EOS>', '平台', '分析', '业务'] - 消费者\n",
      "77 [MASK] = ['安全性', '成本', '社区', '可靠性', '用户'] - 效率\n",
      "78 [MASK] = ['<EOS>', '2017', '苹果', '融资', '投资'] - 降\n",
      "79 [MASK] = ['<EOS>', '优化', '更', '之外', '确保'] - 产品\n",
      "80 [MASK] = ['寻找', '<EOS>', '空间', '必然选择', '监督'] - 人工智能\n",
      "81 [MASK] = ['信号', '切片', '云端', '服务', '传播'] - 需求\n",
      "82√ [MASK] = ['套餐', '误导', '发行', '流量', '加速'] - 套餐\n",
      "83√ [MASK] = ['亏损', '营收', '营业', '净利润', '主营业务'] - 营收\n",
      "84 [MASK] = ['OEM', 'MEB', '交由', '效率', '原生态'] - 环境\n",
      "85 [MASK] = ['阶段', '面向', '更', '高等', '产品'] - 资金\n",
      "86 [MASK] = ['电影', '品类', '效益', '生态', '产业链'] - 安全\n",
      "87√ [MASK] = ['平台', '服务', '原生', '业务', '上线'] - 服务\n",
      "88√ [MASK] = ['黑箱', '收益', '蔚来', '亏损', '猎豹'] - 亏损\n",
      "89√ [MASK] = ['国家', '多家', '<EOS>', '腾讯', '多个'] - 国家\n",
      "90 [MASK] = ['<EOS>', '公司', '美国', 'B2B', '运营商'] - 损失\n",
      "91 [MASK] = ['5G', '构建', '<EOS>', '方向', '语音'] - 合作\n",
      "92 [MASK] = ['思路', '主题', '新机遇', '契机', '节奏'] - 速度\n",
      "93 [MASK] = ['企业', '发展', '行业', '上网', '金融'] - 人工智能\n",
      "94 [MASK] = ['产品', '用户', '行业', '机型', '便是'] - 智能机\n",
      "95 [MASK] = ['商店', '机场', 'VR', '智能', '做'] - 产品\n",
      "96 [MASK] = ['阿里', '想要', '告诉', '可行性', '自上'] - 无线电\n",
      "97 [MASK] = ['<EOS>', '去年', '二季度', '收入', '同比增加'] - 营收\n",
      "98 [MASK] = ['<EOS>', '技术', '更', '灵活', '新'] - 安全\n",
      "99 [MASK] = ['<EOS>', '大幅', '需', '月', '包含'] - 价格\n",
      "100√ [MASK] = ['企业', '旅游', '互联网', '机构', '科技'] - 企业\n",
      "Accuracy: 18.00%\n"
     ]
    }
   ],
   "source": [
    "groundtrue = [line[:-1] for line in open(\"answer.txt\",\"r\",encoding=\"utf-8\")]\n",
    "acc = 0\n",
    "\n",
    "myanswer = open(\"myanswer.txt\",\"w\",encoding=\"utf-8\")\n",
    "print(\"Use LSTM model to predict\")\n",
    "\n",
    "with open(\"questions.txt\",\"r\",encoding=\"utf-8\") as question_file:\n",
    "    for i,question_str in enumerate(question_file,1):\n",
    "        pred = predict(device, net, question_str, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "        if groundtrue[i-1] in pred:\n",
    "            acc += 1\n",
    "            print(\"{}√ [MASK] = {} - {}\".format(i,pred,groundtrue[i-1]),flush=True)\n",
    "        else:\n",
    "            print(\"{} [MASK] = {} - {}\".format(i,pred,groundtrue[i-1]),flush=True)\n",
    "        myanswer.write(\"{}\\n\".format(pred[0]))\n",
    "print(\"Accuracy: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
