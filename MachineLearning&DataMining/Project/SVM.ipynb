{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "n_users = len(data)\n",
    "posts = data[\"posts\"]\n",
    "labels = data[\"type\"].unique()\n",
    "n_class = len(labels)\n",
    "type2num = {label: i for i,label in enumerate(labels)}\n",
    "Y = np.array(list(map(lambda s: type2num[s], data[\"type\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution():\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    type_val = data[\"type\"].value_counts()\n",
    "    labels = type_val.keys()\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, type_val.values)\n",
    "    ax.set_ylabel(\"# of people\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels,rotation='45')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user posts\n"
     ]
    }
   ],
   "source": [
    "def generate_posts(path=\"\"):\n",
    "    filename = os.path.join(path,\"posts.pkl\")\n",
    "    user_posts = []\n",
    "    if not os.path.isfile(filename):\n",
    "        stopwords = pd.read_csv(\"stopwords.csv\").to_numpy().reshape(-1)\n",
    "        stopwords = np.array(list(map(lambda s: s.replace(\"'\",\"\"),stopwords)))\n",
    "        for uid in range(n_users):\n",
    "            # add empty space first (better used for regex parsing)\n",
    "            new_post = posts[uid].replace(\"|||\",\" ||| \")\n",
    "            new_post = new_post.replace(\",\",\", \")\n",
    "            # remove url links\n",
    "            new_post = re.sub(\"(http|https):\\/\\/.*?( |'|\\\")\",\"\",new_post)\n",
    "            # avoid words in two sentences merged together after removing spaces\n",
    "            new_post = new_post.replace(\".\",\". \")\n",
    "            # change emoji to word\n",
    "            new_post = new_post.replace(\":)\",\" smile \")\n",
    "            new_post = new_post.replace(\":(\",\" sad \")\n",
    "            # remove useless numbers and punctuations\n",
    "            new_post = re.sub(r\"[0-9]+\", \"\", new_post)\n",
    "            new_post = new_post.translate(str.maketrans('', '', string.punctuation))\n",
    "            # remove redundant empty spaces\n",
    "            new_post = re.sub(\" +\",\" \",new_post).strip()\n",
    "            # make all characters lower\n",
    "            new_post = new_post.lower()\n",
    "            temp = []\n",
    "            # remove stopping words\n",
    "            for word in new_post.split():\n",
    "                if len(word) != 1 and word not in stopwords:\n",
    "                    temp.append(word)\n",
    "            user_posts.append(temp)\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating word list\")\n",
    "        pickle.dump(user_posts,open(filename,\"wb\"))\n",
    "        with open(\"posts.corpus\",\"w\") as corpus:\n",
    "            for post in user_posts:\n",
    "                corpus.write(\" \".join(post) + \"\\n\")\n",
    "        print(\"Saved to posts.corpus!\")\n",
    "    else:\n",
    "        user_posts = pickle.load(open(filename,\"rb\"))\n",
    "        print(\"Loaded user posts\")\n",
    "    return user_posts\n",
    "\n",
    "user_posts = generate_posts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tfidf.npy\n",
      "Vocabulary size: 27127\n"
     ]
    }
   ],
   "source": [
    "def generate_tfidf(user_posts, path=\"\", retrain=False):\n",
    "    filename = os.path.join(path,\"tfidf.npy\")\n",
    "    if not os.path.isfile(filename) or retrain:\n",
    "        word_lst = []\n",
    "        for post in user_posts:\n",
    "            word_lst += post\n",
    "\n",
    "        # make dictionary (used for TF-IDF)\n",
    "        word_counts = Counter(word_lst)\n",
    "        # remove words that don't occur too frequently\n",
    "        print(\"# of words before:\",len(word_counts))\n",
    "        for word in list(word_counts): # avoid changing size\n",
    "            if word_counts[word] < 6:\n",
    "                del word_counts[word]\n",
    "        print(\"# of words after:\",len(word_counts))\n",
    "\n",
    "        # generate IDF value\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        int_to_word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        word_to_int = {w: k for k, w in int_to_word.items()}\n",
    "        np.save(\"int2word.npy\",int_to_word)\n",
    "        idf = np.zeros((len(sorted_vocab),))\n",
    "        for uid, post in enumerate(user_posts):\n",
    "            set_words = set(post) # avoid duplication\n",
    "            for word in set_words:\n",
    "                if word in sorted_vocab:\n",
    "                    idf[word_to_int[word]] += 1 # count frequency\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        idf = np.log(len(user_posts) / (idf + 1)) # avoid divided by 0\n",
    "        print(\"Finished generating IDF values\")\n",
    "        np.save(\"idf.npy\",idf)\n",
    "\n",
    "        # generate TF value\n",
    "        tfidf_values = np.zeros((len(user_posts),len(idf)))\n",
    "        for i, post in enumerate(user_posts):\n",
    "            for post_word in post:\n",
    "                idx = word_to_int.get(post_word,None)\n",
    "                if idx != None:\n",
    "                    tfidf_values[i][idx] += 1\n",
    "            if len(post) != 0:\n",
    "                tfidf_values[i] /= len(post)\n",
    "        print(\"Finished generating TF values\")\n",
    "        tfidf_values *= idf\n",
    "        print(tfidf_values.shape)\n",
    "        np.save(filename,tfidf_values)\n",
    "        print(\"Saved to {}!\".format(filename))\n",
    "    else:\n",
    "        tfidf_values = np.load(filename,allow_pickle=True)\n",
    "        print(\"Loaded {}\".format(filename))\n",
    "    n_words = tfidf_values.shape[1]\n",
    "    print('Vocabulary size:', n_words)\n",
    "    return tfidf_values\n",
    "\n",
    "tfidf = generate_tfidf(user_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained Word2Vec Model\n"
     ]
    }
   ],
   "source": [
    "def train_w2v(RETRAIN=False):\n",
    "    if not os.path.isfile(\"posts.bin\") or RETRAIN:\n",
    "        print(\"Training Word2Vec Model ...\")\n",
    "        start = time.time()\n",
    "        w2v = Word2Vec(corpus_file=\"posts.corpus\",size=EMBEDDING_SIZE,\n",
    "                       window=5,min_count=1,iter=40,\n",
    "                       workers=multiprocessing.cpu_count())\n",
    "        end = time.time()\n",
    "        print(\"Word2Vec Time: {:.2f}s\".format(end - start))\n",
    "        w2v.save(\"posts.bin\")\n",
    "        w2v.wv.save_word2vec_format(\"posts.vec\",binary=False)\n",
    "    else:\n",
    "        w2v = Word2Vec.load(\"posts.bin\")\n",
    "        print(\"Loaded pretrained Word2Vec Model\")\n",
    "    return w2v\n",
    "\n",
    "wv = train_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentence vectors ...\n",
      "Finished! Time: 17.10s\n",
      "(8675, 1024)\n"
     ]
    }
   ],
   "source": [
    "def aggregate_wv(n_users,user_posts,w2v,filename,RETRAIN=False):\n",
    "    if not os.path.isfile(filename) or RETRAIN:\n",
    "        print(\"Generating sentence vectors ...\")\n",
    "        begin_time = time.time()\n",
    "        user_vec = np.zeros((n_users,EMBEDDING_SIZE))\n",
    "        for uid, post in enumerate(user_posts):\n",
    "            cnt = 0\n",
    "            for uid, word in enumerate(post):\n",
    "                try:\n",
    "                    user_vec[uid] += w2v.wv[word]\n",
    "                    cnt += 1\n",
    "                except:\n",
    "                    pass\n",
    "            if cnt != 0: # avoid divided by 0\n",
    "                user_vec[uid] /= cnt\n",
    "            if uid * 10 % n_users == 0:\n",
    "                print(\"Done {}%={}/{}\".format(uid*100//n_users, uid, n_users),flush=True)\n",
    "        end_time = time.time()\n",
    "        np.save(filename,user_vec)\n",
    "        print(\"Finished! Time: {:.2f}s\".format(end_time - begin_time))\n",
    "    else:\n",
    "        user_vec = np.load(filename)\n",
    "        print(\"Loaded aggregated sentence vectors ({})\".format(filename))\n",
    "    return user_vec\n",
    "\n",
    "X_vec = aggregate_wv(n_users,user_posts,wv,\"w2v.feat\")\n",
    "print(X_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_balanced(data, target, test_size=0.2):\n",
    "    classes = np.unique(target)\n",
    "    n_test = np.round(len(target) * test_size)\n",
    "    n_train = max(0, len(target) - n_test)\n",
    "    n_train_per_class = max(1, int(np.floor(n_train / len(classes))))\n",
    "    n_test_per_class = max(1, int(np.floor(n_test / len(classes))))\n",
    "\n",
    "    idxs = []\n",
    "    for cl in classes:\n",
    "        n_in_class = np.sum(target == cl)\n",
    "        n_train_per_class = np.round(n_in_class * (1 - test_size))\n",
    "        n_test_per_class = max(0, n_in_class - n_train_per_class)\n",
    "        if (n_train_per_class + n_test_per_class) > np.sum(target == cl):\n",
    "            # if data has too few samples for this class, do upsampling\n",
    "            # split the data to training and testing before sampling so data points won't be\n",
    "            # shared among training and test data\n",
    "            splitidx = int(np.ceil(n_train_per_class / (n_train_per_class+n_test_per_class) * np.sum(target == cl)))\n",
    "            idxs.append(np.r_[np.random.choice(np.nonzero(target == cl)[0][:splitidx], n_train_per_class),\n",
    "                np.random.choice(np.nonzero(target == cl)[0][splitidx:], n_test_per_class)])\n",
    "        else:\n",
    "            print(cl)\n",
    "            idxs.append(np.random.choice(np.nonzero(target == cl)[0], n_train_per_class+n_test_per_class,\n",
    "                replace=False))\n",
    "\n",
    "    # take same num of samples from all classes\n",
    "    idx_train = np.concatenate([x[:n_train_per_class] for x in idxs])\n",
    "    idx_test = np.concatenate([x[n_train_per_class:(n_train_per_class+n_test_per_class)] for x in idxs])\n",
    "\n",
    "    X_train = data[idx_train,:]\n",
    "    X_test = data[idx_test,:]\n",
    "    y_train = target[idx_train]\n",
    "    y_test = target[idx_test]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf\n",
    "# X = X_vec\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "# sss = StratifiedShuffleSplit(test_size=0.3)\n",
    "X_train, X_test, y_train, y_test = split_balanced(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    clf = RandomForestClassifier(n_jobs=multiprocessing.cpu_count()) # use all processors\n",
    "    clf.fit(X_train, y_train)\n",
    "    predict = clf.score(X_test, y_test)\n",
    "    print(\"Random Forest acc: {:.2f}%\".format(predict * 100))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test,y_pred,target_names=labels))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest acc: 50.06%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INFJ       0.37      0.71      0.49       108\n",
      "        ENTP       0.40      0.64      0.49       108\n",
      "        INTP       0.37      0.69      0.48       108\n",
      "        INTJ       0.38      0.66      0.48       108\n",
      "        ENTJ       0.51      0.33      0.40       108\n",
      "        ENFJ       0.87      0.56      0.69       108\n",
      "        INFP       0.28      0.53      0.37       108\n",
      "        ENFP       0.41      0.60      0.49       108\n",
      "        ISFP       0.69      0.63      0.66       108\n",
      "        ISTP       0.64      0.56      0.60       108\n",
      "        ISFJ       0.70      0.57      0.63       108\n",
      "        ISTJ       0.86      0.56      0.68       108\n",
      "        ESTP       0.94      0.42      0.58       108\n",
      "        ESFP       0.94      0.14      0.24       108\n",
      "        ESTJ       1.00      0.18      0.30       108\n",
      "        ESFJ       1.00      0.22      0.36       108\n",
      "\n",
      "    accuracy                           0.50      1728\n",
      "   macro avg       0.65      0.50      0.50      1728\n",
      "weighted avg       0.65      0.50      0.50      1728\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=48, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = random_forest(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassifier(X_train, y_train, X_test, y_test):\n",
    "    clf = svm.SVC(kernel=\"linear\",verbose=True)\n",
    "    clf = OneVsRestClassifier(svm.SVC(kernel=\"linear\"),n_jobs=multiprocessing.cpu_count())\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = np.sum(y_pred == y_test) / len(y_pred)\n",
    "    print(\"Support Vector Machine (SVM) acc: {:.2f}%\".format(acc * 100))\n",
    "    print(classification_report(y_test,y_pred,target_names=labels))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (SVM) acc: 60.88%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INFJ       0.40      0.65      0.49        66\n",
      "        ENTP       0.54      0.65      0.59        89\n",
      "        INTP       0.47      0.77      0.59        66\n",
      "        INTJ       0.50      0.82      0.62        66\n",
      "        ENTJ       0.60      0.62      0.61       104\n",
      "        ENFJ       0.74      0.63      0.68       126\n",
      "        INFP       0.30      0.62      0.40        52\n",
      "        ENFP       0.53      0.68      0.59        84\n",
      "        ISFP       0.71      0.60      0.65       128\n",
      "        ISTP       0.61      0.53      0.57       125\n",
      "        ISFJ       0.66      0.59      0.62       121\n",
      "        ISTJ       0.76      0.66      0.71       124\n",
      "        ESTP       0.83      0.60      0.70       150\n",
      "        ESFP       0.58      0.51      0.55       123\n",
      "        ESTJ       0.79      0.55      0.65       154\n",
      "        ESFJ       0.72      0.52      0.60       150\n",
      "\n",
      "    accuracy                           0.61      1728\n",
      "   macro avg       0.61      0.63      0.60      1728\n",
      "weighted avg       0.65      0.61      0.62      1728\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='linear', max_iter=-1,\n",
       "                                  probability=False, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False),\n",
       "                    n_jobs=48)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVMClassifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_idx, test_idx in sss.split(X, Y):\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "#     random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svm,open(\"svm.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (SVM) acc: 60.88%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INFJ       0.65      0.40      0.49       108\n",
      "        ENTP       0.65      0.54      0.59       108\n",
      "        INTP       0.77      0.47      0.59       108\n",
      "        INTJ       0.82      0.50      0.62       108\n",
      "        ENTJ       0.62      0.60      0.61       108\n",
      "        ENFJ       0.63      0.74      0.68       108\n",
      "        INFP       0.62      0.30      0.40       108\n",
      "        ENFP       0.68      0.53      0.59       108\n",
      "        ISFP       0.60      0.71      0.65       108\n",
      "        ISTP       0.53      0.61      0.57       108\n",
      "        ISFJ       0.59      0.66      0.62       108\n",
      "        ISTJ       0.66      0.76      0.71       108\n",
      "        ESTP       0.60      0.83      0.70       108\n",
      "        ESFP       0.51      0.58      0.55       108\n",
      "        ESTJ       0.55      0.79      0.65       108\n",
      "        ESFJ       0.52      0.72      0.60       108\n",
      "\n",
      "    accuracy                           0.61      1728\n",
      "   macro avg       0.63      0.61      0.60      1728\n",
      "weighted avg       0.63      0.61      0.60      1728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# y_pred = svm.predict(X_test)\n",
    "acc = np.sum(y_pred == y_test) / len(y_pred)\n",
    "print(\"Support Vector Machine (SVM) acc: {:.2f}%\".format(acc * 100))\n",
    "print(classification_report(y_test,y_pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFP    1832\n",
       "INFJ    1470\n",
       "INTP    1304\n",
       "INTJ    1091\n",
       "ENTP     685\n",
       "ENFP     675\n",
       "ISTP     337\n",
       "ISFP     271\n",
       "ENTJ     231\n",
       "ISTJ     205\n",
       "ENFJ     190\n",
       "ISFJ     166\n",
       "ESTP      89\n",
       "ESFP      48\n",
       "ESFJ      42\n",
       "ESTJ      39\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
