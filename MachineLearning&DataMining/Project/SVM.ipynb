{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "n_users = len(data)\n",
    "posts = data[\"posts\"]\n",
    "labels = data[\"type\"].unique()\n",
    "n_class = len(labels)\n",
    "type2num = {label: i for i,label in enumerate(labels)}\n",
    "Y = np.array(list(map(lambda s: type2num[s], data[\"type\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution():\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    type_val = data[\"type\"].value_counts()\n",
    "    labels = type_val.keys()\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, type_val.values)\n",
    "    ax.set_ylabel(\"# of people\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels,rotation='45')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posts(path=\"\"):\n",
    "    filename = os.path.join(path,\"posts.pkl\")\n",
    "    user_posts = []\n",
    "    if not os.path.isfile(filename):\n",
    "        stopwords = pd.read_csv(\"stopwords.csv\").to_numpy().reshape(-1)\n",
    "        stopwords = np.array(list(map(lambda s: s.replace(\"'\",\"\"),stopwords)))\n",
    "        for uid in range(n_users):\n",
    "            # add empty space first (better used for regex parsing)\n",
    "            new_post = posts[uid].replace(\"|||\",\" ||| \")\n",
    "            new_post = new_post.replace(\",\",\", \")\n",
    "            # remove url links\n",
    "            new_post = re.sub(\"(http|https):\\/\\/.*?( |'|\\\")\",\"\",new_post)\n",
    "            # avoid words in two sentences merged together after removing spaces\n",
    "            new_post = new_post.replace(\".\",\". \")\n",
    "            # remove useless numbers and punctuations\n",
    "            new_post = re.sub(r\"[0-9]+\", \"\", new_post)\n",
    "            new_post = new_post.translate(str.maketrans('', '', string.punctuation))\n",
    "            # remove redundant empty spaces\n",
    "            new_post = re.sub(\" +\",\" \",new_post).strip()\n",
    "            # make all characters lower\n",
    "            new_post = new_post.lower()\n",
    "            temp = []\n",
    "            # remove stopping words\n",
    "            for word in new_post.split():\n",
    "                if len(word) != 1 and word not in stopwords:\n",
    "                    temp.append(word)\n",
    "            user_posts.append(temp)\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating word list\")\n",
    "        pickle.dump(user_posts,open(filename,\"wb\"))\n",
    "    else:\n",
    "        user_posts = pickle.load(open(filename,\"rb\"))\n",
    "        print(\"Loaded user posts\")\n",
    "    return user_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(user_posts,path=\"\"):\n",
    "    filename = os.path.join(path,\"word_dict.npz\")\n",
    "    if not os.path.isfile(filename):\n",
    "        word_lst = []\n",
    "        for post in user_posts:\n",
    "            word_lst += post\n",
    "\n",
    "        # make dictionary (used for bag of words, BOW)\n",
    "        word_counts = Counter(word_lst)\n",
    "        word_counts[\"<UNK>\"] = max(word_counts.values()) + 1\n",
    "        # remove words that don't occur too frequently\n",
    "        print(\"# of words before:\",len(word_counts))\n",
    "        for word in list(word_counts): # avoid changing size\n",
    "            if word_counts[word] < 6:\n",
    "                del word_counts[word]\n",
    "        print(\"# of words after:\",len(word_counts))\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        int_to_word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        word_to_int = {w: k for k, w in int_to_word.items()}\n",
    "        np.savez(filename,int2word=int_to_word,word2int=word_to_int)\n",
    "    else:\n",
    "        infile = np.load(filename,allow_pickle=True)\n",
    "        int_to_word = infile[\"int2word\"].item()\n",
    "        word_to_int = infile[\"word2int\"].item()\n",
    "        print(\"Loaded {}\".format(filename))\n",
    "    n_words = len(int_to_word)\n",
    "    print('Vocabulary size:', n_words)\n",
    "    return word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(user_posts,word_to_int):\n",
    "    filename = \"bow.npy\"\n",
    "    if not os.path.isfile(filename):\n",
    "        n_users = len(user_posts)\n",
    "        n_words = len(word_to_int)\n",
    "        feature = np.zeros((n_users,n_words))\n",
    "        print(feature.shape)\n",
    "        for uid, post in enumerate(user_posts):\n",
    "            count = Counter(post)\n",
    "            for key in count:\n",
    "                feature[uid][word_to_int.get(key,0)] = count[key]\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating BoW model\")\n",
    "        np.save(filename,feature)\n",
    "        print(\"Saved {}\".format(filename))\n",
    "    else:\n",
    "        feature = np.load(filename)\n",
    "        print(\"Loaded BoW model\")\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user posts\n",
      "Loaded word_dict.npz\n",
      "Vocabulary size: 27129\n",
      "(8675, 27129)\n",
      "Done 0/8675 = 0.0%\n",
      "Done 347/8675 = 4.0%\n",
      "Done 694/8675 = 8.0%\n",
      "Done 1041/8675 = 12.0%\n",
      "Done 1388/8675 = 16.0%\n",
      "Done 1735/8675 = 20.0%\n",
      "Done 2082/8675 = 24.0%\n",
      "Done 2429/8675 = 28.0%\n",
      "Done 2776/8675 = 32.0%\n",
      "Done 3123/8675 = 36.0%\n",
      "Done 3470/8675 = 40.0%\n",
      "Done 3817/8675 = 44.0%\n",
      "Done 4164/8675 = 48.0%\n",
      "Done 4511/8675 = 52.0%\n",
      "Done 4858/8675 = 56.0%\n",
      "Done 5205/8675 = 60.0%\n",
      "Done 5552/8675 = 64.0%\n",
      "Done 5899/8675 = 68.0%\n",
      "Done 6246/8675 = 72.0%\n",
      "Done 6593/8675 = 76.0%\n",
      "Done 6940/8675 = 80.0%\n",
      "Done 7287/8675 = 84.0%\n",
      "Done 7634/8675 = 88.0%\n",
      "Done 7981/8675 = 92.0%\n",
      "Done 8328/8675 = 96.0%\n",
      "Finished generating BoW model\n",
      "Saved bow.npy\n"
     ]
    }
   ],
   "source": [
    "user_posts = generate_posts()\n",
    "word2int, int2word = generate_dict(user_posts)\n",
    "X = generate_bow(user_posts,word2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "unique_X_val = []\n",
    "for attr in range(X.shape[1]): # disadvantage\n",
    "    unique_X_val.append(np.unique(X[:,attr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(verbose=True)\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.score(X_test, y_test)\n",
    "print(\"Support Vector Machine (SVM) acc: {:.2f}%\".format(predict * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
