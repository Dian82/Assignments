{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "n_users = len(data)\n",
    "posts = data[\"posts\"]\n",
    "labels = data[\"type\"].unique()\n",
    "print(data[\"type\"].value_counts())\n",
    "n_class = len(labels)\n",
    "type2num = {label: i for i,label in enumerate(labels)}\n",
    "Y = np.array(list(map(lambda s: type2num[s], data[\"type\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution():\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    type_val = data[\"type\"].value_counts()\n",
    "    labels = type_val.keys()\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, type_val.values)\n",
    "    ax.set_ylabel(\"# of people\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels,rotation='45')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posts(path=\"\"):\n",
    "    filename = os.path.join(path,\"posts.pkl\")\n",
    "    user_posts = []\n",
    "    if not os.path.isfile(filename):\n",
    "        stopwords = pd.read_csv(\"stopwords.csv\").to_numpy().reshape(-1)\n",
    "        stopwords = np.array(list(map(lambda s: s.replace(\"'\",\"\"),stopwords)))\n",
    "        for uid in range(n_users):\n",
    "            # add empty space first (better used for regex parsing)\n",
    "            new_post = posts[uid].replace(\"|||\",\" ||| \")\n",
    "            new_post = new_post.replace(\",\",\", \")\n",
    "            # remove url links\n",
    "            new_post = re.sub(\"(http|https):\\/\\/.*?( |'|\\\")\",\"\",new_post)\n",
    "            # avoid words in two sentences merged together after removing spaces\n",
    "            new_post = new_post.replace(\".\",\". \")\n",
    "            # change emoji to word\n",
    "            new_post = new_post.replace(\":)\",\" smile \")\n",
    "            new_post = new_post.replace(\":(\",\" sad \")\n",
    "            # remove useless numbers and punctuations\n",
    "            new_post = re.sub(r\"[0-9]+\", \"\", new_post)\n",
    "            new_post = new_post.translate(str.maketrans('', '', string.punctuation))\n",
    "            # remove redundant empty spaces\n",
    "            new_post = re.sub(\" +\",\" \",new_post).strip()\n",
    "            # make all characters lower\n",
    "            new_post = new_post.lower()\n",
    "            temp = []\n",
    "            # remove stopping words\n",
    "            for word in new_post.split():\n",
    "                if len(word) != 1 and word not in stopwords:\n",
    "                    temp.append(word)\n",
    "            user_posts.append(temp)\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating word list\")\n",
    "        pickle.dump(user_posts,open(filename,\"wb\"))\n",
    "        with open(\"posts.corpus\",\"w\") as corpus:\n",
    "            for post in user_posts:\n",
    "                corpus.write(\" \".join(post) + \"\\n\")\n",
    "        print(\"Saved to posts.corpus!\")\n",
    "    else:\n",
    "        user_posts = pickle.load(open(filename,\"rb\"))\n",
    "        print(\"Loaded user posts\")\n",
    "    return user_posts\n",
    "\n",
    "user_posts = generate_posts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfidf(user_posts, path=\"\", retrain=False):\n",
    "    filename = os.path.join(path,\"tfidf.npy\")\n",
    "    if not os.path.isfile(filename) or retrain:\n",
    "        word_lst = []\n",
    "        for post in user_posts:\n",
    "            word_lst += post\n",
    "\n",
    "        # make dictionary (used for TF-IDF)\n",
    "        word_counts = Counter(word_lst)\n",
    "        # remove words that don't occur too frequently\n",
    "        print(\"# of words before:\",len(word_counts))\n",
    "        for word in list(word_counts): # avoid changing size\n",
    "            if word_counts[word] < 20:\n",
    "                del word_counts[word]\n",
    "        print(\"# of words after:\",len(word_counts))\n",
    "\n",
    "        # generate IDF value\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        int_to_word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        word_to_int = {w: k for k, w in int_to_word.items()}\n",
    "        np.save(\"int2word.npy\",int_to_word)\n",
    "        idf = np.zeros((len(sorted_vocab),))\n",
    "        for uid, post in enumerate(user_posts):\n",
    "            set_words = set(post) # avoid duplication\n",
    "            for word in set_words:\n",
    "                if word in sorted_vocab:\n",
    "                    idf[word_to_int[word]] += 1 # count frequency\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        idf = np.log(len(user_posts) / (idf + 1)) # avoid divided by 0\n",
    "        print(\"Finished generating IDF values\")\n",
    "        np.save(\"idf.npy\",idf)\n",
    "\n",
    "        # generate TF value\n",
    "        tfidf_values = np.zeros((len(user_posts),len(idf)))\n",
    "        for i, post in enumerate(user_posts):\n",
    "            for post_word in post:\n",
    "                idx = word_to_int.get(post_word,None)\n",
    "                if idx != None:\n",
    "                    tfidf_values[i][idx] += 1\n",
    "            if len(post) != 0:\n",
    "                tfidf_values[i] /= len(post)\n",
    "        print(\"Finished generating TF values\")\n",
    "        tfidf_values *= idf\n",
    "        print(tfidf_values.shape)\n",
    "        np.save(filename,tfidf_values)\n",
    "        print(\"Saved to {}!\".format(filename))\n",
    "    else:\n",
    "        tfidf_values = np.load(filename,allow_pickle=True)\n",
    "        print(\"Loaded {}\".format(filename))\n",
    "    n_words = tfidf_values.shape[1]\n",
    "    print('Vocabulary size:', n_words)\n",
    "    return tfidf_values\n",
    "\n",
    "tfidf = generate_tfidf(user_posts,retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_SIZE=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(RETRAIN=True):\n",
    "    if not os.path.isfile(\"posts.bin\") or RETRAIN:\n",
    "        print(\"Training Word2Vec Model ...\")\n",
    "        start = time.time()\n",
    "        w2v = Word2Vec(corpus_file=\"posts.corpus\",size=EMBEDDING_SIZE,\n",
    "                       window=5,min_count=20,iter=30,\n",
    "                       workers=multiprocessing.cpu_count())\n",
    "        end = time.time()\n",
    "        print(\"Word2Vec Time: {:.2f}s\".format(end - start))\n",
    "        w2v.save(\"posts.bin\")\n",
    "        w2v.wv.save_word2vec_format(\"posts.vec\",binary=False)\n",
    "    else:\n",
    "        w2v = Word2Vec.load(\"posts.bin\")\n",
    "        print(\"Loaded pretrained Word2Vec Model\")\n",
    "    return w2v\n",
    "\n",
    "wv = train_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_wv(n_users,user_posts,w2v,filename,RETRAIN=False):\n",
    "    if not os.path.isfile(filename+\".npy\") or RETRAIN:\n",
    "        print(\"Generating sentence vectors ...\")\n",
    "        begin_time = time.time()\n",
    "        user_vec = np.zeros((n_users,EMBEDDING_SIZE))\n",
    "        for uid, post in enumerate(user_posts):\n",
    "            cnt = 0\n",
    "            for uid, word in enumerate(post):\n",
    "                try:\n",
    "                    user_vec[uid] += w2v.wv[word]\n",
    "                    cnt += 1\n",
    "                except:\n",
    "                    pass\n",
    "            if cnt != 0: # avoid divided by 0\n",
    "                user_vec[uid] /= cnt\n",
    "            if uid * 10 % n_users == 0:\n",
    "                print(\"Done {}%={}/{}\".format(uid*100//n_users, uid, n_users),flush=True)\n",
    "        end_time = time.time()\n",
    "        np.save(filename,user_vec)\n",
    "        print(\"Finished! Time: {:.2f}s\".format(end_time - begin_time))\n",
    "    else:\n",
    "        user_vec = np.load(filename)\n",
    "        print(\"Loaded aggregated sentence vectors ({})\".format(filename))\n",
    "    return user_vec\n",
    "\n",
    "X_vec = aggregate_wv(n_users,user_posts,wv,\"w2v.feat\")\n",
    "print(X_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_balanced(data, target, test_size=0.2):\n",
    "    classes = np.unique(target)\n",
    "    n_test = np.round(len(target) * test_size)\n",
    "    n_train = max(0, len(target) - n_test)\n",
    "\n",
    "    idxs = []\n",
    "    n_train_class = []\n",
    "    n_test_class = []\n",
    "    for i, cl in enumerate(classes):\n",
    "        n_in_class = np.sum(target == cl)\n",
    "        n_train_class.append(int(np.round(n_in_class * (1 - test_size))))\n",
    "        n_test_class.append(max(0, n_in_class - n_train_class[i]))\n",
    "        idxs.append(np.random.choice(np.nonzero(target == cl)[0],\n",
    "                                     n_train_class[i] + n_test_class[i],\n",
    "                                     replace=False))\n",
    "\n",
    "    idx_train = np.concatenate([idxs[i][:n_train_class[i]]\n",
    "                                for i in range(len(classes))])\n",
    "    idx_test = np.concatenate([idxs[i][n_train_class[i]:n_train_class[i] + n_test_class[i]]\n",
    "                               for i in range(len(classes))])\n",
    "\n",
    "    X_train = data[idx_train,:]\n",
    "    X_test = data[idx_test,:]\n",
    "    y_train = target[idx_train]\n",
    "    y_test = target[idx_test]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf\n",
    "# X = X_vec\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "# sss = StratifiedShuffleSplit(test_size=0.3)\n",
    "X_train, X_test, y_train, y_test = split_balanced(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    clf = RandomForestClassifier(n_jobs=multiprocessing.cpu_count()) # use all processors\n",
    "    clf.fit(X_train, y_train)\n",
    "    predict = clf.score(X_test, y_test)\n",
    "    print(\"Random Forest acc: {:.2f}%\".format(predict * 100))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test,y_pred,target_names=labels))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = random_forest(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassifier(X_train, y_train, X_test, y_test):\n",
    "    begin_time = time.time()\n",
    "    clf = OneVsRestClassifier(SVC(kernel=\"linear\"),n_jobs=multiprocessing.cpu_count())\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Finished! Time: {:.2f}s\".format(end_time - begin_time))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = np.sum(y_pred == y_test) / len(y_pred)\n",
    "    print(\"Support Vector Machine (SVM) acc: {:.2f}%\".format(acc * 100))\n",
    "    print(classification_report(y_test,y_pred,target_names=labels))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVMClassifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_idx, test_idx in sss.split(X, Y):\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "#     random_forest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svm,open(\"svm.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
