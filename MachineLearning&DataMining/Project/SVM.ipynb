{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "n_users = len(data)\n",
    "posts = data[\"posts\"]\n",
    "labels = data[\"type\"].unique()\n",
    "n_class = len(labels)\n",
    "type2num = {label: i for i,label in enumerate(labels)}\n",
    "Y = np.array(list(map(lambda s: type2num[s], data[\"type\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution():\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    type_val = data[\"type\"].value_counts()\n",
    "    labels = type_val.keys()\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, type_val.values)\n",
    "    ax.set_ylabel(\"# of people\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels,rotation='45')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posts(path=\"\"):\n",
    "    filename = os.path.join(path,\"posts.pkl\")\n",
    "    user_posts = []\n",
    "    if not os.path.isfile(filename):\n",
    "        stopwords = pd.read_csv(\"stopwords.csv\").to_numpy().reshape(-1)\n",
    "        stopwords = np.array(list(map(lambda s: s.replace(\"'\",\"\"),stopwords)))\n",
    "        for uid in range(n_users):\n",
    "            # add empty space first (better used for regex parsing)\n",
    "            new_post = posts[uid].replace(\"|||\",\" ||| \")\n",
    "            new_post = new_post.replace(\",\",\", \")\n",
    "            # remove url links\n",
    "            new_post = re.sub(\"(http|https):\\/\\/.*?( |'|\\\")\",\"\",new_post)\n",
    "            # avoid words in two sentences merged together after removing spaces\n",
    "            new_post = new_post.replace(\".\",\". \")\n",
    "            # remove useless numbers and punctuations\n",
    "            new_post = re.sub(r\"[0-9]+\", \"\", new_post)\n",
    "            new_post = new_post.translate(str.maketrans('', '', string.punctuation))\n",
    "            # remove redundant empty spaces\n",
    "            new_post = re.sub(\" +\",\" \",new_post).strip()\n",
    "            # make all characters lower\n",
    "            new_post = new_post.lower()\n",
    "            new_post\n",
    "            temp = []\n",
    "            # remove stopping words\n",
    "            for word in new_post.split():\n",
    "                if len(word) != 1 and word not in stopwords:\n",
    "                    temp.append(word)\n",
    "            user_posts.append(temp)\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating word list\")\n",
    "        pickle.dump(user_posts,open(filename,\"wb\"))\n",
    "    else:\n",
    "        user_posts = pickle.load(open(filename,\"rb\"))\n",
    "        print(\"Loaded user posts\")\n",
    "    return user_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(user_posts,path=\"\"):\n",
    "    filename = os.path.join(path,\"word_dict.npz\")\n",
    "    if not os.path.isfile(filename):\n",
    "        word_lst = []\n",
    "        for post in user_posts:\n",
    "            word_lst += post\n",
    "\n",
    "        # make dictionary (used for bag of words, BOW)\n",
    "        word_counts = Counter(word_lst)\n",
    "        word_counts[\"<UNK>\"] = max(word_counts.values()) + 1\n",
    "        # remove words that don't occur too frequently\n",
    "        print(\"# of words before:\",len(word_counts))\n",
    "        for word in list(word_counts): # avoid changing size\n",
    "            if word_counts[word] < 6:\n",
    "                del word_counts[word]\n",
    "        print(\"# of words after:\",len(word_counts))\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        int_to_word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        word_to_int = {w: k for k, w in int_to_word.items()}\n",
    "        np.savez(filename,int2word=int_to_word,word2int=word_to_int)\n",
    "    else:\n",
    "        infile = np.load(filename,allow_pickle=True)\n",
    "        int_to_word = infile[\"int2word\"].item()\n",
    "        word_to_int = infile[\"word2int\"].item()\n",
    "        print(\"Loaded {}\".format(filename))\n",
    "    n_words = len(int_to_word)\n",
    "    print('Vocabulary size:', n_words)\n",
    "    return word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(user_posts,word_to_int):\n",
    "    filename = \"bow.npy\"\n",
    "    if not os.path.isfile(filename):\n",
    "        n_users = len(user_posts)\n",
    "        n_words = len(word_to_int)\n",
    "        feature = np.zeros((n_users,n_words))\n",
    "        print(feature.shape)\n",
    "        for uid, post in enumerate(user_posts):\n",
    "            count = Counter(post)\n",
    "            for key in count:\n",
    "                feature[uid][word_to_int.get(key,0)] = count[key]\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating BoW model\")\n",
    "        np.save(filename,feature)\n",
    "        print(\"Saved {}\".format(filename))\n",
    "    else:\n",
    "        feature = np.load(filename)\n",
    "        print(\"Loaded BoW model\")\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user posts\n",
      "Loaded word_dict.npz\n",
      "Vocabulary size: 27129\n",
      "(8675, 27129)\n",
      "Done 0/8675 = 0.0%\n",
      "Done 347/8675 = 4.0%\n",
      "Done 694/8675 = 8.0%\n",
      "Done 1041/8675 = 12.0%\n",
      "Done 1388/8675 = 16.0%\n",
      "Done 1735/8675 = 20.0%\n",
      "Done 2082/8675 = 24.0%\n",
      "Done 2429/8675 = 28.0%\n",
      "Done 2776/8675 = 32.0%\n",
      "Done 3123/8675 = 36.0%\n",
      "Done 3470/8675 = 40.0%\n",
      "Done 3817/8675 = 44.0%\n",
      "Done 4164/8675 = 48.0%\n",
      "Done 4511/8675 = 52.0%\n",
      "Done 4858/8675 = 56.0%\n",
      "Done 5205/8675 = 60.0%\n",
      "Done 5552/8675 = 64.0%\n",
      "Done 5899/8675 = 68.0%\n",
      "Done 6246/8675 = 72.0%\n",
      "Done 6593/8675 = 76.0%\n",
      "Done 6940/8675 = 80.0%\n",
      "Done 7287/8675 = 84.0%\n",
      "Done 7634/8675 = 88.0%\n",
      "Done 7981/8675 = 92.0%\n",
      "Done 8328/8675 = 96.0%\n",
      "Finished generating BoW model\n",
      "Saved bow.npy\n"
     ]
    }
   ],
   "source": [
    "user_posts = generate_posts()\n",
    "word2int, int2word = generate_dict(user_posts)\n",
    "X = generate_bow(user_posts,word2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "unique_X_val = []\n",
    "for attr in range(X.shape[1]): # disadvantage\n",
    "    unique_X_val.append(np.unique(X[:,attr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Support Vector Machine (SVM) acc: 65.39%\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(verbose=True)\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.score(X_test, y_test)\n",
    "print(\"Support Vector Machine (SVM) acc: {:.2f}%\".format(predict * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf,open(\"svm.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4ed6a7ad9c88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_test' is not defined"
     ]
    }
   ],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_pred,Y_test,target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        INFJ       0.69      0.65      0.67       438\n",
      "        ENTP       0.57      0.68      0.62       179\n",
      "        INTP       0.82      0.59      0.69       561\n",
      "        INTJ       0.69      0.67      0.68       326\n",
      "        ENTJ       0.28      0.80      0.42        25\n",
      "        ENFJ       0.22      0.61      0.32        18\n",
      "        INFP       0.84      0.64      0.73       720\n",
      "        ENFP       0.60      0.79      0.68       173\n",
      "        ISFP       0.22      0.68      0.34        28\n",
      "        ISTP       0.54      0.75      0.63        76\n",
      "        ISFJ       0.42      0.85      0.56        26\n",
      "        ISTJ       0.28      0.52      0.36        27\n",
      "        ESTP       0.21      0.83      0.33         6\n",
      "        ESFP       0.00      0.00      0.00         0\n",
      "        ESTJ       0.00      0.00      0.00         0\n",
      "        ESFJ       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.65      2603\n",
      "   macro avg       0.40      0.57      0.44      2603\n",
      "weighted avg       0.72      0.65      0.67      2603\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_pred,y_test,target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
