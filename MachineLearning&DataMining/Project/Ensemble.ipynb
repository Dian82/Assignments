{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "n_users = len(data)\n",
    "posts = data[\"posts\"]\n",
    "labels = data[\"type\"].unique()\n",
    "n_class = len(labels)\n",
    "type2num = {label: i for i,label in enumerate(labels)}\n",
    "Y = np.array(list(map(lambda s: type2num[s], data[\"type\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution():\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    type_val = data[\"type\"].value_counts()\n",
    "    labels = type_val.keys()\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, type_val.values)\n",
    "    ax.set_ylabel(\"# of people\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels,rotation='45')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posts(path=\"\"):\n",
    "    filename = os.path.join(path,\"posts.pkl\")\n",
    "    user_posts = []\n",
    "    if not os.path.isfile(filename):\n",
    "        stopwords = pd.read_csv(\"stopwords.csv\").to_numpy().reshape(-1)\n",
    "        stopwords = np.array(list(map(lambda s: s.replace(\"'\",\"\"),stopwords)))\n",
    "        for uid in range(n_users):\n",
    "            # add empty space first (better used for regex parsing)\n",
    "            new_post = posts[uid].replace(\"|||\",\" ||| \")\n",
    "            new_post = new_post.replace(\",\",\", \")\n",
    "            # remove url links\n",
    "            new_post = re.sub(\"(http|https):\\/\\/.*?( |'|\\\")\",\"\",new_post)\n",
    "            # avoid words in two sentences merged together after removing spaces\n",
    "            new_post = new_post.replace(\".\",\". \")\n",
    "            # remove useless numbers and punctuations\n",
    "            new_post = re.sub(r\"[0-9]+\", \"\", new_post)\n",
    "            new_post = new_post.translate(str.maketrans('', '', string.punctuation))\n",
    "            # remove redundant empty spaces\n",
    "            new_post = re.sub(\" +\",\" \",new_post).strip()\n",
    "            # make all characters lower\n",
    "            new_post = new_post.lower()\n",
    "            temp = []\n",
    "            # remove stopping words\n",
    "            for word in new_post.split():\n",
    "                if len(word) != 1 and word not in stopwords:\n",
    "                    temp.append(word)\n",
    "            user_posts.append(temp)\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating word list\")\n",
    "        pickle.dump(user_posts,open(filename,\"wb\"))\n",
    "    else:\n",
    "        user_posts = pickle.load(open(filename,\"rb\"))\n",
    "        print(\"Loaded user posts\")\n",
    "    return user_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(user_posts,path=\"\"):\n",
    "    filename = os.path.join(path,\"word_dict.npz\")\n",
    "    if not os.path.isfile(filename):\n",
    "        word_lst = []\n",
    "        for post in user_posts:\n",
    "            word_lst += post\n",
    "\n",
    "        # make dictionary (used for bag of words, BOW)\n",
    "        word_counts = Counter(word_lst)\n",
    "        word_counts[\"<UNK>\"] = max(word_counts.values()) + 1\n",
    "        # remove words that don't occur too frequently\n",
    "        print(\"# of words before:\",len(word_counts))\n",
    "        for word in list(word_counts): # avoid changing size\n",
    "            if word_counts[word] < 6:\n",
    "                del word_counts[word]\n",
    "        print(\"# of words after:\",len(word_counts))\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        int_to_word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        word_to_int = {w: k for k, w in int_to_word.items()}\n",
    "        np.savez(filename,int2word=int_to_word,word2int=word_to_int)\n",
    "    else:\n",
    "        infile = np.load(filename,allow_pickle=True)\n",
    "        int_to_word = infile[\"int2word\"].item()\n",
    "        word_to_int = infile[\"word2int\"].item()\n",
    "        print(\"Loaded {}\".format(filename))\n",
    "    n_words = len(int_to_word)\n",
    "    print('Vocabulary size:', n_words)\n",
    "    return word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(user_posts,word_to_int):\n",
    "    filename = \"bow.npy\"\n",
    "    if not os.path.isfile(filename):\n",
    "        n_users = len(user_posts)\n",
    "        n_words = len(word_to_int)\n",
    "        feature = np.zeros((n_users,n_words))\n",
    "        print(feature.shape)\n",
    "        for uid, post in enumerate(user_posts):\n",
    "            count = Counter(post)\n",
    "            for key in count:\n",
    "                feature[uid][word_to_int.get(key,0)] = count[key]\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating BoW model\")\n",
    "        np.save(filename,feature)\n",
    "        print(\"Saved {}\".format(filename))\n",
    "    else:\n",
    "        feature = np.load(filename)\n",
    "        print(\"Loaded BoW model\")\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_posts = generate_posts()\n",
    "word2int, int2word = generate_dict(user_posts)\n",
    "X = generate_bow(user_posts,word2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TREES = 3\n",
    "MAX_DEPTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
    "unique_X_val = []\n",
    "for attr in range(X.shape[1]): # disadvantage\n",
    "    unique_X_val.append(np.unique(X[:,attr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_rf(n_trees=5): # used for comparison\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    print(\"Begin training...\")\n",
    "    clf = RandomForestClassifier(n_estimators=n_trees,verbose=2,n_jobs=4) # use all processors\n",
    "    clf.fit(X_train, y_train)\n",
    "    predict = clf.score(X_test, y_test)\n",
    "    print(\"Random forest acc: {:.2f}%\".format(predict*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_rf(N_TREES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(arr):\n",
    "    \"\"\"\n",
    "    Input: 1D numpy array\n",
    "    Output: mode of this array\n",
    "    \"\"\"\n",
    "    counts = np.bincount(arr)\n",
    "    return np.argmax(counts)\n",
    "\n",
    "def value_counts(arr,normalize=False):\n",
    "    \"\"\"\n",
    "    Input: arr (numpy array)\n",
    "    Output: Counts of unique elements in arr\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    counts = counts if not normalize else (counts / counts.sum())\n",
    "    return unique, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"\n",
    "    Input: p (prob) is a numpy array\n",
    "    Output: Ent = - \\sum_i p_i \\log p_i\n",
    "    \"\"\"\n",
    "    if p.ndim == 1:\n",
    "        new_p = p[p != 0]\n",
    "        return -np.sum(new_p * np.log2(new_p))\n",
    "    else: # high dimensional input (should be guaranteed no zeros exist)\n",
    "        return -np.sum(p * np.log2(p),axis=1)\n",
    "    \n",
    "def information_gain(D,a,L):\n",
    "    \"\"\"\n",
    "    Input: D (dataset), a (attribute), L (labels)\n",
    "        attributes are features, and all discrete\n",
    "    Output: Gain = Ent(D) - \\sum_v |D_v|/|D| Ent(D_v)\n",
    "        where v\\in V is the unique values of a\n",
    "    \"\"\"\n",
    "    _, pk = value_counts(L,normalize=True)\n",
    "    V, prop_Dv = value_counts(D[:,a],normalize=True) # proportion = |D_v|/|D|\n",
    "    # prob (|V|,|class|)\n",
    "    sumup = 0\n",
    "    for av, prop in zip(V,prop_Dv):\n",
    "        _, prob_Dv = value_counts(L[D[:,a] == av],normalize=True)\n",
    "        sumup += prop * entropy(prob_Dv)\n",
    "    return (entropy(pk) - sumup, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node in decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.branch = {}\n",
    "\n",
    "    def setLeaf(self,catagory,cnt=1):\n",
    "        \"\"\"\n",
    "        Set this node as a leaf with \"catagory\"\n",
    "        \"\"\"\n",
    "        self.label = \"Leaf\"\n",
    "        self.catagory = catagory\n",
    "\n",
    "    def setBranch(self,attr,value,node):\n",
    "        \"\"\"\n",
    "        Set this node as a parent node with \"attr\"\n",
    "        Add a child \"node\" with \"value\"\n",
    "        \"\"\"\n",
    "        self.label = \"Branch\"\n",
    "        self.attr = attr\n",
    "        self.branch[value] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3:\n",
    "    \"\"\"\n",
    "    ID3 Decision Tree with pre-partition\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,train_set=None,test_set=None,\n",
    "                 attributes=None,unique_val=None,\n",
    "                 prune=False,tid=0,max_depth=5,random=False):\n",
    "        \"\"\"\n",
    "        Add datasets into the class\n",
    "        \"\"\"\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.attributes = attributes\n",
    "        self.unique_val = unique_val\n",
    "        self.prune = prune\n",
    "        self.tid = tid\n",
    "        self.max_depth = max_depth\n",
    "        self.random = random\n",
    "        print(\"Loaded dataset. # features: {}\".format(len(self.attributes)))\n",
    "\n",
    "    def TreeGenerate(self,dataset,labels,attributes,depth,\n",
    "                     cnt_leaves=0,root=None,prev_best=None):\n",
    "        \"\"\"\n",
    "        Core algorithm of ID3 that generates the whole tree\n",
    "        \"\"\"\n",
    "        catagory = np.unique(labels)\n",
    "        node = Node() if root == None else root # better used for validation indexing\n",
    "        cnt_leaves += 1\n",
    "\n",
    "        # 1) All samples in \"dataset\" belongs to the same catagory\n",
    "        if len(catagory) == 1:\n",
    "            node.setLeaf(catagory[0],cnt_leaves)\n",
    "            return node\n",
    "\n",
    "        # 2) \"attributes\" is empty, or the values of \"dataset\" on \"attributes\" are the same\n",
    "        if len(attributes) == 0 or \\\n",
    "          np.array([len(np.unique(dataset[:,a])) == 1 for a in attributes]).all() == True:\n",
    "            node.setLeaf(mode(labels),cnt_leaves)\n",
    "            return node\n",
    "\n",
    "        \"\"\"The general case\"\"\"\n",
    "        # find the attribute with greatest information gain\n",
    "        max_gain = (-0x3f3f3f3f,None)\n",
    "        if self.random:\n",
    "            k = int(np.log2(len(attributes))) + 1 # random set!!!\n",
    "        else:\n",
    "            k = len(attributes) // 2\n",
    "        random_attributes = attributes[np.random.choice(len(attributes),k,replace=False)]\n",
    "        for a in random_attributes:\n",
    "            gain = information_gain(dataset,a,labels)\n",
    "            if gain[0] > max_gain[0]:\n",
    "                a_best, max_gain = a, gain\n",
    "        unique = self.unique_val[a_best] # be careful, not dataset!\n",
    "        num_leaves = len(unique)\n",
    "        print(\"Tree {} Depth {}: {} - {}\\t # leaves: {}\".format(self.tid, depth, prev_best, max_gain, num_leaves))\n",
    "        \n",
    "        if self.prune and self.test_set != None:\n",
    "            # without partition\n",
    "            node.setLeaf(mode(labels),cnt_leaves)\n",
    "            acc_without_partition = self.test(val=True)\n",
    "\n",
    "            # with partition (make branches)\n",
    "            for av in unique:\n",
    "                Dv = dataset[dataset[:,a_best] == av,:]\n",
    "                labels_v = labels[dataset[:,a_best] == av]\n",
    "                cnt_leaves += 1\n",
    "                leafnode = Node()\n",
    "                if len(Dv) == 0:\n",
    "                    leafnode.setLeaf(mode(labels),cnt_leaves)\n",
    "                else:\n",
    "                    leafnode.setLeaf(mode(labels_v),cnt_leaves)\n",
    "                node.setBranch(a_best,av,leafnode)\n",
    "            print(\"Depth {} ({}): \".format(depth,cnt_leaves))\n",
    "            acc_with_partition = self.test(val=True,print_flag=True)\n",
    "\n",
    "            # pre-pruning (to make sure it has generated sufficient nodes, depth is set here)\n",
    "            if depth > 5 and acc_without_partition >= acc_with_partition:\n",
    "                cnt_leaves -= num_leaves\n",
    "                print(\"Prune at {}: {} (without) >= {} (with)\".format(a_best,acc_without_partition,acc_with_partition))\n",
    "                node.setLeaf(mode(labels))\n",
    "                return node\n",
    "            elif depth > 5:\n",
    "                print(a_best,acc_without_partition,acc_with_partition)\n",
    "\n",
    "            # true partition (branching makes more gains)\n",
    "            for av in unique:\n",
    "                Dv = dataset[dataset[:,a_best] == av,:]\n",
    "                labels_v = labels[dataset[:,a_best] == av]\n",
    "                # 3) \"Dv\" is empty, which can not be partitioned\n",
    "                if len(Dv) != 0:\n",
    "                    node.setBranch(a_best,av,self.TreeGenerate(Dv,labels_v,\n",
    "                                                               attributes[attributes != a_best],\n",
    "                                                               depth+1,cnt_leaves))\n",
    "        else:\n",
    "            if depth > self.max_depth:\n",
    "                node.setLeaf(mode(labels),cnt_leaves)\n",
    "                return node\n",
    "            for av in unique:\n",
    "                Dv = dataset[dataset[:,a_best] == av,:]\n",
    "                labels_v = labels[dataset[:,a_best] == av]\n",
    "                cnt_leaves += 1\n",
    "                leafnode = Node()\n",
    "                if len(Dv) == 0:\n",
    "                    leafnode.setLeaf(mode(labels),cnt_leaves)\n",
    "                    node.setBranch(a_best,av,leafnode)\n",
    "                else:\n",
    "                    node.setBranch(a_best,av,self.TreeGenerate(Dv,labels_v,\n",
    "                                                               attributes[attributes != a_best],\n",
    "                                                               depth+1,cnt_leaves,prev_best=a_best))\n",
    "        return node\n",
    "\n",
    "    def train(self,train_set=None):\n",
    "        \"\"\"\n",
    "        Train the decision tree\n",
    "        \"\"\"\n",
    "        if train_set != None:\n",
    "            self.train_set = train_set\n",
    "        start_time = time.time()\n",
    "        self.root = Node()\n",
    "        self.root = self.TreeGenerate(self.train_set[0],self.train_set[1],\n",
    "                                      self.attributes,\n",
    "                                      depth=1,root=self.root,\n",
    "                                      cnt_leaves=0)\n",
    "        print(\"Tree {} Time: {:.2f}s\".format(self.tid, time.time() - start_time))\n",
    "\n",
    "    def predict(self,feat):\n",
    "        p = self.root\n",
    "        while p.label != \"Leaf\": # get to the leaf node\n",
    "            p = p.branch[feat[p.attr]]\n",
    "        return p.catagory\n",
    "\n",
    "    def test(self,val=False,print_flag=False):\n",
    "        \"\"\"\n",
    "        Testing/Validation and calculate the accuracy\n",
    "        \"\"\"\n",
    "        acc = 0\n",
    "        test_X = self.test_set[0]\n",
    "        test_Y = self.test_set[1]\n",
    "        for i, row in enumerate(test_X):\n",
    "            pred = self.predict(row)\n",
    "            if pred == test_Y[i]:\n",
    "                acc += 1\n",
    "        acc /= len(test_X)\n",
    "        if print_flag:\n",
    "            print(\"Accurary: {:.2f}%\".format(acc * 100))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watermelon_test(): # used for checking the correctness\n",
    "    watermelon = pd.read_csv(\"watermelon.csv\",header=None).to_numpy()\n",
    "    X_train = watermelon[:,:6].astype(int)\n",
    "    y_train = watermelon[:,-1].astype(int).reshape(-1)\n",
    "    unique_X_val = []\n",
    "    for attr in range(X_train.shape[1]): # disadvantage\n",
    "        unique_X_val.append(np.unique(X_train[:,attr]))\n",
    "    dt = ID3(train_set=(X_train,y_train),\n",
    "             test_set=(X_train,y_train),\n",
    "             attributes=np.arange(0,6),\n",
    "             unique_val=unique_X_val)\n",
    "    dt.train()\n",
    "\n",
    "# watermelon_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self,n_trees=3,max_depth=5):\n",
    "        self.n_trees = n_trees\n",
    "        self.dt = [0] * self.n_trees\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self,train_X,train_Y):\n",
    "        start_time = time.time()\n",
    "        tasks = []\n",
    "        for i in range(self.n_trees):\n",
    "            def train_dt(idx):\n",
    "                samp_X, samp_Y = resample(train_X,train_Y,n_samples=len(train_X))\n",
    "                print(\"Building tree {}\".format(idx))\n",
    "                dt = ID3(train_set=(samp_X.astype(int), samp_Y.astype(int)),\n",
    "                         attributes=np.array(list(range(train_X.shape[1]))),\n",
    "                         unique_val=unique_X_val,\n",
    "                         prune=False,\n",
    "                         max_depth=self.max_depth,\n",
    "                         tid=idx)\n",
    "                dt.train()\n",
    "                pickle.dump(dt,open(\"tree-{}.pkl\".format(idx),\"wb\"))\n",
    "            p = multiprocessing.Process(target=train_dt,args=(i,))\n",
    "            tasks.append(p)\n",
    "        [x.start() for x in tasks]\n",
    "        [x.join() for x in tasks]\n",
    "        for i in range(self.n_trees):\n",
    "            self.dt[i] = pickle.load(open(\"tree-{}.pkl\".format(i),\"rb\"))\n",
    "        print(\"Random forest time: {}s\".format(time.time() - start_time))\n",
    "\n",
    "    def predict(self,test_X):\n",
    "        print(\"Generating prediction...\")\n",
    "        pred = np.zeros((len(test_X),))\n",
    "        for i, row in enumerate(test_X):\n",
    "            counts = np.zeros((len(labels),))\n",
    "            for j in range(self.n_trees):\n",
    "                pred_one = self.dt[j].predict(row)\n",
    "                counts[pred_one] += 1\n",
    "            pred[i] = np.argmax(counts)\n",
    "        return pred\n",
    "\n",
    "    def score(self,test_X,test_Y):\n",
    "        pred_Y = self.predict(test_X)\n",
    "        acc = np.sum(pred_Y == test_Y) / len(test_Y)\n",
    "        print(classification_report(pred_Y,test_Y,target_names=labels))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForest(n_trees=N_TREES,max_depth=MAX_DEPTH)\n",
    "rf.fit(X_train,y_train)\n",
    "acc = rf.score(X_test,y_test)\n",
    "print(\"Acc: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
