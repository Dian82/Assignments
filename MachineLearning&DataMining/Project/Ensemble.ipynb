{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "n_users = len(data)\n",
    "posts = data[\"posts\"]\n",
    "labels = data[\"type\"].unique()\n",
    "type2num = {label: i for i,label in enumerate(labels)}\n",
    "Y = np.array(list(map(lambda s: type2num[s], data[\"type\"].to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution():\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    type_val = data[\"type\"].value_counts()\n",
    "    labels = type_val.keys()\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, type_val.values)\n",
    "    ax.set_ylabel(\"# of people\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels,rotation='45')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_posts(path=\"\"):\n",
    "    filename = os.path.join(path,\"posts.pkl\")\n",
    "    user_posts = []\n",
    "    if not os.path.isfile(filename):\n",
    "        stopwords = pd.read_csv(\"stopwords.csv\").to_numpy().reshape(-1)\n",
    "        stopwords = np.array(list(map(lambda s: s.replace(\"'\",\"\"),stopwords)))\n",
    "        for uid in range(n_users):\n",
    "            # add empty space first (better used for regex parsing)\n",
    "            new_post = posts[uid].replace(\"|||\",\" ||| \")\n",
    "            new_post = new_post.replace(\",\",\", \")\n",
    "            # remove url links\n",
    "            new_post = re.sub(\"(http|https):\\/\\/.*?( |'|\\\")\",\"\",new_post)\n",
    "            # avoid words in two sentences merged together after removing spaces\n",
    "            new_post = new_post.replace(\".\",\". \")\n",
    "            # remove useless numbers and punctuations\n",
    "            new_post = re.sub(r\"[0-9]+\", \"\", new_post)\n",
    "            new_post = new_post.translate(str.maketrans('', '', string.punctuation))\n",
    "            # remove redundant empty spaces\n",
    "            new_post = re.sub(\" +\",\" \",new_post).strip()\n",
    "            # make all characters lower\n",
    "            new_post = new_post.lower()\n",
    "            temp = []\n",
    "            for word in new_post.split():\n",
    "                if len(word) != 1 and word not in stopwords:\n",
    "                    temp.append(word)\n",
    "            user_posts.append(temp)\n",
    "            if uid * 100 % n_users == 0:\n",
    "                print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "        print(\"Finished generating word list\")\n",
    "        pickle.dump(user_posts,open(filename,\"wb\"))\n",
    "    else:\n",
    "        user_posts = pickle.load(open(filename,\"rb\"))\n",
    "        print(\"Loaded user posts\")\n",
    "    return user_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(user_posts,path=\"\"):\n",
    "    filename = os.path.join(path,\"word_dict.npz\")\n",
    "    if not os.path.isfile(filename):\n",
    "        word_lst = []\n",
    "        for post in user_posts:\n",
    "            word_lst += post\n",
    "\n",
    "        # make dictionary (used for bag of words, BOW)\n",
    "        word_counts = Counter(word_lst)\n",
    "        word_counts[\"<UNK>\"] = max(word_counts.values()) + 1\n",
    "        # remove words that donâ€™t occur too frequently\n",
    "        print(\"# of words before:\",len(word_counts))\n",
    "        for word in list(word_counts): # avoid changing size\n",
    "            if word_counts[word] < 6:\n",
    "                del word_counts[word]\n",
    "        print(\"# of words after:\",len(word_counts))\n",
    "        # sort based on counts, but only remain the word strings\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "        # make embedding based on the occurance frequency of the words\n",
    "        int_to_word = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        word_to_int = {w: k for k, w in int_to_word.items()}\n",
    "        np.savez(filename,int2word=int_to_word,word2int=word_to_int)\n",
    "    else:\n",
    "        infile = np.load(filename,allow_pickle=True)\n",
    "        int_to_word = infile[\"int2word\"].item()\n",
    "        word_to_int = infile[\"word2int\"].item()\n",
    "        print(\"Loaded {}\".format(filename))\n",
    "    n_words = len(int_to_word)\n",
    "    print('Vocabulary size:', n_words)\n",
    "    return word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(user_posts,word_to_int):\n",
    "    n_users = len(user_posts)\n",
    "    n_words = len(word_to_int)\n",
    "    feature = np.zeros((n_users,n_words))\n",
    "    print(feature.shape)\n",
    "    for uid, post in enumerate(user_posts):\n",
    "        count = Counter(post)\n",
    "        for key in count:\n",
    "            feature[uid][word_to_int.get(key,0)] = count[key]\n",
    "        if uid * 100 % n_users == 0:\n",
    "            print(\"Done {}/{} = {}%\".format(uid,n_users,uid*100/n_users))\n",
    "    print(\"Finished generating BoW model\")\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_posts = generate_posts()\n",
    "word2int, int2word = generate_dict(user_posts)\n",
    "X = generate_bow(user_posts,word2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=5,verbose=2,n_jobs=4) # use all processors\n",
    "clf.fit(X_train, y_train)\n",
    "predict = clf.score(X_test, y_test)\n",
    "print(\"Random forest acc: {:.2f}%\".format(predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
