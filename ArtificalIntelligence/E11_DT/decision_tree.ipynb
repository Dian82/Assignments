{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level = logging.INFO)\n",
    "handler = logging.FileHandler(\"DT-prepruning2.log\")\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "attr_dict = {\"age\":0, \"workclass\":1, \"fnlwgt\":0, \"education\":1, \"education-num\":0, \"marital-status\":1, \"occupation\":1, \"relationship\":1, \"race\":1, \"sex\":1, \"capital-gain\":0, \"capital-loss\":0, \"hours-per-week\":0, \"native-country\":1, \"salary\":0} # 0: continuous, 1: discrete\n",
    "\n",
    "train_data = pd.read_csv(\"adult.data\",names=attr_dict.keys(),index_col=False)\n",
    "test_data = pd.read_csv(\"adult.test\",names=attr_dict.keys(),index_col=False,header=0)\n",
    "\n",
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    Select some useful attributes\n",
    "    \"\"\"\n",
    "    # attributes = [\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\",\"salary\"]\n",
    "    attributes = list(attr_dict.keys())\n",
    "    attributes.remove(\"fnlwgt\")\n",
    "    return data[attributes]\n",
    "\n",
    "def fill_data(data):\n",
    "    \"\"\"\n",
    "    Fill in missing data (?)\n",
    "    \"\"\"\n",
    "    for a in attr_dict:\n",
    "        if attr_dict[a]: # discrete\n",
    "            data.loc[data[a] == \" ?\",a] = data[a].value_counts().argmax() # view or copy? Use loc!\n",
    "        else: # continuous\n",
    "            pass\n",
    "\n",
    "# Data cleaning\n",
    "train_data = preprocessing(train_data)\n",
    "test_data = preprocessing(test_data)\n",
    "fill_data(train_data)\n",
    "fill_data(test_data)\n",
    "\n",
    "# Generate validation set (for pre-pruning)\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "cut = int(0.9 * len(train_data))\n",
    "# cut = int(len(train_data))\n",
    "train_data, validation_data = train_data[:cut], train_data[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"\n",
    "    Input: p is a numpy array\n",
    "    Output: Ent = - \\sum_i p_i \\log p_i\n",
    "    \"\"\"\n",
    "    if p.ndim == 1:\n",
    "        new_p = p[p != 0]\n",
    "        return -np.sum(new_p * np.log2(new_p))\n",
    "    else: # high dimensional input (should be guaranteed no zeros exist) \n",
    "        # new_p = p[(p[:,0] != 0) & (p[:,1] != 0)]\n",
    "        return -np.sum(p * np.log2(p),axis=1)\n",
    "\n",
    "def information_gain(D,a,discrete_flag=False):\n",
    "    \"\"\"\n",
    "    Input: D (dataset), a (attribute), discrete_flag (whether a is discrete)\n",
    "    Output: Gain = Ent(D) - \\sum_v |D_v|/|D| Ent(D_v)\n",
    "    \"\"\"\n",
    "    pk = D[\"salary\"].value_counts(normalize=True).values\n",
    "    if discrete_flag: # discrete\n",
    "        prop_Dv = D[a].value_counts(normalize=True).values # proportion = |D_v|/|D|\n",
    "        prob_Dv = np.array([D.loc[D[a] == av][\"salary\"].value_counts(normalize=True).get(\" >50K\",0) for av in D[a].unique()])\n",
    "        # delete all the zero terms\n",
    "        pp_stack = np.column_stack((prop_Dv,prob_Dv))\n",
    "        pp_stack = pp_stack[(pp_stack[:,1] != 0) & (pp_stack[:,1] != 1)]\n",
    "        prop_Dv = pp_stack[:,0]\n",
    "        prob_Dv = pp_stack[:,1]\n",
    "        # since it only has two categories, the probability of the other can be easily calculated\n",
    "        prob_Dv_neg = 1 - prob_Dv\n",
    "        return (entropy(pk) - np.sum(prop_Dv * entropy(np.column_stack((prob_Dv,prob_Dv_neg)))), a)\n",
    "    else: # continuous\n",
    "        # firstly sort all the existed values\n",
    "        a_sort = sorted(D[a].unique())\n",
    "        # calculate the partition point (a_i + a_{i+1}) / 2\n",
    "        Ta = [(a_sort[i] + a_sort[i+1]) / 2 for i in range(len(a_sort)-1)]\n",
    "        # find the one with minimum weighted entropy sum (\\sum |D_t^\\lambda|/|D| Ent(D_t^\\lambda))\n",
    "        min_ent, min_t = 0x3f3f3f3f, a_sort[0]\n",
    "        for t in Ta: # bi-partition\n",
    "            prop_Dv = len(D[D[a] < t]) / len(D)\n",
    "            prop_Dv = np.array([prop_Dv,1-prop_Dv]) # proportion\n",
    "            prob_Dv_smaller = D[D[a] < t][\"salary\"].value_counts(normalize=True).get(\" >50K\",0)\n",
    "            prob_Dv_bigger = D[D[a] >= t][\"salary\"].value_counts(normalize=True).get(\" >50K\",0)\n",
    "            prob_Dv = np.array([[prob_Dv_smaller,1-prob_Dv_smaller],[prob_Dv_bigger,1-prob_Dv_bigger]])\n",
    "            # remove all zero terms\n",
    "            prob_Dv = prob_Dv[(prob_Dv[:,0] != 0) & (prob_Dv[:,1] != 0)]\n",
    "            # calculate weighted entropy sum\n",
    "            if len(prob_Dv) == 0:\n",
    "                sumup = 0\n",
    "            else:\n",
    "                sumup = np.sum(prop_Dv * entropy(prob_Dv))\n",
    "            if min_ent > sumup:\n",
    "                min_ent = sumup\n",
    "                min_t = t\n",
    "        # return Gain and the partition point\n",
    "        return (entropy(pk) - min_ent, min_t)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node in decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.branch = {}\n",
    "\n",
    "    def setLeaf(self,catagory,cnt=1):\n",
    "        \"\"\"\n",
    "        Set this node as a leaf with `catagory`\n",
    "        \"\"\"\n",
    "        logger.info(\"{} - Create leaf: {}\".format(cnt,catagory))\n",
    "        if cnt % 10 == 0:\n",
    "            print(\"{} - Create leaf: {}\".format(cnt,catagory),flush=True)\n",
    "        self.label = \"Leaf\"\n",
    "        self.catagory = catagory\n",
    "        \n",
    "    def setBranch(self,attr,value,node,branch_value=None):\n",
    "        \"\"\"\n",
    "        Set this node as a parent node with `attr`\n",
    "        Add a child `node` with `value`\n",
    "        If the attribute is continuous, `branch_value` is also given\n",
    "        \"\"\"\n",
    "        logger.info(\"Create branch: {} ({})\".format(attr,value))\n",
    "        self.label = \"Branch\"\n",
    "        self.attr = attr\n",
    "        self.branch[value] = node\n",
    "        if branch_value != None:\n",
    "            self.branch_value = branch_value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,sys\n",
    "\n",
    "class ID3:\n",
    "    \"\"\"\n",
    "    ID3 Decision Tree with pre-partition and support continuous attributes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,train_set=None,validation_set=None,test_set=None,attr_dict=None):\n",
    "        \"\"\"\n",
    "        Add datasets into the class\n",
    "        `attr_dict` gives whether an attribute is discrete\n",
    "        \"\"\"\n",
    "        self.train_set = train_set\n",
    "        self.validation_set = validation_set\n",
    "        self.test_set = test_set\n",
    "        self.attr_dict = attr_dict\n",
    "\n",
    "    def TreeGenerate(self,dataset,attributes,depth,cnt_leaves=0,root=None):\n",
    "        \"\"\"\n",
    "        Core algorithm of ID3 that generates the whole tree\n",
    "        \"\"\"\n",
    "        catagory = dataset[\"salary\"].unique()\n",
    "        node = Node() if root == None else root # better used for validation indexing\n",
    "        cnt_leaves += 1\n",
    "\n",
    "        # 1) All samples in `dataset` belongs to the same catagory\n",
    "        if len(catagory) == 1:\n",
    "            node.setLeaf(catagory[0],cnt_leaves)\n",
    "            return node\n",
    "\n",
    "        # 2) `attributes` is empty, or the values of `dataset` on `attributes` are the same\n",
    "        if len(attributes) == 0 or np.array([len(dataset[a].unique()) == 1 for a in attributes]).all() == True:\n",
    "            node.setLeaf(dataset[\"salary\"].value_counts().argmax(),cnt_leaves)\n",
    "            return node\n",
    "\n",
    "        \"\"\"The general case\"\"\"\n",
    "        # without partition\n",
    "        node.setLeaf(dataset[\"salary\"].value_counts().argmax(),cnt_leaves)\n",
    "        acc_without_partition = self.validation()\n",
    "\n",
    "        # with partition\n",
    "        # find the attribute with greatest information gain\n",
    "        max_gain = (-0x3f3f3f3f,None)\n",
    "        for a in attributes:\n",
    "            gain = information_gain(dataset,a,self.attr_dict[a])\n",
    "            if gain[0] > max_gain[0]:\n",
    "                a_best, max_gain = a, gain\n",
    "        num_leaves = 0\n",
    "        # make branches\n",
    "        if self.attr_dict[a_best]: # discrete\n",
    "            num_leaves = len(self.train_set[a_best].unique())\n",
    "            for av in self.train_set[a_best].unique(): # be careful, not dataset!\n",
    "                Dv = dataset[dataset[a_best] == av]\n",
    "                cnt_leaves += 1\n",
    "                leafnode = Node()\n",
    "                if len(Dv) == 0:\n",
    "                    leafnode.setLeaf(dataset[\"salary\"].value_counts().argmax(),cnt_leaves)\n",
    "                else:\n",
    "                    leafnode.setLeaf(Dv[\"salary\"].value_counts().argmax(),cnt_leaves)\n",
    "                node.setBranch(a_best,av,leafnode)\n",
    "        else: # continuous\n",
    "            num_leaves = 2\n",
    "            for flag in [\"Smaller\",\"Bigger\"]:\n",
    "                Dv = dataset[dataset[a_best] < max_gain[1]] if flag == \"Smaller\" else dataset[dataset[a_best] >= max_gain[1]]\n",
    "                cnt_leaves += 1\n",
    "                leafnode = Node()\n",
    "                if len(Dv) == 0:\n",
    "                    leafnode.setLeaf(dataset[\"salary\"].value_counts().argmax(),cnt_leaves)\n",
    "                else:\n",
    "                    leafnode.setLeaf(Dv[\"salary\"].value_counts().argmax(),cnt_leaves)\n",
    "                node.setBranch(a_best,flag,leafnode,branch_value=max_gain[1])\n",
    "        acc_with_partition = self.validation()\n",
    "\n",
    "        if depth > 5 and acc_without_partition >= acc_with_partition: # pre-pruning\n",
    "            cnt_leaves -= num_leaves\n",
    "            print(\"Prune at {}: {} (without) >= {} (with)\".format(a_best,acc_without_partition,acc_with_partition))\n",
    "            logger.info(\"Prune at {}: {} (without) >= {} (with)\".format(a_best,acc_without_partition,acc_with_partition))\n",
    "            node.setLeaf(dataset[\"salary\"].value_counts().argmax())\n",
    "            return node\n",
    "        elif depth > 5:\n",
    "            print(a_best,acc_without_partition,acc_with_partition)\n",
    "\n",
    "        # true partition (branching makes more gains)\n",
    "        if self.attr_dict[a_best]: # discrete\n",
    "            for av in self.train_set[a_best].unique(): # be careful, not dataset!\n",
    "                Dv = dataset[dataset[a_best] == av]\n",
    "                # 3) `Dv` is empty, which can not be partitioned\n",
    "                if len(Dv) != 0:\n",
    "                    node.setBranch(a_best,av,self.TreeGenerate(Dv,attributes[attributes != a_best],depth+1,cnt_leaves))\n",
    "        else: # continuous\n",
    "            for flag in [\"Smaller\",\"Bigger\"]:\n",
    "                Dv = dataset[dataset[a_best] < max_gain[1]] if flag == \"Smaller\" else dataset[dataset[a_best] >= max_gain[1]]\n",
    "                if len(Dv) != 0:\n",
    "                    node.setBranch(a_best,flag,self.TreeGenerate(Dv,attributes,depth+1,cnt_leaves),branch_value=max_gain[1])\n",
    "        return node\n",
    "\n",
    "    def train(self,train_set=None):\n",
    "        \"\"\"\n",
    "        Train the decision tree\n",
    "        \"\"\"\n",
    "        if train_set != None:\n",
    "            self.train_set = train_set\n",
    "        start_time = time.time()\n",
    "        self.root = Node()\n",
    "        self.root = self.TreeGenerate(self.train_set,self.train_set.columns.values[self.train_set.columns.values != \"salary\"],depth=1,root=self.root,cnt_leaves=0)\n",
    "        logger.info(\"Time: {:.2f}s\".format(time.time()-start_time))\n",
    "        print(\"Time: {:.2f}s\".format(time.time()-start_time))\n",
    "\n",
    "    def validation(self,validation_set=None):\n",
    "        \"\"\"\n",
    "        Validate the partition on validation set\n",
    "        \"\"\"\n",
    "        if validation_set != None:\n",
    "            self.validation_set = validation_set\n",
    "        acc = 0\n",
    "        for i,row in self.validation_set.iterrows():\n",
    "            p = self.root\n",
    "            while p.label != \"Leaf\": # get to the leaf node\n",
    "                if self.attr_dict[p.attr]: # discrete\n",
    "                    p = p.branch[row[p.attr]]\n",
    "                else: # continuous\n",
    "                    p = p.branch[\"Smaller\"] if row[p.attr] < p.branch_value else p.branch[\"Bigger\"]\n",
    "            if p.catagory == row[\"salary\"]:\n",
    "                acc += 1\n",
    "        acc /= len(self.validation_set)\n",
    "        return acc\n",
    "\n",
    "    def test(self,test_set=None):\n",
    "        \"\"\"\n",
    "        Final tesing and calculate the accuracy\n",
    "        \"\"\"\n",
    "        if test_set != None:\n",
    "            self.test_set = test_set\n",
    "        acc = 0\n",
    "        for i,row in self.test_set.iterrows():\n",
    "            p = self.root\n",
    "            while p.label != \"Leaf\": # get to the leaf node\n",
    "                if self.attr_dict[p.attr]: # discrete\n",
    "                    p = p.branch[row[p.attr]]\n",
    "                else: # continuous\n",
    "                    p = p.branch[\"Smaller\"] if row[p.attr] < p.branch_value else p.branch[\"Bigger\"]\n",
    "            if p.catagory == row[\"salary\"][:-1]: # be careful of \".\"\n",
    "                acc += 1\n",
    "        acc /= len(self.test_set)\n",
    "        logger.info(\"Accurary: {:.2f}%\".format(acc * 100))\n",
    "        print(\"Accurary: {:.2f}%\".format(acc * 100))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = ID3(train_set=train_data,validation_set=validation_data,test_set=test_data,attr_dict=attr_dict)\n",
    "dt.train()\n",
    "dt.test()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}