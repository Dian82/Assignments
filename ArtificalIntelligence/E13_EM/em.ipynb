{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "dataset = pd.read_csv(\"football.txt\",sep=\",\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Calculate the Gaussian distribution\n",
    "\n",
    "    N(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{D/2}|\\Sigma|^{1/2}}\\exp\\{-\\frac{1}{2}(x-\\mu)^T}\\Sigma^{-1}(x-\\mu)\\}\n",
    "\n",
    "    Notice the input x here should be a 1-d vector\n",
    "    \"\"\"\n",
    "    if x.ndim != 1 or mu.ndim != 1:\n",
    "        raise RuntimeError(\"Dimension error!\")\n",
    "    # print(x,mu,sigma)\n",
    "    D = x.shape[0]\n",
    "    expOn = - 1 / 2 * np.matmul(np.matmul((x - mu).T,np.linalg.inv(sigma)),x - mu)\n",
    "    divBy = np.power(2 * np.pi, D / 2) * np.sqrt(np.linalg.det(sigma))\n",
    "    return np.exp(expOn) / divBy\n",
    "\n",
    "def EM(dataMat, n_components=3, maxIter=100):\n",
    "    \"\"\"\n",
    "    Expectation-Maximization (EM) algorithm\n",
    "    \n",
    "    This implementation has been extended to support different number of components\n",
    "    \"\"\"\n",
    "    n_samples, D = np.shape(dataMat) # n_samples=16 D=7\n",
    "    \n",
    "    # 1. Initialize Gaussian parameters\n",
    "    pi_k = np.ones(n_components) / n_components # mixing coefficients\n",
    "    # randomly select k(n_components) samples as the mean of each class\n",
    "    # choices = np.random.choice(n_samples,n_components)\n",
    "    # predefined mean of each class\n",
    "    choices = [1,13,11]\n",
    "    print([(i,dataset[\"Country\"][i]) for i in choices])\n",
    "    mu_k = np.array([dataMat[i,:] for i in choices]).reshape(n_components,D) # k * D\n",
    "    # mu_k = [dataMat[5, :], dataMat[21, :], dataMat[26, :]]\n",
    "    sigma_k = [np.eye(D) for x in range(n_components)] # k * D * D\n",
    "\n",
    "    gamma_k = np.zeros((n_samples, n_components)) # n * k\n",
    "    # Iterate for maxIter times\n",
    "    for i in range(maxIter):\n",
    "        \"\"\"\n",
    "        2. E step\n",
    "        \\gamma(z_{nk}) = \\frac{\\pi_k prob(x_n|\\mu_k,\\Sigma_k)}{sum_pi_mul}\n",
    "        sum_pi_mul = \\sum_{j=1}^K \\pi_j prob(x_n|\\mu_j,\\Sigma_j)\n",
    "        \"\"\"\n",
    "        for n in range(n_samples):\n",
    "            sum_pi_mul = 0 # denominator\n",
    "            for k in range(n_components):\n",
    "                gamma_k[n, k] = pi_k[k] * prob(dataMat[n, :], mu_k[k], sigma_k[k])\n",
    "                sum_pi_mul += gamma_k[n, k]\n",
    "            # normalization\n",
    "            for k in range(n_components):\n",
    "                gamma_k[n, k] /= sum_pi_mul\n",
    "        # summarize gamma_k along different samples\n",
    "        N_k = np.sum(gamma_k, axis=0)\n",
    "\n",
    "        \"\"\"\n",
    "        3. M step\n",
    "        \\mu_k^{new} = \\frac{1}{N_k}\\sum_{n=1}^N\\gamma(z_{nk})x_n\n",
    "\t    \\Sigma_k^{new} = \\frac{1}{N_k}\\sum_{n=1}^N\\gamma(z_{nk})(x_n-\\mu_k^{new})(x_n-\\mu_k^{new})^T\n",
    "\t    \\pi_k^{new} = \\frac{N_k}{N}\n",
    "\n",
    "        N_k=\\sum_{n=1}^N\\gamma(z_{nk})\n",
    "        \"\"\"\n",
    "        for k in range(n_components):\n",
    "            # Calculate \\mu_k\n",
    "            mu_k[k] = np.zeros(D,dtype=np.float64)\n",
    "            for n in range(n_samples):\n",
    "                mu_k[k] += gamma_k[n, k] * dataMat[n, :]\n",
    "            mu_k[k] /= N_k[k]\n",
    "\n",
    "            # Calculate \\Sigma_k\n",
    "            sigma_k[k] = np.zeros((D, D),dtype=np.float64)\n",
    "            for n in range(n_samples):\n",
    "                sigma_k[k] += gamma_k[n, k] * np.matmul((dataMat[n, :] - mu_k[k]).reshape(1,-1).T, (dataMat[n, :] - mu_k[k]).reshape(1,-1)) # be careful of outer product!\n",
    "            sigma_k[k] /= N_k[k]\n",
    "            \n",
    "            # Calculate new mixing coefficient\n",
    "            pi_k[k] = N_k[k] / n_samples\n",
    "\n",
    "        sigma_k += np.eye(D)\n",
    "\n",
    "    print(\"gamma: \",gamma_k)\n",
    "    print(\"mu: \",mu_k)\n",
    "    print(\"Sigma: \",sigma_k)\n",
    "\n",
    "    return gamma_k"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianCluster(dataMat, n_components, max_iter):\n",
    "    n_samples, D = np.shape(dataMat)\n",
    "    centroids = np.zeros((n_components, D))\n",
    "    gamma = EM(dataMat,n_components,max_iter)\n",
    "\n",
    "    # get the cluster result\n",
    "    clusterAssign = np.zeros((n_samples, 2))\n",
    "    for n in range(n_samples):\n",
    "        clusterAssign[n, :] = np.argmax(gamma[n, :]), np.amax(gamma[n, :])\n",
    "\n",
    "    # calculate the final results\n",
    "    for k in range(n_components):\n",
    "        pointsInCluster = dataMat[np.nonzero(clusterAssign[:, 0] == k)[0]]\n",
    "        centroids[k, :] = np.mean(pointsInCluster, axis=0)\n",
    "    return centroids, clusterAssign[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "\n",
    "def sklearn_em(data,n_components,max_iter):\n",
    "    clst = mixture.GaussianMixture(n_components=n_components,max_iter=max_iter,covariance_type=\"full\")\n",
    "    clst.fit(data)\n",
    "    predicted_labels = clst.predict(data)\n",
    "    return clst.means_, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def showCluster(dataset, k, centroids, clusterAssment):\n",
    "    numSamples, dim = dataset.shape\n",
    "\n",
    "    mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', '<r', 'pr']\n",
    "    if k > len(mark):\n",
    "        print(\"Sorry! Your k is too large!\")\n",
    "        return 1\n",
    "\n",
    "    # draw all samples\n",
    "    for i in range(numSamples):\n",
    "        markIndex = int(clusterAssment[i])\n",
    "        plt.plot(dataset[i, 0], dataset[i, 1], mark[markIndex])\n",
    "\n",
    "    mark = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', '<b', 'pb']\n",
    "    # draw the centroids\n",
    "    for i in range(k):\n",
    "        plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "data = dataset[dataset.columns.values[dataset.columns.values != \"Country\"]].to_numpy()\n",
    "# centroids, labels = sklearn_em(data,n_components,100)\n",
    "centroids, labels = gaussianCluster(data.astype(np.float64),n_components,100)\n",
    "showCluster(data, n_components, centroids, labels)\n",
    "res = {0:[],1:[],2:[]}\n",
    "for i,label in enumerate(labels):\n",
    "    res[label].append(dataset[\"Country\"][i])\n",
    "for key in res:\n",
    "    print(key,*res[key])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}