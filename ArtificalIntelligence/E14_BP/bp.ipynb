{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level = logging.INFO)\n",
    "handler = logging.FileHandler(\"log/bp-40.log\")\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Index(['rectal temperature', 'pulse', 'respiratory rate', 'packed cell volume',\n       'total protein', 'surgery_category_1', 'surgery_category_2',\n       'mucous membranes_category_1', 'mucous membranes_category_2',\n       'mucous membranes_category_3', 'mucous membranes_category_4',\n       'mucous membranes_category_5', 'mucous membranes_category_6',\n       'pain_category_1', 'pain_category_2', 'pain_category_3',\n       'pain_category_4', 'pain_category_5', 'abdominal distension_category_1',\n       'abdominal distension_category_2', 'abdominal distension_category_3',\n       'abdominal distension_category_4', 'nasogastric tube_category_1',\n       'nasogastric tube_category_2', 'nasogastric tube_category_3',\n       'abdomen_category_1', 'abdomen_category_2', 'abdomen_category_3',\n       'abdomen_category_4', 'abdomen_category_5',\n       'abdominocentesis appearance_category_1',\n       'abdominocentesis appearance_category_2',\n       'abdominocentesis appearance_category_3', 'surgical lesion_category_1',\n       'surgical lesion_category_2', 'type of lesion 2_category_0',\n       'type of lesion 2_category_1400', 'type of lesion 2_category_2208',\n       'type of lesion 2_category_3111', 'type of lesion 2_category_3112',\n       'type of lesion 2_category_3205', 'type of lesion 2_category_6112',\n       'type of lesion 2_category_7111', 'type of lesion 3_category_0',\n       'type of lesion 3_category_2209', 'cp_data_category_1',\n       'cp_data_category_2'],\n      dtype='object')\n47\n"
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0: Continuous\n",
    "# 1: Nominal\n",
    "# 2: Categorical / Discrete\n",
    "attr_dict = {\"surgery\": 1,\n",
    " \"Age\": 2,\n",
    " \"Hospital Number\": 1,\n",
    " \"rectal temperature\": 0,\n",
    " \"pulse\": 0,\n",
    " \"respiratory rate\": 0,\n",
    " \"temperature of extremities\": 2,\n",
    " \"peripheral pulse\": 2,\n",
    " \"mucous membranes\": 1,\n",
    " \"capillary refill time\": 2,\n",
    " \"pain\": 1,\n",
    " \"peristalsis\": 2,\n",
    " \"abdominal distension\": 1,\n",
    " \"nasogastric tube\": 1,\n",
    " \"nasogastric reflux\": 2,\n",
    " \"nasogastric reflux PH\": 0,\n",
    " \"rectal examination\": 2,\n",
    " \"abdomen\": 1,\n",
    " \"packed cell volume\": 0,\n",
    " \"total protein\": 0,\n",
    " \"abdominocentesis appearance\": 1,\n",
    " \"abdomcentesis total protein\": 0,\n",
    " \"outcome\": 1,\n",
    " \"surgical lesion\": 1,\n",
    " \"type of lesion 1\": 1,\n",
    " \"type of lesion 2\": 1,\n",
    " \"type of lesion 3\": 1,\n",
    " \"cp_data\": 1} # 0: continuous, 1: discrete\n",
    "\n",
    "train_data = pd.read_csv(\"horse-colic.data\",names=attr_dict.keys(),index_col=False,delim_whitespace=True)\n",
    "test_data = pd.read_csv(\"horse-colic.test\",names=attr_dict.keys(),index_col=False,delim_whitespace=True)\n",
    "# for a in train_data.columns.values:\n",
    "#     print(train_data[a].value_counts())\n",
    "\n",
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    Select some useful attributes\n",
    "    \"\"\"\n",
    "    drop_attr = [\"type of lesion 2\", \"type of lesion 3\",\"Hospital Number\",\"nasogastric reflux PH\",\"abdomcentesis total protein\"]\n",
    "    # drop_attr = []\n",
    "    attributes = []\n",
    "    for a in data.columns.values:\n",
    "        in_flag = attr_dict.get(a,None)\n",
    "        if in_flag == None: # newly appended\n",
    "            attributes.append(a)\n",
    "        elif in_flag == 0 and a not in drop_attr: # continuous\n",
    "            # data[a] = (data[a] - data[a].mean()) / data[a].std() # normalization\n",
    "            attributes.append(a)\n",
    "        else: # discrete, no need to append\n",
    "            pass\n",
    "    df = data[attributes]\n",
    "    return df\n",
    "    # normalized_df = (df-df.mean())/df.std() # (df.max()-df.min())\n",
    "    # return normalized_df\n",
    "\n",
    "def fill_data(data):\n",
    "    \"\"\"\n",
    "    Fill in missing data (?)\n",
    "    For continuous attributes, fill them with mean values\n",
    "    For discrete attributes, fill them with the values appear most\n",
    "    \"\"\"\n",
    "    for a in data.columns.values:\n",
    "        if a in [\"type of lesion 1\", \"Hospital Number\"]: # remove\n",
    "        # if a in [\"Hospital Number\"]: # remove\n",
    "            continue\n",
    "        if data[a].dtype != np.int64: # has ?\n",
    "            have_data = data[data[a] != \"?\"][a]\n",
    "            if attr_dict[a]: # discrete\n",
    "                data.loc[data[a] == \"?\",a] = have_data.value_counts().idxmax() # view or copy? Use loc!\n",
    "                if a != \"outcome\" and attr_dict[a] != 2:\n",
    "                    # generate one-hot encoding\n",
    "                    data[a] = pd.Categorical(data[a])\n",
    "                    dummies = pd.get_dummies(data[a],prefix=\"{}_category\".format(a))\n",
    "                    data = pd.concat([data,dummies],axis=1)\n",
    "            else: # continuous\n",
    "                data.loc[data[a] == \"?\",a] = np.mean(have_data.astype(np.float))\n",
    "        elif attr_dict[a] == 1:\n",
    "            # generate one-hot encoding\n",
    "            data[a] = pd.Categorical(data[a])\n",
    "            dummies = pd.get_dummies(data[a],prefix=\"{}_category\".format(a))\n",
    "            data = pd.concat([data,dummies],axis=1)\n",
    "    return data.astype(np.float)\n",
    "\n",
    "# Data cleaning\n",
    "data = pd.concat([train_data,test_data],axis=0)\n",
    "data = fill_data(data)\n",
    "label = data[\"outcome\"].astype(np.float)\n",
    "train_label, test_label = label[:len(train_data)], label[len(train_data):]\n",
    "# train_label /= 3\n",
    "train_label = [[1,0,0] if label == 1 else ([0,1,0] if label == 2 else [0,0,1]) for label in train_label]\n",
    "# train_data, test_data = data[:len(train_data)], data[len(train_data):]\n",
    "# train_data = preprocessing(train_data)\n",
    "# test_data = preprocessing(test_data)\n",
    "data = preprocessing(data)\n",
    "train_data, test_data = data[:len(train_data)], data[len(train_data):]\n",
    "print(train_data.columns)\n",
    "print(len(train_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def get_batches(data,label,batch_size=1):\n",
    "    num_batches = len(data) // batch_size\n",
    "    for i in range(0,num_batches,batch_size):\n",
    "        # yield data[i:i+batch_size].to_numpy(), label[i:i+batch_size].to_numpy().reshape(-1,1)\n",
    "        yield data[i:i+batch_size].to_numpy(), np.array(label[i:i+batch_size])\n",
    "\n",
    "def train(net,max_iter=1000,early_stop=False):\n",
    "    \"\"\"\n",
    "    Train the network\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    loss_history, accuracy_history = [], []\n",
    "    losses = []\n",
    "    for i in range(max_iter):\n",
    "        net.train()\n",
    "\n",
    "        batches = get_batches(train_data,train_label,16) # generator\n",
    "        for x, y in batches:\n",
    "            Y_hat = net.forward(x)\n",
    "            loss = net.MSE(Y_hat, y)\n",
    "            # loss = net.cross_entropy(Y_hat,y).mean()\n",
    "            losses.append(loss)\n",
    "            # net.backward(Y_hat,y) # disable weight decay\n",
    "            net.backward(Y_hat,y,0.1) # enable weight decay\n",
    "\n",
    "        # logging\n",
    "        if (i+1) % 100 == 0:\n",
    "            avg_loss = np.array(losses).mean()\n",
    "            loss_history.append(avg_loss)\n",
    "            losses = []\n",
    "            acc = test(net,test_data,test_label)\n",
    "            accuracy_history.append(acc)\n",
    "            # Early syop\n",
    "            if early_stop and (i+1) % 1000 and len(accuracy_history) >= 5 and accuracy_history[-1] < min(accuracy_history[-10:-1]):\n",
    "                # net.learning_rate *= 0.999 # learning rate decay\n",
    "                logger.info(\"Early stop at iter {}/{} Loss: {} Accuracy: {}%\".format(i+1,max_iter,avg_loss,acc*100))\n",
    "                print(\"Early stop at iter {}/{} Loss: {} Accuracy: {}%\".format(i+1,max_iter,avg_loss,acc*100))\n",
    "                break\n",
    "            logger.info(\"Iter {}/{} Loss: {} Accuracy: {}%\".format(i+1,max_iter,avg_loss,acc*100))\n",
    "            print(\"Iter {}/{} Loss: {} Accuracy: {}%\".format(i+1,max_iter,avg_loss,acc*100))\n",
    "            if acc == max(accuracy_history):\n",
    "                with open(\"checkpoint/nn-best.pkl\",\"wb\") as netfile:\n",
    "                    pickle.dump(net,netfile)\n",
    "        if (i+1) % 1000 == 0:\n",
    "            with open(\"checkpoint/nn-{}.pkl\".format(i+1),\"wb\") as netfile:\n",
    "                pickle.dump(net,netfile)\n",
    "                \n",
    "    print(\"Time: {}s\".format(time.time() - start_time))\n",
    "    return loss_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,test_X,test_y,flag=True,print_flag=False):\n",
    "    \"\"\"\n",
    "    Test the network\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for j, x in test_X.iterrows():\n",
    "        net.eval()\n",
    "        Y_hat = net.forward(x.to_numpy().reshape(1,-1))\n",
    "        # predicted = np.argmin(abs(np.array([1/3,2/3,1]) - Y_hat)) + 1\n",
    "        # print(Y_hat,np.argmax(Y_hat))\n",
    "        predicted = np.argmax(Y_hat) + 1\n",
    "        y = test_y[j]\n",
    "        if print_flag:\n",
    "            print(Y_hat,predicted,y)\n",
    "        if flag:\n",
    "            if predicted == y:\n",
    "                cnt += 1\n",
    "        else:\n",
    "            if [1 if t + 1 == predicted else 0 for t in range(3)] == y:\n",
    "                cnt += 1\n",
    "    return (cnt / len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iter 100/100000 Loss: 0.7472033600839799 Accuracy: 87.58823529411765%\nIter 200/100000 Loss: 0.7210646473800734 Accuracy: 87.58823529411765%\nIter 300/100000 Loss: 0.7203806758380732 Accuracy: 87.58823529411765%\nIter 400/100000 Loss: 0.7202126831022119 Accuracy: 87.58823529411765%\nIter 500/100000 Loss: 0.7201424105202138 Accuracy: 87.58823529411765%\nIter 600/100000 Loss: 0.7200995180156826 Accuracy: 87.58823529411765%\nIter 700/100000 Loss: 0.7200665576711384 Accuracy: 87.58823529411765%\nIter 800/100000 Loss: 0.7200373063048133 Accuracy: 87.58823529411765%\nIter 900/100000 Loss: 0.7173977384527243 Accuracy: 87.58823529411765%\nIter 1000/100000 Loss: 0.6910220450181086 Accuracy: 87.58823529411765%\nIter 1100/100000 Loss: 0.6750272830662208 Accuracy: 87.58823529411765%\nIter 1200/100000 Loss: 0.665812515211816 Accuracy: 87.58823529411765%\nIter 1300/100000 Loss: 0.6597815957555795 Accuracy: 87.58823529411765%\nIter 1400/100000 Loss: 0.6545913532922571 Accuracy: 90.5294117647059%\nIter 1500/100000 Loss: 0.6480593287768979 Accuracy: 92.0%\nIter 1600/100000 Loss: 0.6427443958094696 Accuracy: 92.0%\nIter 1700/100000 Loss: 0.6384380537773697 Accuracy: 92.0%\nIter 1800/100000 Loss: 0.6353141344986356 Accuracy: 92.0%\nIter 1900/100000 Loss: 0.6330266213543354 Accuracy: 92.0%\nIter 2000/100000 Loss: 0.6312329687173412 Accuracy: 92.0%\nIter 2100/100000 Loss: 0.6296858360830062 Accuracy: 92.0%\nIter 2200/100000 Loss: 0.6282317805795642 Accuracy: 92.0%\nIter 2300/100000 Loss: 0.6268011951659986 Accuracy: 92.0%\nEarly stop at iter 2400/100000 Loss: 0.625693064579745 Accuracy: 90.5294117647059%\nTime: 3.130645275115967s\n"
    }
   ],
   "source": [
    "net = nn.Network(len(train_data.columns.values),5,3,0.1)\n",
    "# with open(\"checkpoint/nn-20000.pkl\",\"rb\") as netfile:\n",
    "#     net = pickle.load(netfile)\n",
    "#     net.learning_rate = 0.01\n",
    "loss_history, accuracy_history = train(net,100000,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the train loss curve\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "lns1 = ax.plot(loss_history,label=\"Loss\")\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(accuracy_history,\"-r\",label=\"Accuracy\")\n",
    "\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns,labs,loc=0)\n",
    "ax.set_xlabel(\"Iteration (x100)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_ylim(0,1)\n",
    "\n",
    "plt.savefig(r\"fig/iteration.pdf\",format=\"pdf\",dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(net,train_data,train_label,False)\n",
    "# test(net,test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}